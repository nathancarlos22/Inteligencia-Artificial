{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrW3_-5vlPlt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from scipy.io import arff\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYp08R9PzHCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH_DATA = '/content/emotions.arff'\n",
        "\n",
        "data = arff.loadarff(PATH_DATA)\n",
        "\n",
        "df = pd.DataFrame(data[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXVc7-FKzAPB",
        "colab_type": "code",
        "outputId": "8a137e21-2f88-45f2-e48f-03e19d437ffb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_Centroid</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_Rolloff</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_Flux</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_0</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_1</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_2</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_3</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_4</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_5</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_6</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_7</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_8</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_9</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_10</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_11</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_12</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_Centroid</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_Rolloff</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_Flux</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_0</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_1</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_2</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_3</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_4</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_5</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_6</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_7</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_8</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_9</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_10</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_11</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_12</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_Centroid</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_Rolloff</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_Flux</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_0</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_1</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_2</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_3</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_4</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_5</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_6</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_7</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_8</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_9</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_10</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_11</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_12</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_Centroid</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_Rolloff</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_Flux</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_0</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_1</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_2</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_3</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_4</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_5</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_6</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_7</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_8</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_9</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_10</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_11</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_12</th>\n",
              "      <th>BH_LowPeakAmp</th>\n",
              "      <th>BH_LowPeakBPM</th>\n",
              "      <th>BH_HighPeakAmp</th>\n",
              "      <th>BH_HighPeakBPM</th>\n",
              "      <th>BH_HighLowRatio</th>\n",
              "      <th>BHSUM1</th>\n",
              "      <th>BHSUM2</th>\n",
              "      <th>BHSUM3</th>\n",
              "      <th>amazed-suprised</th>\n",
              "      <th>happy-pleased</th>\n",
              "      <th>relaxing-calm</th>\n",
              "      <th>quiet-still</th>\n",
              "      <th>sad-lonely</th>\n",
              "      <th>angry-aggresive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.034741</td>\n",
              "      <td>0.089665</td>\n",
              "      <td>0.091225</td>\n",
              "      <td>-73.302422</td>\n",
              "      <td>6.215179</td>\n",
              "      <td>0.615074</td>\n",
              "      <td>2.037160</td>\n",
              "      <td>0.804065</td>\n",
              "      <td>1.301409</td>\n",
              "      <td>0.558576</td>\n",
              "      <td>0.672063</td>\n",
              "      <td>0.783788</td>\n",
              "      <td>0.766640</td>\n",
              "      <td>0.458712</td>\n",
              "      <td>0.530384</td>\n",
              "      <td>0.812429</td>\n",
              "      <td>0.028851</td>\n",
              "      <td>0.129039</td>\n",
              "      <td>0.039614</td>\n",
              "      <td>5.762173</td>\n",
              "      <td>1.636819</td>\n",
              "      <td>1.170034</td>\n",
              "      <td>1.051511</td>\n",
              "      <td>0.764163</td>\n",
              "      <td>0.642705</td>\n",
              "      <td>0.617868</td>\n",
              "      <td>0.510265</td>\n",
              "      <td>0.566213</td>\n",
              "      <td>0.509149</td>\n",
              "      <td>0.477275</td>\n",
              "      <td>0.505073</td>\n",
              "      <td>0.463535</td>\n",
              "      <td>0.013519</td>\n",
              "      <td>0.050591</td>\n",
              "      <td>0.009025</td>\n",
              "      <td>8.156257</td>\n",
              "      <td>1.077167</td>\n",
              "      <td>0.624711</td>\n",
              "      <td>0.810244</td>\n",
              "      <td>0.399568</td>\n",
              "      <td>0.279947</td>\n",
              "      <td>0.314215</td>\n",
              "      <td>0.231439</td>\n",
              "      <td>0.345401</td>\n",
              "      <td>0.285389</td>\n",
              "      <td>0.210613</td>\n",
              "      <td>0.321896</td>\n",
              "      <td>0.290551</td>\n",
              "      <td>0.022774</td>\n",
              "      <td>0.095801</td>\n",
              "      <td>0.015057</td>\n",
              "      <td>4.748694</td>\n",
              "      <td>0.536378</td>\n",
              "      <td>0.296306</td>\n",
              "      <td>0.273210</td>\n",
              "      <td>0.175800</td>\n",
              "      <td>0.105508</td>\n",
              "      <td>0.168246</td>\n",
              "      <td>0.115849</td>\n",
              "      <td>0.136020</td>\n",
              "      <td>0.110514</td>\n",
              "      <td>0.100517</td>\n",
              "      <td>0.118630</td>\n",
              "      <td>0.094923</td>\n",
              "      <td>0.051035</td>\n",
              "      <td>68.0</td>\n",
              "      <td>0.014937</td>\n",
              "      <td>136.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.245457</td>\n",
              "      <td>0.105065</td>\n",
              "      <td>0.405399</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'1'</td>\n",
              "      <td>b'1'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.081374</td>\n",
              "      <td>0.272747</td>\n",
              "      <td>0.085733</td>\n",
              "      <td>-62.584437</td>\n",
              "      <td>3.183163</td>\n",
              "      <td>-0.218145</td>\n",
              "      <td>0.163038</td>\n",
              "      <td>0.620251</td>\n",
              "      <td>0.458514</td>\n",
              "      <td>0.041426</td>\n",
              "      <td>0.308287</td>\n",
              "      <td>0.538152</td>\n",
              "      <td>0.594871</td>\n",
              "      <td>0.734332</td>\n",
              "      <td>0.415489</td>\n",
              "      <td>0.761508</td>\n",
              "      <td>0.066288</td>\n",
              "      <td>0.262370</td>\n",
              "      <td>0.034438</td>\n",
              "      <td>3.480874</td>\n",
              "      <td>1.596532</td>\n",
              "      <td>0.943803</td>\n",
              "      <td>0.804444</td>\n",
              "      <td>0.511229</td>\n",
              "      <td>0.498670</td>\n",
              "      <td>0.523039</td>\n",
              "      <td>0.480916</td>\n",
              "      <td>0.488657</td>\n",
              "      <td>0.483166</td>\n",
              "      <td>0.445187</td>\n",
              "      <td>0.415994</td>\n",
              "      <td>0.405593</td>\n",
              "      <td>0.013621</td>\n",
              "      <td>0.073041</td>\n",
              "      <td>0.010094</td>\n",
              "      <td>1.243981</td>\n",
              "      <td>0.829790</td>\n",
              "      <td>0.252972</td>\n",
              "      <td>0.347831</td>\n",
              "      <td>0.205087</td>\n",
              "      <td>0.168601</td>\n",
              "      <td>0.178009</td>\n",
              "      <td>0.144080</td>\n",
              "      <td>0.178703</td>\n",
              "      <td>0.146937</td>\n",
              "      <td>0.125580</td>\n",
              "      <td>0.128202</td>\n",
              "      <td>0.107007</td>\n",
              "      <td>0.020028</td>\n",
              "      <td>0.066940</td>\n",
              "      <td>0.029483</td>\n",
              "      <td>3.963534</td>\n",
              "      <td>0.382360</td>\n",
              "      <td>0.168389</td>\n",
              "      <td>0.117525</td>\n",
              "      <td>0.098341</td>\n",
              "      <td>0.087046</td>\n",
              "      <td>0.057991</td>\n",
              "      <td>0.059393</td>\n",
              "      <td>0.059457</td>\n",
              "      <td>0.053439</td>\n",
              "      <td>0.067684</td>\n",
              "      <td>0.070075</td>\n",
              "      <td>0.041565</td>\n",
              "      <td>0.295031</td>\n",
              "      <td>70.0</td>\n",
              "      <td>0.276366</td>\n",
              "      <td>140.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.343547</td>\n",
              "      <td>0.276366</td>\n",
              "      <td>0.710924</td>\n",
              "      <td>b'1'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.110545</td>\n",
              "      <td>0.273567</td>\n",
              "      <td>0.084410</td>\n",
              "      <td>-65.235325</td>\n",
              "      <td>2.794964</td>\n",
              "      <td>0.639047</td>\n",
              "      <td>1.281297</td>\n",
              "      <td>0.757896</td>\n",
              "      <td>0.489412</td>\n",
              "      <td>0.627636</td>\n",
              "      <td>0.469322</td>\n",
              "      <td>0.644336</td>\n",
              "      <td>0.441556</td>\n",
              "      <td>0.335964</td>\n",
              "      <td>0.290713</td>\n",
              "      <td>0.158538</td>\n",
              "      <td>0.082743</td>\n",
              "      <td>0.215373</td>\n",
              "      <td>0.035970</td>\n",
              "      <td>4.834742</td>\n",
              "      <td>1.213443</td>\n",
              "      <td>0.864034</td>\n",
              "      <td>0.909222</td>\n",
              "      <td>0.780572</td>\n",
              "      <td>0.550833</td>\n",
              "      <td>0.639740</td>\n",
              "      <td>0.573309</td>\n",
              "      <td>0.526312</td>\n",
              "      <td>0.562622</td>\n",
              "      <td>0.538407</td>\n",
              "      <td>0.492292</td>\n",
              "      <td>0.455562</td>\n",
              "      <td>0.029112</td>\n",
              "      <td>0.070433</td>\n",
              "      <td>0.008525</td>\n",
              "      <td>2.759906</td>\n",
              "      <td>0.592634</td>\n",
              "      <td>0.761852</td>\n",
              "      <td>0.568740</td>\n",
              "      <td>0.589827</td>\n",
              "      <td>0.281181</td>\n",
              "      <td>0.437752</td>\n",
              "      <td>0.479889</td>\n",
              "      <td>0.227320</td>\n",
              "      <td>0.296224</td>\n",
              "      <td>0.273855</td>\n",
              "      <td>0.191804</td>\n",
              "      <td>0.198025</td>\n",
              "      <td>0.038119</td>\n",
              "      <td>0.065427</td>\n",
              "      <td>0.029622</td>\n",
              "      <td>3.371796</td>\n",
              "      <td>0.430373</td>\n",
              "      <td>0.172862</td>\n",
              "      <td>0.177523</td>\n",
              "      <td>0.184333</td>\n",
              "      <td>0.095718</td>\n",
              "      <td>0.139323</td>\n",
              "      <td>0.109279</td>\n",
              "      <td>0.090650</td>\n",
              "      <td>0.117886</td>\n",
              "      <td>0.100852</td>\n",
              "      <td>0.079917</td>\n",
              "      <td>0.085821</td>\n",
              "      <td>0.161574</td>\n",
              "      <td>61.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>183.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.188693</td>\n",
              "      <td>0.045941</td>\n",
              "      <td>0.457372</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'1'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.042481</td>\n",
              "      <td>0.199281</td>\n",
              "      <td>0.093447</td>\n",
              "      <td>-80.305152</td>\n",
              "      <td>5.824409</td>\n",
              "      <td>0.648848</td>\n",
              "      <td>1.754870</td>\n",
              "      <td>1.495532</td>\n",
              "      <td>0.739909</td>\n",
              "      <td>0.809644</td>\n",
              "      <td>0.460945</td>\n",
              "      <td>0.409566</td>\n",
              "      <td>0.680122</td>\n",
              "      <td>0.590405</td>\n",
              "      <td>0.481380</td>\n",
              "      <td>0.621956</td>\n",
              "      <td>0.049939</td>\n",
              "      <td>0.281616</td>\n",
              "      <td>0.044727</td>\n",
              "      <td>6.719538</td>\n",
              "      <td>1.377811</td>\n",
              "      <td>1.265771</td>\n",
              "      <td>0.986178</td>\n",
              "      <td>0.710955</td>\n",
              "      <td>0.706904</td>\n",
              "      <td>0.710147</td>\n",
              "      <td>0.688825</td>\n",
              "      <td>0.699573</td>\n",
              "      <td>0.577976</td>\n",
              "      <td>0.533882</td>\n",
              "      <td>0.501818</td>\n",
              "      <td>0.495368</td>\n",
              "      <td>0.020749</td>\n",
              "      <td>0.106318</td>\n",
              "      <td>0.009108</td>\n",
              "      <td>3.992357</td>\n",
              "      <td>0.656429</td>\n",
              "      <td>0.927692</td>\n",
              "      <td>0.569916</td>\n",
              "      <td>0.378919</td>\n",
              "      <td>0.530714</td>\n",
              "      <td>0.317807</td>\n",
              "      <td>0.308447</td>\n",
              "      <td>0.324934</td>\n",
              "      <td>0.263444</td>\n",
              "      <td>0.359477</td>\n",
              "      <td>0.274257</td>\n",
              "      <td>0.233287</td>\n",
              "      <td>0.032678</td>\n",
              "      <td>0.119480</td>\n",
              "      <td>0.028707</td>\n",
              "      <td>4.125111</td>\n",
              "      <td>0.461304</td>\n",
              "      <td>0.280751</td>\n",
              "      <td>0.246108</td>\n",
              "      <td>0.142805</td>\n",
              "      <td>0.183657</td>\n",
              "      <td>0.124399</td>\n",
              "      <td>0.155513</td>\n",
              "      <td>0.167114</td>\n",
              "      <td>0.113774</td>\n",
              "      <td>0.112815</td>\n",
              "      <td>0.129145</td>\n",
              "      <td>0.122330</td>\n",
              "      <td>0.043012</td>\n",
              "      <td>66.0</td>\n",
              "      <td>0.206562</td>\n",
              "      <td>132.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.102839</td>\n",
              "      <td>0.241934</td>\n",
              "      <td>0.351009</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'1'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.074550</td>\n",
              "      <td>0.140880</td>\n",
              "      <td>0.079789</td>\n",
              "      <td>-93.697749</td>\n",
              "      <td>5.543229</td>\n",
              "      <td>1.064262</td>\n",
              "      <td>0.899152</td>\n",
              "      <td>0.890336</td>\n",
              "      <td>0.702328</td>\n",
              "      <td>0.490685</td>\n",
              "      <td>0.796904</td>\n",
              "      <td>0.745373</td>\n",
              "      <td>0.911234</td>\n",
              "      <td>0.594429</td>\n",
              "      <td>0.454186</td>\n",
              "      <td>0.384836</td>\n",
              "      <td>0.035751</td>\n",
              "      <td>0.085592</td>\n",
              "      <td>0.029413</td>\n",
              "      <td>4.755293</td>\n",
              "      <td>1.116290</td>\n",
              "      <td>0.926772</td>\n",
              "      <td>0.634988</td>\n",
              "      <td>0.639660</td>\n",
              "      <td>0.552653</td>\n",
              "      <td>0.527708</td>\n",
              "      <td>0.584705</td>\n",
              "      <td>0.696173</td>\n",
              "      <td>0.648611</td>\n",
              "      <td>0.689096</td>\n",
              "      <td>0.643595</td>\n",
              "      <td>0.578063</td>\n",
              "      <td>0.047014</td>\n",
              "      <td>0.136984</td>\n",
              "      <td>0.010356</td>\n",
              "      <td>7.713140</td>\n",
              "      <td>1.592642</td>\n",
              "      <td>1.027190</td>\n",
              "      <td>0.591399</td>\n",
              "      <td>0.565654</td>\n",
              "      <td>0.524420</td>\n",
              "      <td>0.554501</td>\n",
              "      <td>0.606200</td>\n",
              "      <td>0.616760</td>\n",
              "      <td>0.596926</td>\n",
              "      <td>0.524291</td>\n",
              "      <td>0.637971</td>\n",
              "      <td>0.637960</td>\n",
              "      <td>0.036151</td>\n",
              "      <td>0.087741</td>\n",
              "      <td>0.030180</td>\n",
              "      <td>5.085385</td>\n",
              "      <td>0.551937</td>\n",
              "      <td>0.257562</td>\n",
              "      <td>0.159950</td>\n",
              "      <td>0.175855</td>\n",
              "      <td>0.150907</td>\n",
              "      <td>0.142092</td>\n",
              "      <td>0.222804</td>\n",
              "      <td>0.329188</td>\n",
              "      <td>0.251668</td>\n",
              "      <td>0.265049</td>\n",
              "      <td>0.284196</td>\n",
              "      <td>0.189988</td>\n",
              "      <td>0.029308</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.144039</td>\n",
              "      <td>200.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.195196</td>\n",
              "      <td>0.310801</td>\n",
              "      <td>0.683817</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'1'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Mean_Acc1298_Mean_Mem40_Centroid  ...  angry-aggresive\n",
              "0                          0.034741  ...             b'0'\n",
              "1                          0.081374  ...             b'1'\n",
              "2                          0.110545  ...             b'1'\n",
              "3                          0.042481  ...             b'0'\n",
              "4                          0.074550  ...             b'0'\n",
              "\n",
              "[5 rows x 78 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhRPaQk71pXC",
        "colab_type": "code",
        "outputId": "50fc06e7-5e3b-430a-85c5-7d29a3e98c2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "print(\"######### VALORES NULOS  ######## \\n\")\n",
        "print(df.isnull().sum())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "######### VALORES NULOS  ######## \n",
            "\n",
            "Mean_Acc1298_Mean_Mem40_Centroid    0\n",
            "Mean_Acc1298_Mean_Mem40_Rolloff     0\n",
            "Mean_Acc1298_Mean_Mem40_Flux        0\n",
            "Mean_Acc1298_Mean_Mem40_MFCC_0      0\n",
            "Mean_Acc1298_Mean_Mem40_MFCC_1      0\n",
            "                                   ..\n",
            "happy-pleased                       0\n",
            "relaxing-calm                       0\n",
            "quiet-still                         0\n",
            "sad-lonely                          0\n",
            "angry-aggresive                     0\n",
            "Length: 78, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ok6v6aJ3G0q7",
        "colab_type": "code",
        "outputId": "e4a2aae9-09ae-4d2f-bae3-3fbcdee48762",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "print(\"######### VALORES DUPLICADOS #######\\n\")\n",
        "print(df.duplicated())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "######### VALORES DUPLICADOS #######\n",
            "\n",
            "0      False\n",
            "1      False\n",
            "2      False\n",
            "3      False\n",
            "4      False\n",
            "       ...  \n",
            "588    False\n",
            "589    False\n",
            "590    False\n",
            "591    False\n",
            "592    False\n",
            "Length: 593, dtype: bool\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdi249SNHMfZ",
        "colab_type": "text"
      },
      "source": [
        "# **Normalizando**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvQZIosnHQVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "class_name = ['amazed-suprised',\n",
        "'happy-pleased',\n",
        "'relaxing-calm',\n",
        "'quiet-still',\n",
        "'sad-lonely',\n",
        "'angry-aggresive']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAoz-OymHTh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for col in df.columns: \n",
        "    if col not in class_name:\n",
        "        x_train = np.array(0)\n",
        "        x_train = df[col]\n",
        "        x_nomalized = preprocessing.normalize([x_train])\n",
        "        df[col] = x_nomalized[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfUyXFBeHomD",
        "colab_type": "text"
      },
      "source": [
        "# **Correlação**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db8nzkpcHq3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corr_matrix = df.corr().abs()\n",
        "\n",
        "# matriz de triangulo superior\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "\n",
        "# pegando os indices com correlação maior que 0.80\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.80)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW69Z5i5H1bM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_semCorrelação = df.drop(df[to_drop], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kebG304IXz_",
        "colab_type": "code",
        "outputId": "e61465a3-a809-430b-b450-f3cff7132f13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        }
      },
      "source": [
        "df_semCorrelação"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_Centroid</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_Rolloff</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_Flux</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_0</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_1</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_2</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_3</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_4</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_5</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_6</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_7</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_8</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_9</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_10</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_11</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_12</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_Centroid</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_0</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_1</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_2</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_3</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_8</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_9</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_Centroid</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_Rolloff</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_Flux</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_0</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_1</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_2</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_3</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_4</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_5</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_6</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_7</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_9</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_Centroid</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_Rolloff</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_Flux</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_0</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_1</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_2</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_3</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_4</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_5</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_6</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_7</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_8</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_9</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_10</th>\n",
              "      <th>BH_LowPeakAmp</th>\n",
              "      <th>BH_LowPeakBPM</th>\n",
              "      <th>BH_HighPeakBPM</th>\n",
              "      <th>BH_HighLowRatio</th>\n",
              "      <th>amazed-suprised</th>\n",
              "      <th>happy-pleased</th>\n",
              "      <th>relaxing-calm</th>\n",
              "      <th>quiet-still</th>\n",
              "      <th>sad-lonely</th>\n",
              "      <th>angry-aggresive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.018763</td>\n",
              "      <td>0.016149</td>\n",
              "      <td>0.043463</td>\n",
              "      <td>-0.041016</td>\n",
              "      <td>0.047331</td>\n",
              "      <td>0.021373</td>\n",
              "      <td>0.046482</td>\n",
              "      <td>0.042515</td>\n",
              "      <td>0.058628</td>\n",
              "      <td>0.038775</td>\n",
              "      <td>0.044554</td>\n",
              "      <td>0.058590</td>\n",
              "      <td>0.055764</td>\n",
              "      <td>0.035826</td>\n",
              "      <td>0.044299</td>\n",
              "      <td>0.068257</td>\n",
              "      <td>0.021723</td>\n",
              "      <td>0.048957</td>\n",
              "      <td>0.049430</td>\n",
              "      <td>0.049055</td>\n",
              "      <td>0.053520</td>\n",
              "      <td>0.044277</td>\n",
              "      <td>0.040537</td>\n",
              "      <td>0.021159</td>\n",
              "      <td>0.029441</td>\n",
              "      <td>0.044803</td>\n",
              "      <td>0.082391</td>\n",
              "      <td>0.047352</td>\n",
              "      <td>0.037992</td>\n",
              "      <td>0.061658</td>\n",
              "      <td>0.039579</td>\n",
              "      <td>0.032049</td>\n",
              "      <td>0.037665</td>\n",
              "      <td>0.027924</td>\n",
              "      <td>0.034880</td>\n",
              "      <td>0.031352</td>\n",
              "      <td>0.055758</td>\n",
              "      <td>0.020917</td>\n",
              "      <td>0.046671</td>\n",
              "      <td>0.051209</td>\n",
              "      <td>0.047828</td>\n",
              "      <td>0.052492</td>\n",
              "      <td>0.044358</td>\n",
              "      <td>0.033350</td>\n",
              "      <td>0.056821</td>\n",
              "      <td>0.039504</td>\n",
              "      <td>0.045909</td>\n",
              "      <td>0.036780</td>\n",
              "      <td>0.034219</td>\n",
              "      <td>0.005595</td>\n",
              "      <td>0.036803</td>\n",
              "      <td>0.035337</td>\n",
              "      <td>0.038684</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'1'</td>\n",
              "      <td>b'1'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.043948</td>\n",
              "      <td>0.049122</td>\n",
              "      <td>0.040846</td>\n",
              "      <td>-0.035019</td>\n",
              "      <td>0.024241</td>\n",
              "      <td>-0.007580</td>\n",
              "      <td>0.003720</td>\n",
              "      <td>0.032795</td>\n",
              "      <td>0.020656</td>\n",
              "      <td>0.002876</td>\n",
              "      <td>0.020438</td>\n",
              "      <td>0.040228</td>\n",
              "      <td>0.043270</td>\n",
              "      <td>0.057352</td>\n",
              "      <td>0.034703</td>\n",
              "      <td>0.063979</td>\n",
              "      <td>0.049910</td>\n",
              "      <td>0.029574</td>\n",
              "      <td>0.048214</td>\n",
              "      <td>0.039570</td>\n",
              "      <td>0.040945</td>\n",
              "      <td>0.038212</td>\n",
              "      <td>0.038469</td>\n",
              "      <td>0.021319</td>\n",
              "      <td>0.042506</td>\n",
              "      <td>0.050110</td>\n",
              "      <td>0.012566</td>\n",
              "      <td>0.036477</td>\n",
              "      <td>0.015385</td>\n",
              "      <td>0.026469</td>\n",
              "      <td>0.020315</td>\n",
              "      <td>0.019302</td>\n",
              "      <td>0.021338</td>\n",
              "      <td>0.017384</td>\n",
              "      <td>0.017958</td>\n",
              "      <td>0.027572</td>\n",
              "      <td>0.038960</td>\n",
              "      <td>0.040957</td>\n",
              "      <td>0.038954</td>\n",
              "      <td>0.036504</td>\n",
              "      <td>0.027181</td>\n",
              "      <td>0.022580</td>\n",
              "      <td>0.024814</td>\n",
              "      <td>0.027514</td>\n",
              "      <td>0.019585</td>\n",
              "      <td>0.020253</td>\n",
              "      <td>0.020068</td>\n",
              "      <td>0.017785</td>\n",
              "      <td>0.023042</td>\n",
              "      <td>0.032346</td>\n",
              "      <td>0.037886</td>\n",
              "      <td>0.036376</td>\n",
              "      <td>0.038684</td>\n",
              "      <td>b'1'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.059703</td>\n",
              "      <td>0.049270</td>\n",
              "      <td>0.040216</td>\n",
              "      <td>-0.036502</td>\n",
              "      <td>0.021285</td>\n",
              "      <td>0.022206</td>\n",
              "      <td>0.029236</td>\n",
              "      <td>0.040073</td>\n",
              "      <td>0.022048</td>\n",
              "      <td>0.043569</td>\n",
              "      <td>0.031114</td>\n",
              "      <td>0.048166</td>\n",
              "      <td>0.032118</td>\n",
              "      <td>0.026239</td>\n",
              "      <td>0.024281</td>\n",
              "      <td>0.013320</td>\n",
              "      <td>0.062299</td>\n",
              "      <td>0.041077</td>\n",
              "      <td>0.036645</td>\n",
              "      <td>0.036225</td>\n",
              "      <td>0.046278</td>\n",
              "      <td>0.041156</td>\n",
              "      <td>0.044795</td>\n",
              "      <td>0.045564</td>\n",
              "      <td>0.040988</td>\n",
              "      <td>0.042321</td>\n",
              "      <td>0.027879</td>\n",
              "      <td>0.026052</td>\n",
              "      <td>0.046332</td>\n",
              "      <td>0.043280</td>\n",
              "      <td>0.058425</td>\n",
              "      <td>0.032191</td>\n",
              "      <td>0.052473</td>\n",
              "      <td>0.057900</td>\n",
              "      <td>0.036204</td>\n",
              "      <td>0.052477</td>\n",
              "      <td>0.038080</td>\n",
              "      <td>0.041151</td>\n",
              "      <td>0.033139</td>\n",
              "      <td>0.041088</td>\n",
              "      <td>0.027903</td>\n",
              "      <td>0.034108</td>\n",
              "      <td>0.046512</td>\n",
              "      <td>0.030255</td>\n",
              "      <td>0.047053</td>\n",
              "      <td>0.037263</td>\n",
              "      <td>0.030596</td>\n",
              "      <td>0.039234</td>\n",
              "      <td>0.034333</td>\n",
              "      <td>0.017714</td>\n",
              "      <td>0.033015</td>\n",
              "      <td>0.047548</td>\n",
              "      <td>0.058026</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'1'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.022943</td>\n",
              "      <td>0.035891</td>\n",
              "      <td>0.044521</td>\n",
              "      <td>-0.044934</td>\n",
              "      <td>0.044355</td>\n",
              "      <td>0.022546</td>\n",
              "      <td>0.040041</td>\n",
              "      <td>0.079076</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.056204</td>\n",
              "      <td>0.030558</td>\n",
              "      <td>0.030616</td>\n",
              "      <td>0.049470</td>\n",
              "      <td>0.046111</td>\n",
              "      <td>0.040206</td>\n",
              "      <td>0.052254</td>\n",
              "      <td>0.037600</td>\n",
              "      <td>0.057091</td>\n",
              "      <td>0.041608</td>\n",
              "      <td>0.053068</td>\n",
              "      <td>0.050195</td>\n",
              "      <td>0.054705</td>\n",
              "      <td>0.046017</td>\n",
              "      <td>0.032475</td>\n",
              "      <td>0.061871</td>\n",
              "      <td>0.045215</td>\n",
              "      <td>0.040329</td>\n",
              "      <td>0.028856</td>\n",
              "      <td>0.056418</td>\n",
              "      <td>0.043370</td>\n",
              "      <td>0.037533</td>\n",
              "      <td>0.060758</td>\n",
              "      <td>0.038095</td>\n",
              "      <td>0.037215</td>\n",
              "      <td>0.032198</td>\n",
              "      <td>0.044987</td>\n",
              "      <td>0.069539</td>\n",
              "      <td>0.039879</td>\n",
              "      <td>0.040542</td>\n",
              "      <td>0.044041</td>\n",
              "      <td>0.045318</td>\n",
              "      <td>0.047285</td>\n",
              "      <td>0.036033</td>\n",
              "      <td>0.058052</td>\n",
              "      <td>0.042013</td>\n",
              "      <td>0.053029</td>\n",
              "      <td>0.056403</td>\n",
              "      <td>0.037865</td>\n",
              "      <td>0.038406</td>\n",
              "      <td>0.004716</td>\n",
              "      <td>0.035721</td>\n",
              "      <td>0.034297</td>\n",
              "      <td>0.038684</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'1'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.040263</td>\n",
              "      <td>0.025373</td>\n",
              "      <td>0.038014</td>\n",
              "      <td>-0.052428</td>\n",
              "      <td>0.042214</td>\n",
              "      <td>0.036981</td>\n",
              "      <td>0.020516</td>\n",
              "      <td>0.047076</td>\n",
              "      <td>0.031640</td>\n",
              "      <td>0.034062</td>\n",
              "      <td>0.052831</td>\n",
              "      <td>0.055718</td>\n",
              "      <td>0.066281</td>\n",
              "      <td>0.046425</td>\n",
              "      <td>0.037935</td>\n",
              "      <td>0.032332</td>\n",
              "      <td>0.026918</td>\n",
              "      <td>0.040402</td>\n",
              "      <td>0.033711</td>\n",
              "      <td>0.038856</td>\n",
              "      <td>0.032320</td>\n",
              "      <td>0.054439</td>\n",
              "      <td>0.051641</td>\n",
              "      <td>0.073583</td>\n",
              "      <td>0.079717</td>\n",
              "      <td>0.051410</td>\n",
              "      <td>0.077915</td>\n",
              "      <td>0.070012</td>\n",
              "      <td>0.062469</td>\n",
              "      <td>0.045005</td>\n",
              "      <td>0.056030</td>\n",
              "      <td>0.060037</td>\n",
              "      <td>0.066468</td>\n",
              "      <td>0.073139</td>\n",
              "      <td>0.072955</td>\n",
              "      <td>0.049768</td>\n",
              "      <td>0.051067</td>\n",
              "      <td>0.041926</td>\n",
              "      <td>0.049980</td>\n",
              "      <td>0.052694</td>\n",
              "      <td>0.041574</td>\n",
              "      <td>0.030731</td>\n",
              "      <td>0.044372</td>\n",
              "      <td>0.047700</td>\n",
              "      <td>0.047988</td>\n",
              "      <td>0.075974</td>\n",
              "      <td>0.111105</td>\n",
              "      <td>0.083757</td>\n",
              "      <td>0.090232</td>\n",
              "      <td>0.003213</td>\n",
              "      <td>0.054122</td>\n",
              "      <td>0.051966</td>\n",
              "      <td>0.038684</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'1'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>588</th>\n",
              "      <td>0.014659</td>\n",
              "      <td>0.008564</td>\n",
              "      <td>0.034324</td>\n",
              "      <td>-0.044697</td>\n",
              "      <td>0.061832</td>\n",
              "      <td>0.066970</td>\n",
              "      <td>0.038698</td>\n",
              "      <td>0.021038</td>\n",
              "      <td>0.038633</td>\n",
              "      <td>0.021016</td>\n",
              "      <td>0.044593</td>\n",
              "      <td>0.055927</td>\n",
              "      <td>0.030037</td>\n",
              "      <td>0.066590</td>\n",
              "      <td>0.049874</td>\n",
              "      <td>0.038322</td>\n",
              "      <td>0.009863</td>\n",
              "      <td>0.026142</td>\n",
              "      <td>0.026044</td>\n",
              "      <td>0.026837</td>\n",
              "      <td>0.031313</td>\n",
              "      <td>0.042567</td>\n",
              "      <td>0.043005</td>\n",
              "      <td>0.020109</td>\n",
              "      <td>0.025668</td>\n",
              "      <td>0.043711</td>\n",
              "      <td>0.025868</td>\n",
              "      <td>0.044316</td>\n",
              "      <td>0.031815</td>\n",
              "      <td>0.061041</td>\n",
              "      <td>0.068755</td>\n",
              "      <td>0.047220</td>\n",
              "      <td>0.055317</td>\n",
              "      <td>0.060421</td>\n",
              "      <td>0.055529</td>\n",
              "      <td>0.039378</td>\n",
              "      <td>0.040673</td>\n",
              "      <td>0.042249</td>\n",
              "      <td>0.049556</td>\n",
              "      <td>0.043490</td>\n",
              "      <td>0.028573</td>\n",
              "      <td>0.031882</td>\n",
              "      <td>0.031469</td>\n",
              "      <td>0.037336</td>\n",
              "      <td>0.065703</td>\n",
              "      <td>0.050073</td>\n",
              "      <td>0.046971</td>\n",
              "      <td>0.053844</td>\n",
              "      <td>0.054209</td>\n",
              "      <td>0.009866</td>\n",
              "      <td>0.047086</td>\n",
              "      <td>0.045210</td>\n",
              "      <td>0.038684</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'1'</td>\n",
              "      <td>b'1'</td>\n",
              "      <td>b'1'</td>\n",
              "      <td>b'0'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>589</th>\n",
              "      <td>0.051215</td>\n",
              "      <td>0.036831</td>\n",
              "      <td>0.039460</td>\n",
              "      <td>-0.034336</td>\n",
              "      <td>0.022589</td>\n",
              "      <td>0.021813</td>\n",
              "      <td>0.032865</td>\n",
              "      <td>0.045273</td>\n",
              "      <td>0.050018</td>\n",
              "      <td>0.027382</td>\n",
              "      <td>0.048151</td>\n",
              "      <td>0.066181</td>\n",
              "      <td>0.037650</td>\n",
              "      <td>-0.014556</td>\n",
              "      <td>0.044358</td>\n",
              "      <td>0.026223</td>\n",
              "      <td>0.039382</td>\n",
              "      <td>0.029845</td>\n",
              "      <td>0.027742</td>\n",
              "      <td>0.043757</td>\n",
              "      <td>0.039605</td>\n",
              "      <td>0.042358</td>\n",
              "      <td>0.043982</td>\n",
              "      <td>0.033826</td>\n",
              "      <td>0.027647</td>\n",
              "      <td>0.036587</td>\n",
              "      <td>0.014344</td>\n",
              "      <td>0.023266</td>\n",
              "      <td>0.023650</td>\n",
              "      <td>0.033034</td>\n",
              "      <td>0.031572</td>\n",
              "      <td>0.030397</td>\n",
              "      <td>0.025208</td>\n",
              "      <td>0.020434</td>\n",
              "      <td>0.030242</td>\n",
              "      <td>0.032888</td>\n",
              "      <td>0.027463</td>\n",
              "      <td>0.042042</td>\n",
              "      <td>0.039744</td>\n",
              "      <td>0.034644</td>\n",
              "      <td>0.033942</td>\n",
              "      <td>0.040963</td>\n",
              "      <td>0.028882</td>\n",
              "      <td>0.050122</td>\n",
              "      <td>0.034030</td>\n",
              "      <td>0.039277</td>\n",
              "      <td>0.031224</td>\n",
              "      <td>0.031281</td>\n",
              "      <td>0.036877</td>\n",
              "      <td>0.003928</td>\n",
              "      <td>0.035180</td>\n",
              "      <td>0.033778</td>\n",
              "      <td>0.038684</td>\n",
              "      <td>b'1'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'1'</td>\n",
              "      <td>b'1'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>590</th>\n",
              "      <td>0.018994</td>\n",
              "      <td>0.011779</td>\n",
              "      <td>0.035841</td>\n",
              "      <td>-0.045743</td>\n",
              "      <td>0.078528</td>\n",
              "      <td>0.003205</td>\n",
              "      <td>0.018684</td>\n",
              "      <td>0.082992</td>\n",
              "      <td>0.082527</td>\n",
              "      <td>0.003972</td>\n",
              "      <td>0.027800</td>\n",
              "      <td>0.036056</td>\n",
              "      <td>0.054201</td>\n",
              "      <td>0.031496</td>\n",
              "      <td>0.070008</td>\n",
              "      <td>0.059799</td>\n",
              "      <td>0.009963</td>\n",
              "      <td>0.034931</td>\n",
              "      <td>0.027530</td>\n",
              "      <td>0.044328</td>\n",
              "      <td>0.025265</td>\n",
              "      <td>0.033068</td>\n",
              "      <td>0.041098</td>\n",
              "      <td>0.025089</td>\n",
              "      <td>0.020690</td>\n",
              "      <td>0.048447</td>\n",
              "      <td>0.058353</td>\n",
              "      <td>0.049399</td>\n",
              "      <td>0.050525</td>\n",
              "      <td>0.025822</td>\n",
              "      <td>0.099444</td>\n",
              "      <td>0.050496</td>\n",
              "      <td>0.067569</td>\n",
              "      <td>0.057263</td>\n",
              "      <td>0.066429</td>\n",
              "      <td>0.037255</td>\n",
              "      <td>0.031100</td>\n",
              "      <td>0.042537</td>\n",
              "      <td>0.047186</td>\n",
              "      <td>0.060425</td>\n",
              "      <td>0.056661</td>\n",
              "      <td>0.025590</td>\n",
              "      <td>0.088026</td>\n",
              "      <td>0.051191</td>\n",
              "      <td>0.043860</td>\n",
              "      <td>0.046742</td>\n",
              "      <td>0.039889</td>\n",
              "      <td>0.040108</td>\n",
              "      <td>0.028266</td>\n",
              "      <td>0.003693</td>\n",
              "      <td>0.047628</td>\n",
              "      <td>0.045730</td>\n",
              "      <td>0.038684</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'1'</td>\n",
              "      <td>b'1'</td>\n",
              "      <td>b'1'</td>\n",
              "      <td>b'0'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591</th>\n",
              "      <td>0.029313</td>\n",
              "      <td>0.042893</td>\n",
              "      <td>0.045707</td>\n",
              "      <td>-0.039733</td>\n",
              "      <td>0.024227</td>\n",
              "      <td>0.053762</td>\n",
              "      <td>0.054939</td>\n",
              "      <td>0.032721</td>\n",
              "      <td>0.044957</td>\n",
              "      <td>0.057280</td>\n",
              "      <td>0.050305</td>\n",
              "      <td>0.054385</td>\n",
              "      <td>0.036374</td>\n",
              "      <td>0.031403</td>\n",
              "      <td>0.045039</td>\n",
              "      <td>0.027424</td>\n",
              "      <td>0.039885</td>\n",
              "      <td>0.057049</td>\n",
              "      <td>0.048318</td>\n",
              "      <td>0.054888</td>\n",
              "      <td>0.044717</td>\n",
              "      <td>0.040657</td>\n",
              "      <td>0.038504</td>\n",
              "      <td>0.026534</td>\n",
              "      <td>0.053179</td>\n",
              "      <td>0.043999</td>\n",
              "      <td>0.032788</td>\n",
              "      <td>0.033155</td>\n",
              "      <td>0.047880</td>\n",
              "      <td>0.033695</td>\n",
              "      <td>0.030789</td>\n",
              "      <td>0.036999</td>\n",
              "      <td>0.029653</td>\n",
              "      <td>0.031081</td>\n",
              "      <td>0.028470</td>\n",
              "      <td>0.036377</td>\n",
              "      <td>0.051279</td>\n",
              "      <td>0.039967</td>\n",
              "      <td>0.040246</td>\n",
              "      <td>0.055823</td>\n",
              "      <td>0.050461</td>\n",
              "      <td>0.041237</td>\n",
              "      <td>0.042653</td>\n",
              "      <td>0.036498</td>\n",
              "      <td>0.037774</td>\n",
              "      <td>0.038447</td>\n",
              "      <td>0.030546</td>\n",
              "      <td>0.030518</td>\n",
              "      <td>0.030732</td>\n",
              "      <td>0.017065</td>\n",
              "      <td>0.045463</td>\n",
              "      <td>0.043651</td>\n",
              "      <td>0.038684</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'1'</td>\n",
              "      <td>b'1'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>592</th>\n",
              "      <td>0.039530</td>\n",
              "      <td>0.025346</td>\n",
              "      <td>0.038374</td>\n",
              "      <td>-0.041696</td>\n",
              "      <td>0.052894</td>\n",
              "      <td>-0.036414</td>\n",
              "      <td>0.044546</td>\n",
              "      <td>0.018080</td>\n",
              "      <td>0.034715</td>\n",
              "      <td>0.010065</td>\n",
              "      <td>0.026905</td>\n",
              "      <td>0.013967</td>\n",
              "      <td>0.036090</td>\n",
              "      <td>0.030389</td>\n",
              "      <td>0.027116</td>\n",
              "      <td>0.016258</td>\n",
              "      <td>0.015090</td>\n",
              "      <td>0.025597</td>\n",
              "      <td>0.024286</td>\n",
              "      <td>0.021984</td>\n",
              "      <td>0.025866</td>\n",
              "      <td>0.040261</td>\n",
              "      <td>0.040456</td>\n",
              "      <td>0.024317</td>\n",
              "      <td>0.019812</td>\n",
              "      <td>0.037351</td>\n",
              "      <td>0.050693</td>\n",
              "      <td>0.037464</td>\n",
              "      <td>0.024941</td>\n",
              "      <td>0.020930</td>\n",
              "      <td>0.029766</td>\n",
              "      <td>0.028445</td>\n",
              "      <td>0.043399</td>\n",
              "      <td>0.059255</td>\n",
              "      <td>0.058919</td>\n",
              "      <td>0.033939</td>\n",
              "      <td>0.030441</td>\n",
              "      <td>0.042079</td>\n",
              "      <td>0.041717</td>\n",
              "      <td>0.033504</td>\n",
              "      <td>0.024139</td>\n",
              "      <td>0.023578</td>\n",
              "      <td>0.016137</td>\n",
              "      <td>0.024678</td>\n",
              "      <td>0.023246</td>\n",
              "      <td>0.027629</td>\n",
              "      <td>0.047022</td>\n",
              "      <td>0.034476</td>\n",
              "      <td>0.038951</td>\n",
              "      <td>0.006151</td>\n",
              "      <td>0.050875</td>\n",
              "      <td>0.048848</td>\n",
              "      <td>0.038684</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'1'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "      <td>b'0'</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>593 rows × 59 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Mean_Acc1298_Mean_Mem40_Centroid  ...  angry-aggresive\n",
              "0                            0.018763  ...             b'0'\n",
              "1                            0.043948  ...             b'1'\n",
              "2                            0.059703  ...             b'1'\n",
              "3                            0.022943  ...             b'0'\n",
              "4                            0.040263  ...             b'0'\n",
              "..                                ...  ...              ...\n",
              "588                          0.014659  ...             b'0'\n",
              "589                          0.051215  ...             b'1'\n",
              "590                          0.018994  ...             b'0'\n",
              "591                          0.029313  ...             b'0'\n",
              "592                          0.039530  ...             b'0'\n",
              "\n",
              "[593 rows x 59 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFFHZLUTIZTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "le = preprocessing.LabelEncoder() #transforma atributos qualitativos em quantitativos\n",
        "for column in df_semCorrelação.columns:\n",
        "    if df_semCorrelação[column].dtypes == 'object':\n",
        "        df_semCorrelação[column] = le.fit_transform(df_semCorrelação[column])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbA42n5jI6Qg",
        "colab_type": "text"
      },
      "source": [
        "# **Categorias das músicas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhyNKTu9IcRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classe = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    classe.append('')\n",
        "\n",
        "for i in range(len(df)):\n",
        "    if df_semCorrelação['amazed-suprised'][i] == 1:\n",
        "        classe[i] = classe[i] + 'suprised'\n",
        "    if df_semCorrelação['happy-pleased'][i] == 1:\n",
        "        classe[i] = classe[i] + 'happy'\n",
        "    if df_semCorrelação['relaxing-calm'][i] == 1:\n",
        "        classe[i] = classe[i] + 'relaxing'    \n",
        "    if df_semCorrelação['quiet-still'][i] == 1:\n",
        "        classe[i] = classe[i] + 'quiet'\n",
        "    if df_semCorrelação['sad-lonely'][i] == 1:\n",
        "        classe[i] = classe[i] + 'sad'\n",
        "    if df_semCorrelação['angry-aggresive'][i] == 1:\n",
        "        classe[i] = classe[i] + 'aggresive'        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w7NTrkWI0df",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_semCorrelação = df_semCorrelação.drop(df_semCorrelação[class_name], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGcTmXIaJczp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_semCorrelação['Classe'] = classe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8TfBEGTJf8n",
        "colab_type": "text"
      },
      "source": [
        "# **Balanceamento**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B--rjVScJdLd",
        "colab_type": "code",
        "outputId": "9947e621-399d-481c-fe11-a968de35e5b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "df_semCorrelação['Classe'].value_counts()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "suprisedaggresive         81\n",
              "happyrelaxing             74\n",
              "aggresive                 72\n",
              "relaxingquietsad          67\n",
              "relaxing                  42\n",
              "suprisedhappy             38\n",
              "quietsad                  37\n",
              "relaxingquiet             30\n",
              "relaxingsad               25\n",
              "suprised                  24\n",
              "happy                     23\n",
              "sad                       12\n",
              "sadaggresive              12\n",
              "suprisedhappyrelaxing     11\n",
              "suprisedhappyaggresive     7\n",
              "happyrelaxingquiet         6\n",
              "suprisedsad                6\n",
              "happyaggresive             5\n",
              "quiet                      5\n",
              "suprisedsadaggresive       4\n",
              "relaxingsadaggresive       3\n",
              "relaxingaggresive          3\n",
              "suprisedrelaxing           2\n",
              "quietsadaggresive          1\n",
              "happysad                   1\n",
              "happyquiet                 1\n",
              "relaxingquietaggresive     1\n",
              "Name: Classe, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Icj5rgGOJj3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = []\n",
        "\n",
        "out =[ \n",
        "'suprisedhappyaggresive',\n",
        "'happyrelaxingquiet',\n",
        "'suprisedsad',\n",
        "'quiet',\n",
        "'happyaggresive',\n",
        "'suprisedsadaggresive',\n",
        "'relaxingsadaggresive',\n",
        "'relaxingaggresive',\n",
        "'suprisedrelaxing',\n",
        "'relaxingquietaggresive',\n",
        "'happyquiet',\n",
        "'quietsadaggresive',\n",
        "'happysad']\n",
        "\n",
        "for i in range(len(df_semCorrelação)):\n",
        "    if df_semCorrelação['Classe'][i] in out:\n",
        "        index.append(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcxId_ONNYkV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_semCorrelação = df_semCorrelação.drop(index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnRr9B8PNwiH",
        "colab_type": "text"
      },
      "source": [
        "# **Oversampling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEOzUEHZNbVF",
        "colab_type": "code",
        "outputId": "f39193fb-d425-4ce1-bdd1-39f3f767ee1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "X = df_semCorrelação[df_semCorrelação.columns[:-1]].values\n",
        "y = df_semCorrelação['Classe'].values\n",
        "\n",
        "ros = RandomOverSampler(random_state=0)\n",
        "ros.fit(X, y)\n",
        "X_resampled, y_resampled = ros.fit_sample(X, y)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctzbjPuENdcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(data = X_resampled , columns = df_semCorrelação.columns[:-1] )\n",
        "df['Classe'] = y_resampled\n",
        "df['Classe'] = le.fit_transform(df['Classe'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8XrAkvAOiKE",
        "colab_type": "text"
      },
      "source": [
        "# **KNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H3-98xUOq7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import cluster, neighbors, svm, metrics, preprocessing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSOIqHv3N9ga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainSample = df.sample(frac=0.8, random_state = 1) #80% da base para treinamento\n",
        "testSample = pd.concat([df, trainSample]).drop_duplicates(keep=False) #concatena a base original com a base de treinamento e exclui os duplicados, sobrando a amostra de teste\n",
        "\n",
        "trainTarget = trainSample[\"Classe\"] #tributo alvo\n",
        "testTarget = testSample[\"Classe\"]\n",
        "\n",
        "del testSample[\"Classe\"]\n",
        "del trainSample[\"Classe\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrJ6GHWdOsSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "knn = neighbors.KNeighborsClassifier(n_neighbors=17, algorithm=\"auto\")\n",
        "knn.fit(trainSample, trainTarget)\n",
        "resultKNN = knn.predict(testSample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLMApvCnYl54",
        "colab_type": "text"
      },
      "source": [
        "# **Medidas de análise de desempenho**\n",
        "\n",
        "  - Medida f1 (Taxa positiva falsa (False Positive Rate))\n",
        "  \n",
        "  - Sensibilidade (Sensitivity) / Recall\n",
        "  - Precisão (Accuracy)\n",
        "  - Sensibilidade\n",
        "  - Especificidade"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPQQMmTCOu3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"\\n ######## K-NN #########\\n\")\n",
        "\n",
        "matrizConfusao = metrics.confusion_matrix(testTarget, resultKNN)\n",
        "print(\"Matriz de Confusão:\\n\", matrizConfusao)\n",
        "\n",
        "TP = matrizConfusao[1, 1] #Verdadeiros Positivos\n",
        "TN = matrizConfusao[0, 0] #Verdadeiros Negativos\n",
        "FP = matrizConfusao[0, 1] #Falsos Positivos\n",
        "FN = matrizConfusao[1, 0] #Falsos Negativos\n",
        "\n",
        "#Formulas obtidas pela matriz de confusão\n",
        "acc = (TP + TN) / float(TP + TN + FP + FN)\n",
        "medf1 = FP / float(TN + FP)\n",
        "sense = metrics.recall_score(testTarget, resultKNN, average='micro')\n",
        "especify = TN / float(TN + FP)\n",
        "\n",
        "print(\"Precisão: \", acc)\n",
        "print(\"Medida F1: \", medf1)\n",
        "print(\"Sensibilidade: %.2f\" % sense)\n",
        "print(\"Especificidade: \", especify)\n",
        "\n",
        "print(\"\\n\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TISGL7-2RspI",
        "colab_type": "text"
      },
      "source": [
        "# **Rede neural**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZUq-Xb4Ow53",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "79313395-d5d6-4a5e-ae46-dff9ba7606d3"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.models import model_from_json"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQlLu3JwRw7z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "4ba82b34-44f7-4e34-a14f-f03cfb994dcb"
      },
      "source": [
        "#Usando 20% para teste\n",
        "test_dataset = df.sample(frac=0.2)\n",
        "\n",
        "train_dataset = df.drop(test_dataset.index)\n",
        "\n",
        "# Classe q sera predita\n",
        "y_train = train_dataset[\"Classe\"]\n",
        "y_test = test_dataset[\"Classe\"]\n",
        "\n",
        "x_train = train_dataset.drop(columns=[\"Classe\"])\n",
        "x_test = test_dataset.drop(columns=[\"Classe\"])\n",
        "\n",
        "x_train.sample()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_Centroid</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_Rolloff</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_Flux</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_0</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_1</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_2</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_3</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_4</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_5</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_6</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_7</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_8</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_9</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_10</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_11</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_12</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_Centroid</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_0</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_1</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_2</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_3</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_8</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_9</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_Centroid</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_Rolloff</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_Flux</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_0</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_1</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_2</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_3</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_4</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_5</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_6</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_7</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_9</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_Centroid</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_Rolloff</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_Flux</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_0</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_1</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_2</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_3</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_4</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_5</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_6</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_7</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_8</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_9</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_10</th>\n",
              "      <th>BH_LowPeakAmp</th>\n",
              "      <th>BH_LowPeakBPM</th>\n",
              "      <th>BH_HighPeakBPM</th>\n",
              "      <th>BH_HighLowRatio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.064168</td>\n",
              "      <td>0.050917</td>\n",
              "      <td>0.046749</td>\n",
              "      <td>-0.040872</td>\n",
              "      <td>0.025324</td>\n",
              "      <td>0.029306</td>\n",
              "      <td>0.044292</td>\n",
              "      <td>0.045983</td>\n",
              "      <td>0.018502</td>\n",
              "      <td>0.068387</td>\n",
              "      <td>0.029492</td>\n",
              "      <td>0.028125</td>\n",
              "      <td>0.023537</td>\n",
              "      <td>0.028562</td>\n",
              "      <td>0.050167</td>\n",
              "      <td>0.033269</td>\n",
              "      <td>0.082657</td>\n",
              "      <td>0.053393</td>\n",
              "      <td>0.060062</td>\n",
              "      <td>0.071759</td>\n",
              "      <td>0.06392</td>\n",
              "      <td>0.046812</td>\n",
              "      <td>0.047189</td>\n",
              "      <td>0.088815</td>\n",
              "      <td>0.065074</td>\n",
              "      <td>0.047503</td>\n",
              "      <td>0.035673</td>\n",
              "      <td>0.034992</td>\n",
              "      <td>0.041784</td>\n",
              "      <td>0.040774</td>\n",
              "      <td>0.052633</td>\n",
              "      <td>0.045993</td>\n",
              "      <td>0.034648</td>\n",
              "      <td>0.026293</td>\n",
              "      <td>0.026717</td>\n",
              "      <td>0.077909</td>\n",
              "      <td>0.040409</td>\n",
              "      <td>0.0395</td>\n",
              "      <td>0.03693</td>\n",
              "      <td>0.048221</td>\n",
              "      <td>0.054539</td>\n",
              "      <td>0.051883</td>\n",
              "      <td>0.060506</td>\n",
              "      <td>0.046829</td>\n",
              "      <td>0.051591</td>\n",
              "      <td>0.051233</td>\n",
              "      <td>0.035645</td>\n",
              "      <td>0.031348</td>\n",
              "      <td>0.036712</td>\n",
              "      <td>0.000943</td>\n",
              "      <td>0.04871</td>\n",
              "      <td>0.046769</td>\n",
              "      <td>0.038684</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Mean_Acc1298_Mean_Mem40_Centroid  ...  BH_HighLowRatio\n",
              "21                          0.064168  ...         0.038684\n",
              "\n",
              "[1 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky006LJXR-d6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "078092ee-e241-41c1-c385-32f8c75672ec"
      },
      "source": [
        "# Definindo modelo\n",
        "model = Sequential()\n",
        "model.add(Dense(38, activation='relu', input_dim = 53))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(14, activation='softmax'))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUpDvxcTSAys",
        "colab_type": "code",
        "outputId": "56bb33e9-980f-46f6-d212-28075407cbbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# compile the keras model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(x_train,y_train, epochs=1500, batch_size=20, validation_data=(x_test, y_test))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 907 samples, validate on 227 samples\n",
            "Epoch 1/1500\n",
            "907/907 [==============================] - 0s 456us/sample - loss: 2.6413 - acc: 0.0639 - val_loss: 2.6404 - val_acc: 0.0529\n",
            "Epoch 2/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 2.6370 - acc: 0.0838 - val_loss: 2.6377 - val_acc: 0.0396\n",
            "Epoch 3/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 2.6306 - acc: 0.0926 - val_loss: 2.6288 - val_acc: 0.0881\n",
            "Epoch 4/1500\n",
            "907/907 [==============================] - 0s 131us/sample - loss: 2.6228 - acc: 0.0849 - val_loss: 2.6077 - val_acc: 0.0881\n",
            "Epoch 5/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 2.5931 - acc: 0.1147 - val_loss: 2.5575 - val_acc: 0.0837\n",
            "Epoch 6/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 2.5306 - acc: 0.1279 - val_loss: 2.4516 - val_acc: 0.1498\n",
            "Epoch 7/1500\n",
            "907/907 [==============================] - 0s 131us/sample - loss: 2.4604 - acc: 0.1433 - val_loss: 2.3633 - val_acc: 0.1498\n",
            "Epoch 8/1500\n",
            "907/907 [==============================] - 0s 145us/sample - loss: 2.3863 - acc: 0.1510 - val_loss: 2.2942 - val_acc: 0.1674\n",
            "Epoch 9/1500\n",
            "907/907 [==============================] - 0s 156us/sample - loss: 2.3467 - acc: 0.1544 - val_loss: 2.2507 - val_acc: 0.1542\n",
            "Epoch 10/1500\n",
            "907/907 [==============================] - 0s 156us/sample - loss: 2.3200 - acc: 0.1786 - val_loss: 2.2227 - val_acc: 0.1894\n",
            "Epoch 11/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 2.3187 - acc: 0.1786 - val_loss: 2.2126 - val_acc: 0.2159\n",
            "Epoch 12/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 2.2915 - acc: 0.1929 - val_loss: 2.1933 - val_acc: 0.1982\n",
            "Epoch 13/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 2.2801 - acc: 0.1654 - val_loss: 2.1742 - val_acc: 0.1674\n",
            "Epoch 14/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 2.2801 - acc: 0.2073 - val_loss: 2.1602 - val_acc: 0.2115\n",
            "Epoch 15/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 2.2639 - acc: 0.2084 - val_loss: 2.1562 - val_acc: 0.2423\n",
            "Epoch 16/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 2.2285 - acc: 0.2073 - val_loss: 2.1504 - val_acc: 0.2115\n",
            "Epoch 17/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 2.2186 - acc: 0.2139 - val_loss: 2.1274 - val_acc: 0.2159\n",
            "Epoch 18/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 2.2057 - acc: 0.2095 - val_loss: 2.1334 - val_acc: 0.2423\n",
            "Epoch 19/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 2.1982 - acc: 0.2227 - val_loss: 2.1233 - val_acc: 0.2070\n",
            "Epoch 20/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 2.1697 - acc: 0.2205 - val_loss: 2.1021 - val_acc: 0.2379\n",
            "Epoch 21/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 2.1648 - acc: 0.2150 - val_loss: 2.1010 - val_acc: 0.2247\n",
            "Epoch 22/1500\n",
            "907/907 [==============================] - 0s 139us/sample - loss: 2.1644 - acc: 0.2249 - val_loss: 2.0802 - val_acc: 0.1982\n",
            "Epoch 23/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 2.1517 - acc: 0.2426 - val_loss: 2.0781 - val_acc: 0.2599\n",
            "Epoch 24/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 2.1378 - acc: 0.2393 - val_loss: 2.0704 - val_acc: 0.2379\n",
            "Epoch 25/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 2.1302 - acc: 0.2514 - val_loss: 2.0758 - val_acc: 0.2291\n",
            "Epoch 26/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 2.1151 - acc: 0.2679 - val_loss: 2.0504 - val_acc: 0.2379\n",
            "Epoch 27/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 2.0913 - acc: 0.2481 - val_loss: 2.0390 - val_acc: 0.2555\n",
            "Epoch 28/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 2.0854 - acc: 0.2613 - val_loss: 2.0510 - val_acc: 0.2643\n",
            "Epoch 29/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 2.0869 - acc: 0.2558 - val_loss: 2.0402 - val_acc: 0.2203\n",
            "Epoch 30/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 2.0995 - acc: 0.2381 - val_loss: 2.0283 - val_acc: 0.2335\n",
            "Epoch 31/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 2.0553 - acc: 0.2657 - val_loss: 2.0083 - val_acc: 0.2599\n",
            "Epoch 32/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 2.0615 - acc: 0.2690 - val_loss: 2.0101 - val_acc: 0.2511\n",
            "Epoch 33/1500\n",
            "907/907 [==============================] - 0s 137us/sample - loss: 2.0637 - acc: 0.2558 - val_loss: 2.0015 - val_acc: 0.2599\n",
            "Epoch 34/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 2.0480 - acc: 0.2602 - val_loss: 1.9920 - val_acc: 0.2775\n",
            "Epoch 35/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 2.0453 - acc: 0.2668 - val_loss: 1.9763 - val_acc: 0.2819\n",
            "Epoch 36/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 2.0142 - acc: 0.2834 - val_loss: 1.9853 - val_acc: 0.2643\n",
            "Epoch 37/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 2.0244 - acc: 0.2922 - val_loss: 1.9880 - val_acc: 0.2555\n",
            "Epoch 38/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 2.0164 - acc: 0.2646 - val_loss: 1.9536 - val_acc: 0.2731\n",
            "Epoch 39/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 2.0080 - acc: 0.2613 - val_loss: 1.9555 - val_acc: 0.2599\n",
            "Epoch 40/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.9952 - acc: 0.3098 - val_loss: 1.9525 - val_acc: 0.2555\n",
            "Epoch 41/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 2.0106 - acc: 0.2679 - val_loss: 1.9378 - val_acc: 0.2599\n",
            "Epoch 42/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.9673 - acc: 0.2856 - val_loss: 1.9453 - val_acc: 0.2335\n",
            "Epoch 43/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 2.0031 - acc: 0.2723 - val_loss: 1.9346 - val_acc: 0.2819\n",
            "Epoch 44/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.9824 - acc: 0.2988 - val_loss: 1.9107 - val_acc: 0.3128\n",
            "Epoch 45/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.9504 - acc: 0.2966 - val_loss: 1.9051 - val_acc: 0.2599\n",
            "Epoch 46/1500\n",
            "907/907 [==============================] - 0s 107us/sample - loss: 1.9728 - acc: 0.2878 - val_loss: 1.9059 - val_acc: 0.2907\n",
            "Epoch 47/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.9630 - acc: 0.2800 - val_loss: 1.8914 - val_acc: 0.2907\n",
            "Epoch 48/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.9238 - acc: 0.2690 - val_loss: 1.8788 - val_acc: 0.3084\n",
            "Epoch 49/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.9130 - acc: 0.3065 - val_loss: 1.8880 - val_acc: 0.2467\n",
            "Epoch 50/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.9432 - acc: 0.2977 - val_loss: 1.8652 - val_acc: 0.3172\n",
            "Epoch 51/1500\n",
            "907/907 [==============================] - 0s 138us/sample - loss: 1.9081 - acc: 0.3021 - val_loss: 1.8655 - val_acc: 0.2996\n",
            "Epoch 52/1500\n",
            "907/907 [==============================] - 0s 155us/sample - loss: 1.9217 - acc: 0.2966 - val_loss: 1.8618 - val_acc: 0.3172\n",
            "Epoch 53/1500\n",
            "907/907 [==============================] - 0s 136us/sample - loss: 1.9643 - acc: 0.2834 - val_loss: 1.8698 - val_acc: 0.3040\n",
            "Epoch 54/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.8958 - acc: 0.3164 - val_loss: 1.8556 - val_acc: 0.3128\n",
            "Epoch 55/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.8846 - acc: 0.3175 - val_loss: 1.8366 - val_acc: 0.2952\n",
            "Epoch 56/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.9037 - acc: 0.3131 - val_loss: 1.8507 - val_acc: 0.2907\n",
            "Epoch 57/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.8918 - acc: 0.3230 - val_loss: 1.8478 - val_acc: 0.3040\n",
            "Epoch 58/1500\n",
            "907/907 [==============================] - 0s 109us/sample - loss: 1.9073 - acc: 0.3197 - val_loss: 1.8157 - val_acc: 0.3216\n",
            "Epoch 59/1500\n",
            "907/907 [==============================] - 0s 137us/sample - loss: 1.9036 - acc: 0.3241 - val_loss: 1.8202 - val_acc: 0.3172\n",
            "Epoch 60/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.8664 - acc: 0.3120 - val_loss: 1.8102 - val_acc: 0.3128\n",
            "Epoch 61/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.8885 - acc: 0.3418 - val_loss: 1.8040 - val_acc: 0.3128\n",
            "Epoch 62/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.8628 - acc: 0.3153 - val_loss: 1.7973 - val_acc: 0.3216\n",
            "Epoch 63/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.8986 - acc: 0.3120 - val_loss: 1.8041 - val_acc: 0.3172\n",
            "Epoch 64/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.8481 - acc: 0.3495 - val_loss: 1.8001 - val_acc: 0.3084\n",
            "Epoch 65/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.8345 - acc: 0.3418 - val_loss: 1.7851 - val_acc: 0.3524\n",
            "Epoch 66/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 1.8522 - acc: 0.3286 - val_loss: 1.7884 - val_acc: 0.3480\n",
            "Epoch 67/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.8317 - acc: 0.3363 - val_loss: 1.7647 - val_acc: 0.3524\n",
            "Epoch 68/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.7946 - acc: 0.3197 - val_loss: 1.7848 - val_acc: 0.3304\n",
            "Epoch 69/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 1.8601 - acc: 0.3396 - val_loss: 1.7598 - val_acc: 0.3260\n",
            "Epoch 70/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.8278 - acc: 0.3429 - val_loss: 1.7589 - val_acc: 0.3216\n",
            "Epoch 71/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 1.8320 - acc: 0.3462 - val_loss: 1.7447 - val_acc: 0.3436\n",
            "Epoch 72/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.7871 - acc: 0.3561 - val_loss: 1.7294 - val_acc: 0.3568\n",
            "Epoch 73/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.8047 - acc: 0.3330 - val_loss: 1.7429 - val_acc: 0.3348\n",
            "Epoch 74/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.8149 - acc: 0.3638 - val_loss: 1.7167 - val_acc: 0.3700\n",
            "Epoch 75/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.7852 - acc: 0.3583 - val_loss: 1.7258 - val_acc: 0.3480\n",
            "Epoch 76/1500\n",
            "907/907 [==============================] - 0s 105us/sample - loss: 1.7567 - acc: 0.3682 - val_loss: 1.7217 - val_acc: 0.3568\n",
            "Epoch 77/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.7789 - acc: 0.3539 - val_loss: 1.7632 - val_acc: 0.3524\n",
            "Epoch 78/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.7973 - acc: 0.3473 - val_loss: 1.7188 - val_acc: 0.3524\n",
            "Epoch 79/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.7984 - acc: 0.3473 - val_loss: 1.7039 - val_acc: 0.3568\n",
            "Epoch 80/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.7653 - acc: 0.3473 - val_loss: 1.6932 - val_acc: 0.3921\n",
            "Epoch 81/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.7394 - acc: 0.3693 - val_loss: 1.6949 - val_acc: 0.3789\n",
            "Epoch 82/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.7872 - acc: 0.3572 - val_loss: 1.6908 - val_acc: 0.3877\n",
            "Epoch 83/1500\n",
            "907/907 [==============================] - 0s 131us/sample - loss: 1.7390 - acc: 0.3771 - val_loss: 1.6766 - val_acc: 0.3789\n",
            "Epoch 84/1500\n",
            "907/907 [==============================] - 0s 136us/sample - loss: 1.7173 - acc: 0.3738 - val_loss: 1.6790 - val_acc: 0.3789\n",
            "Epoch 85/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.7530 - acc: 0.3682 - val_loss: 1.6588 - val_acc: 0.4053\n",
            "Epoch 86/1500\n",
            "907/907 [==============================] - 0s 149us/sample - loss: 1.7516 - acc: 0.3671 - val_loss: 1.6863 - val_acc: 0.3965\n",
            "Epoch 87/1500\n",
            "907/907 [==============================] - 0s 139us/sample - loss: 1.7585 - acc: 0.3561 - val_loss: 1.6691 - val_acc: 0.3833\n",
            "Epoch 88/1500\n",
            "907/907 [==============================] - 0s 147us/sample - loss: 1.7237 - acc: 0.3727 - val_loss: 1.6591 - val_acc: 0.3789\n",
            "Epoch 89/1500\n",
            "907/907 [==============================] - 0s 132us/sample - loss: 1.7269 - acc: 0.3528 - val_loss: 1.6540 - val_acc: 0.3833\n",
            "Epoch 90/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.7266 - acc: 0.3870 - val_loss: 1.6400 - val_acc: 0.4009\n",
            "Epoch 91/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.7164 - acc: 0.3782 - val_loss: 1.6606 - val_acc: 0.3700\n",
            "Epoch 92/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.7131 - acc: 0.3716 - val_loss: 1.6398 - val_acc: 0.4097\n",
            "Epoch 93/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.7321 - acc: 0.3682 - val_loss: 1.6360 - val_acc: 0.3921\n",
            "Epoch 94/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.6966 - acc: 0.3848 - val_loss: 1.6254 - val_acc: 0.4053\n",
            "Epoch 95/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.7220 - acc: 0.3638 - val_loss: 1.6136 - val_acc: 0.4053\n",
            "Epoch 96/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.6886 - acc: 0.3837 - val_loss: 1.6352 - val_acc: 0.3877\n",
            "Epoch 97/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.7025 - acc: 0.3826 - val_loss: 1.6438 - val_acc: 0.3789\n",
            "Epoch 98/1500\n",
            "907/907 [==============================] - 0s 142us/sample - loss: 1.6700 - acc: 0.3958 - val_loss: 1.6376 - val_acc: 0.3921\n",
            "Epoch 99/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.6943 - acc: 0.3550 - val_loss: 1.6119 - val_acc: 0.4141\n",
            "Epoch 100/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.6669 - acc: 0.3991 - val_loss: 1.5988 - val_acc: 0.4097\n",
            "Epoch 101/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.6749 - acc: 0.3892 - val_loss: 1.5954 - val_acc: 0.4229\n",
            "Epoch 102/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.7245 - acc: 0.3660 - val_loss: 1.5913 - val_acc: 0.4185\n",
            "Epoch 103/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.6810 - acc: 0.3782 - val_loss: 1.6057 - val_acc: 0.4053\n",
            "Epoch 104/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.6649 - acc: 0.4135 - val_loss: 1.5799 - val_acc: 0.4229\n",
            "Epoch 105/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.7090 - acc: 0.3705 - val_loss: 1.5828 - val_acc: 0.4185\n",
            "Epoch 106/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.6635 - acc: 0.3804 - val_loss: 1.5799 - val_acc: 0.4185\n",
            "Epoch 107/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.6607 - acc: 0.3870 - val_loss: 1.5737 - val_acc: 0.4185\n",
            "Epoch 108/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.6957 - acc: 0.3804 - val_loss: 1.5639 - val_acc: 0.4273\n",
            "Epoch 109/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.6838 - acc: 0.3837 - val_loss: 1.5546 - val_acc: 0.4141\n",
            "Epoch 110/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 1.6490 - acc: 0.3815 - val_loss: 1.5515 - val_acc: 0.4097\n",
            "Epoch 111/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.6452 - acc: 0.3903 - val_loss: 1.5428 - val_acc: 0.4626\n",
            "Epoch 112/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.6638 - acc: 0.3727 - val_loss: 1.5582 - val_acc: 0.4273\n",
            "Epoch 113/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.6576 - acc: 0.3903 - val_loss: 1.5510 - val_acc: 0.4670\n",
            "Epoch 114/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.6286 - acc: 0.4179 - val_loss: 1.5508 - val_acc: 0.4493\n",
            "Epoch 115/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.6560 - acc: 0.3925 - val_loss: 1.5357 - val_acc: 0.4449\n",
            "Epoch 116/1500\n",
            "907/907 [==============================] - 0s 131us/sample - loss: 1.5987 - acc: 0.4079 - val_loss: 1.5390 - val_acc: 0.4229\n",
            "Epoch 117/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.6305 - acc: 0.4112 - val_loss: 1.5452 - val_acc: 0.4405\n",
            "Epoch 118/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.6450 - acc: 0.3738 - val_loss: 1.5164 - val_acc: 0.4537\n",
            "Epoch 119/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.6380 - acc: 0.4168 - val_loss: 1.5231 - val_acc: 0.4493\n",
            "Epoch 120/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.6047 - acc: 0.4267 - val_loss: 1.5271 - val_acc: 0.4361\n",
            "Epoch 121/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.6400 - acc: 0.3969 - val_loss: 1.5164 - val_acc: 0.4890\n",
            "Epoch 122/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.6683 - acc: 0.3881 - val_loss: 1.5168 - val_acc: 0.4670\n",
            "Epoch 123/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.6372 - acc: 0.4090 - val_loss: 1.5397 - val_acc: 0.4537\n",
            "Epoch 124/1500\n",
            "907/907 [==============================] - 0s 136us/sample - loss: 1.6121 - acc: 0.4157 - val_loss: 1.5311 - val_acc: 0.4670\n",
            "Epoch 125/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.6199 - acc: 0.4101 - val_loss: 1.5251 - val_acc: 0.4229\n",
            "Epoch 126/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.6093 - acc: 0.4123 - val_loss: 1.4907 - val_acc: 0.4581\n",
            "Epoch 127/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.5914 - acc: 0.4223 - val_loss: 1.4957 - val_acc: 0.4581\n",
            "Epoch 128/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.5769 - acc: 0.4234 - val_loss: 1.5059 - val_acc: 0.4317\n",
            "Epoch 129/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.5594 - acc: 0.4443 - val_loss: 1.4765 - val_acc: 0.4802\n",
            "Epoch 130/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.5834 - acc: 0.3925 - val_loss: 1.4853 - val_acc: 0.4405\n",
            "Epoch 131/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.5798 - acc: 0.3903 - val_loss: 1.4815 - val_acc: 0.4581\n",
            "Epoch 132/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.5608 - acc: 0.4432 - val_loss: 1.4670 - val_acc: 0.4449\n",
            "Epoch 133/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.5421 - acc: 0.4278 - val_loss: 1.4707 - val_acc: 0.4493\n",
            "Epoch 134/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.5989 - acc: 0.4057 - val_loss: 1.4743 - val_acc: 0.4449\n",
            "Epoch 135/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.5741 - acc: 0.4278 - val_loss: 1.4563 - val_acc: 0.4581\n",
            "Epoch 136/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.5894 - acc: 0.4157 - val_loss: 1.4677 - val_acc: 0.4846\n",
            "Epoch 137/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.5713 - acc: 0.4289 - val_loss: 1.4610 - val_acc: 0.4714\n",
            "Epoch 138/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.5773 - acc: 0.4245 - val_loss: 1.4548 - val_acc: 0.4626\n",
            "Epoch 139/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.5523 - acc: 0.4322 - val_loss: 1.4435 - val_acc: 0.4846\n",
            "Epoch 140/1500\n",
            "907/907 [==============================] - 0s 133us/sample - loss: 1.5620 - acc: 0.4112 - val_loss: 1.4541 - val_acc: 0.4405\n",
            "Epoch 141/1500\n",
            "907/907 [==============================] - 0s 140us/sample - loss: 1.5808 - acc: 0.4135 - val_loss: 1.4411 - val_acc: 0.4449\n",
            "Epoch 142/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.5920 - acc: 0.4101 - val_loss: 1.4566 - val_acc: 0.4846\n",
            "Epoch 143/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.5601 - acc: 0.4333 - val_loss: 1.4373 - val_acc: 0.4758\n",
            "Epoch 144/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.5375 - acc: 0.4454 - val_loss: 1.4396 - val_acc: 0.4581\n",
            "Epoch 145/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.5699 - acc: 0.4112 - val_loss: 1.4564 - val_acc: 0.4802\n",
            "Epoch 146/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 1.5625 - acc: 0.4245 - val_loss: 1.4264 - val_acc: 0.4890\n",
            "Epoch 147/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.5675 - acc: 0.4212 - val_loss: 1.4445 - val_acc: 0.4758\n",
            "Epoch 148/1500\n",
            "907/907 [==============================] - 0s 108us/sample - loss: 1.5691 - acc: 0.4157 - val_loss: 1.4246 - val_acc: 0.4670\n",
            "Epoch 149/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.5521 - acc: 0.4289 - val_loss: 1.4447 - val_acc: 0.4802\n",
            "Epoch 150/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.5286 - acc: 0.4399 - val_loss: 1.4087 - val_acc: 0.4670\n",
            "Epoch 151/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.5372 - acc: 0.4289 - val_loss: 1.4795 - val_acc: 0.4626\n",
            "Epoch 152/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.5674 - acc: 0.4223 - val_loss: 1.4300 - val_acc: 0.4890\n",
            "Epoch 153/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.5566 - acc: 0.4256 - val_loss: 1.4188 - val_acc: 0.4802\n",
            "Epoch 154/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.5023 - acc: 0.4432 - val_loss: 1.4084 - val_acc: 0.4802\n",
            "Epoch 155/1500\n",
            "907/907 [==============================] - 0s 105us/sample - loss: 1.5008 - acc: 0.4333 - val_loss: 1.4105 - val_acc: 0.5066\n",
            "Epoch 156/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.5070 - acc: 0.4476 - val_loss: 1.4117 - val_acc: 0.5154\n",
            "Epoch 157/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 1.5196 - acc: 0.4267 - val_loss: 1.3935 - val_acc: 0.4802\n",
            "Epoch 158/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.5142 - acc: 0.4476 - val_loss: 1.4293 - val_acc: 0.4978\n",
            "Epoch 159/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.5345 - acc: 0.4432 - val_loss: 1.3968 - val_acc: 0.5198\n",
            "Epoch 160/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.5151 - acc: 0.4311 - val_loss: 1.3894 - val_acc: 0.4890\n",
            "Epoch 161/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.5178 - acc: 0.4399 - val_loss: 1.4046 - val_acc: 0.4626\n",
            "Epoch 162/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.5196 - acc: 0.4168 - val_loss: 1.3966 - val_acc: 0.4714\n",
            "Epoch 163/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.5071 - acc: 0.4432 - val_loss: 1.3730 - val_acc: 0.4846\n",
            "Epoch 164/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.5425 - acc: 0.4333 - val_loss: 1.3910 - val_acc: 0.4758\n",
            "Epoch 165/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.5147 - acc: 0.4267 - val_loss: 1.3880 - val_acc: 0.4978\n",
            "Epoch 166/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.5270 - acc: 0.4509 - val_loss: 1.3831 - val_acc: 0.4846\n",
            "Epoch 167/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.5116 - acc: 0.4454 - val_loss: 1.3928 - val_acc: 0.4846\n",
            "Epoch 168/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.5075 - acc: 0.4587 - val_loss: 1.3557 - val_acc: 0.5330\n",
            "Epoch 169/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.4890 - acc: 0.4686 - val_loss: 1.3798 - val_acc: 0.4934\n",
            "Epoch 170/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.5084 - acc: 0.4542 - val_loss: 1.3708 - val_acc: 0.5154\n",
            "Epoch 171/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.4784 - acc: 0.4487 - val_loss: 1.3827 - val_acc: 0.5110\n",
            "Epoch 172/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.4736 - acc: 0.4344 - val_loss: 1.3529 - val_acc: 0.5110\n",
            "Epoch 173/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.5328 - acc: 0.4355 - val_loss: 1.3715 - val_acc: 0.4890\n",
            "Epoch 174/1500\n",
            "907/907 [==============================] - 0s 145us/sample - loss: 1.4940 - acc: 0.4322 - val_loss: 1.3687 - val_acc: 0.5022\n",
            "Epoch 175/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.4907 - acc: 0.4576 - val_loss: 1.3622 - val_acc: 0.4890\n",
            "Epoch 176/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.4946 - acc: 0.4509 - val_loss: 1.3661 - val_acc: 0.4890\n",
            "Epoch 177/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.5153 - acc: 0.4487 - val_loss: 1.3446 - val_acc: 0.5154\n",
            "Epoch 178/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.4566 - acc: 0.4564 - val_loss: 1.4023 - val_acc: 0.4714\n",
            "Epoch 179/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.5122 - acc: 0.4267 - val_loss: 1.3916 - val_acc: 0.4802\n",
            "Epoch 180/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.4746 - acc: 0.4642 - val_loss: 1.3558 - val_acc: 0.5066\n",
            "Epoch 181/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.4698 - acc: 0.4620 - val_loss: 1.3644 - val_acc: 0.5198\n",
            "Epoch 182/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.4765 - acc: 0.4509 - val_loss: 1.3659 - val_acc: 0.5066\n",
            "Epoch 183/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.4875 - acc: 0.4509 - val_loss: 1.3552 - val_acc: 0.5154\n",
            "Epoch 184/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.4800 - acc: 0.4642 - val_loss: 1.3337 - val_acc: 0.5022\n",
            "Epoch 185/1500\n",
            "907/907 [==============================] - 0s 139us/sample - loss: 1.4752 - acc: 0.4653 - val_loss: 1.3325 - val_acc: 0.5154\n",
            "Epoch 186/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.4412 - acc: 0.4564 - val_loss: 1.3571 - val_acc: 0.5022\n",
            "Epoch 187/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.4606 - acc: 0.4664 - val_loss: 1.3405 - val_acc: 0.4978\n",
            "Epoch 188/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.4551 - acc: 0.4553 - val_loss: 1.3064 - val_acc: 0.5198\n",
            "Epoch 189/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.4898 - acc: 0.4476 - val_loss: 1.3443 - val_acc: 0.5154\n",
            "Epoch 190/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.4530 - acc: 0.4697 - val_loss: 1.3153 - val_acc: 0.5198\n",
            "Epoch 191/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.4483 - acc: 0.4730 - val_loss: 1.3184 - val_acc: 0.5330\n",
            "Epoch 192/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.4381 - acc: 0.4598 - val_loss: 1.3224 - val_acc: 0.5242\n",
            "Epoch 193/1500\n",
            "907/907 [==============================] - 0s 131us/sample - loss: 1.4726 - acc: 0.4598 - val_loss: 1.3161 - val_acc: 0.5242\n",
            "Epoch 194/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.4786 - acc: 0.4730 - val_loss: 1.3123 - val_acc: 0.5110\n",
            "Epoch 195/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.4489 - acc: 0.4752 - val_loss: 1.3122 - val_acc: 0.5286\n",
            "Epoch 196/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.4453 - acc: 0.4741 - val_loss: 1.3687 - val_acc: 0.4714\n",
            "Epoch 197/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.4779 - acc: 0.4399 - val_loss: 1.3562 - val_acc: 0.4978\n",
            "Epoch 198/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.4288 - acc: 0.4476 - val_loss: 1.3185 - val_acc: 0.5286\n",
            "Epoch 199/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.4967 - acc: 0.4410 - val_loss: 1.3144 - val_acc: 0.5198\n",
            "Epoch 200/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.4698 - acc: 0.4509 - val_loss: 1.3100 - val_acc: 0.5242\n",
            "Epoch 201/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.4690 - acc: 0.4476 - val_loss: 1.3178 - val_acc: 0.5022\n",
            "Epoch 202/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.4382 - acc: 0.4862 - val_loss: 1.3055 - val_acc: 0.5463\n",
            "Epoch 203/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.4461 - acc: 0.4598 - val_loss: 1.3210 - val_acc: 0.5110\n",
            "Epoch 204/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.4534 - acc: 0.4564 - val_loss: 1.3086 - val_acc: 0.5330\n",
            "Epoch 205/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.4604 - acc: 0.4564 - val_loss: 1.3563 - val_acc: 0.5066\n",
            "Epoch 206/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.4286 - acc: 0.4642 - val_loss: 1.3295 - val_acc: 0.5022\n",
            "Epoch 207/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.3989 - acc: 0.4895 - val_loss: 1.3184 - val_acc: 0.5022\n",
            "Epoch 208/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.4336 - acc: 0.4774 - val_loss: 1.3462 - val_acc: 0.5242\n",
            "Epoch 209/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.4375 - acc: 0.4664 - val_loss: 1.3265 - val_acc: 0.5110\n",
            "Epoch 210/1500\n",
            "907/907 [==============================] - 0s 144us/sample - loss: 1.4340 - acc: 0.4774 - val_loss: 1.3292 - val_acc: 0.5022\n",
            "Epoch 211/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.4317 - acc: 0.4752 - val_loss: 1.3717 - val_acc: 0.5242\n",
            "Epoch 212/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.4717 - acc: 0.4542 - val_loss: 1.3271 - val_acc: 0.4890\n",
            "Epoch 213/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.4444 - acc: 0.4730 - val_loss: 1.3123 - val_acc: 0.5154\n",
            "Epoch 214/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.4520 - acc: 0.4785 - val_loss: 1.3328 - val_acc: 0.5286\n",
            "Epoch 215/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.4012 - acc: 0.4675 - val_loss: 1.2945 - val_acc: 0.5419\n",
            "Epoch 216/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.4002 - acc: 0.4840 - val_loss: 1.3211 - val_acc: 0.4846\n",
            "Epoch 217/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.4736 - acc: 0.4620 - val_loss: 1.3267 - val_acc: 0.5110\n",
            "Epoch 218/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.4094 - acc: 0.4730 - val_loss: 1.3344 - val_acc: 0.5198\n",
            "Epoch 219/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.4228 - acc: 0.4796 - val_loss: 1.3286 - val_acc: 0.5110\n",
            "Epoch 220/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.4219 - acc: 0.4642 - val_loss: 1.3076 - val_acc: 0.5110\n",
            "Epoch 221/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.3854 - acc: 0.5138 - val_loss: 1.2928 - val_acc: 0.5198\n",
            "Epoch 222/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.4496 - acc: 0.4719 - val_loss: 1.3080 - val_acc: 0.5374\n",
            "Epoch 223/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.4206 - acc: 0.4730 - val_loss: 1.3376 - val_acc: 0.5154\n",
            "Epoch 224/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.4618 - acc: 0.4498 - val_loss: 1.3059 - val_acc: 0.5595\n",
            "Epoch 225/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.4337 - acc: 0.4719 - val_loss: 1.3197 - val_acc: 0.5463\n",
            "Epoch 226/1500\n",
            "907/907 [==============================] - 0s 109us/sample - loss: 1.4048 - acc: 0.4851 - val_loss: 1.3079 - val_acc: 0.4978\n",
            "Epoch 227/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 1.4379 - acc: 0.4730 - val_loss: 1.2996 - val_acc: 0.5419\n",
            "Epoch 228/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.4261 - acc: 0.4719 - val_loss: 1.3137 - val_acc: 0.5286\n",
            "Epoch 229/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.4270 - acc: 0.4785 - val_loss: 1.3215 - val_acc: 0.5374\n",
            "Epoch 230/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.4215 - acc: 0.4730 - val_loss: 1.3150 - val_acc: 0.5242\n",
            "Epoch 231/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.4198 - acc: 0.4796 - val_loss: 1.2772 - val_acc: 0.5463\n",
            "Epoch 232/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.4189 - acc: 0.4807 - val_loss: 1.3124 - val_acc: 0.5330\n",
            "Epoch 233/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 1.3921 - acc: 0.4741 - val_loss: 1.2586 - val_acc: 0.5595\n",
            "Epoch 234/1500\n",
            "907/907 [==============================] - 0s 109us/sample - loss: 1.3910 - acc: 0.4906 - val_loss: 1.2752 - val_acc: 0.5330\n",
            "Epoch 235/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.3925 - acc: 0.4796 - val_loss: 1.2907 - val_acc: 0.5507\n",
            "Epoch 236/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 1.4044 - acc: 0.4862 - val_loss: 1.2729 - val_acc: 0.5419\n",
            "Epoch 237/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.3775 - acc: 0.4895 - val_loss: 1.2771 - val_acc: 0.5242\n",
            "Epoch 238/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 1.3789 - acc: 0.4840 - val_loss: 1.2737 - val_acc: 0.5683\n",
            "Epoch 239/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.3923 - acc: 0.4741 - val_loss: 1.2642 - val_acc: 0.5595\n",
            "Epoch 240/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.3945 - acc: 0.4840 - val_loss: 1.2601 - val_acc: 0.5639\n",
            "Epoch 241/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.4150 - acc: 0.4564 - val_loss: 1.2878 - val_acc: 0.5419\n",
            "Epoch 242/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 1.3820 - acc: 0.4873 - val_loss: 1.2656 - val_acc: 0.5595\n",
            "Epoch 243/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.3931 - acc: 0.4862 - val_loss: 1.2654 - val_acc: 0.5419\n",
            "Epoch 244/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.3926 - acc: 0.4708 - val_loss: 1.2655 - val_acc: 0.5463\n",
            "Epoch 245/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.4044 - acc: 0.4939 - val_loss: 1.2749 - val_acc: 0.5419\n",
            "Epoch 246/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.3511 - acc: 0.5182 - val_loss: 1.2693 - val_acc: 0.5727\n",
            "Epoch 247/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.3877 - acc: 0.4642 - val_loss: 1.2555 - val_acc: 0.5595\n",
            "Epoch 248/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.3709 - acc: 0.4939 - val_loss: 1.2737 - val_acc: 0.5242\n",
            "Epoch 249/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.3820 - acc: 0.4895 - val_loss: 1.2535 - val_acc: 0.5727\n",
            "Epoch 250/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.3832 - acc: 0.4895 - val_loss: 1.2702 - val_acc: 0.5419\n",
            "Epoch 251/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.3723 - acc: 0.5138 - val_loss: 1.2756 - val_acc: 0.5242\n",
            "Epoch 252/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.4342 - acc: 0.4785 - val_loss: 1.2590 - val_acc: 0.5330\n",
            "Epoch 253/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.3612 - acc: 0.5050 - val_loss: 1.2662 - val_acc: 0.5419\n",
            "Epoch 254/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.3796 - acc: 0.4796 - val_loss: 1.2688 - val_acc: 0.5374\n",
            "Epoch 255/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 1.4126 - acc: 0.4697 - val_loss: 1.2740 - val_acc: 0.5110\n",
            "Epoch 256/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.3819 - acc: 0.4719 - val_loss: 1.3038 - val_acc: 0.5286\n",
            "Epoch 257/1500\n",
            "907/907 [==============================] - 0s 146us/sample - loss: 1.3480 - acc: 0.5149 - val_loss: 1.2419 - val_acc: 0.5595\n",
            "Epoch 258/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.4010 - acc: 0.4697 - val_loss: 1.2238 - val_acc: 0.5683\n",
            "Epoch 259/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.3885 - acc: 0.4785 - val_loss: 1.2369 - val_acc: 0.5507\n",
            "Epoch 260/1500\n",
            "907/907 [==============================] - 0s 108us/sample - loss: 1.3199 - acc: 0.5116 - val_loss: 1.2556 - val_acc: 0.5507\n",
            "Epoch 261/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.3992 - acc: 0.4719 - val_loss: 1.2358 - val_acc: 0.5595\n",
            "Epoch 262/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.3628 - acc: 0.4895 - val_loss: 1.2476 - val_acc: 0.5242\n",
            "Epoch 263/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.3624 - acc: 0.4961 - val_loss: 1.2341 - val_acc: 0.5374\n",
            "Epoch 264/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.3468 - acc: 0.5061 - val_loss: 1.2563 - val_acc: 0.5551\n",
            "Epoch 265/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.3297 - acc: 0.5314 - val_loss: 1.2269 - val_acc: 0.5551\n",
            "Epoch 266/1500\n",
            "907/907 [==============================] - 0s 134us/sample - loss: 1.3808 - acc: 0.4961 - val_loss: 1.2687 - val_acc: 0.5242\n",
            "Epoch 267/1500\n",
            "907/907 [==============================] - 0s 107us/sample - loss: 1.3387 - acc: 0.5116 - val_loss: 1.2402 - val_acc: 0.5551\n",
            "Epoch 268/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.3802 - acc: 0.4895 - val_loss: 1.2532 - val_acc: 0.5242\n",
            "Epoch 269/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.3771 - acc: 0.4763 - val_loss: 1.2710 - val_acc: 0.5639\n",
            "Epoch 270/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.3441 - acc: 0.4950 - val_loss: 1.3277 - val_acc: 0.5330\n",
            "Epoch 271/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.3909 - acc: 0.5017 - val_loss: 1.2512 - val_acc: 0.5419\n",
            "Epoch 272/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.3520 - acc: 0.4983 - val_loss: 1.2414 - val_acc: 0.5727\n",
            "Epoch 273/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.3358 - acc: 0.5160 - val_loss: 1.2398 - val_acc: 0.5198\n",
            "Epoch 274/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.3604 - acc: 0.4950 - val_loss: 1.2585 - val_acc: 0.5374\n",
            "Epoch 275/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.3368 - acc: 0.5061 - val_loss: 1.2586 - val_acc: 0.5551\n",
            "Epoch 276/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.3538 - acc: 0.5116 - val_loss: 1.2479 - val_acc: 0.5639\n",
            "Epoch 277/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.3341 - acc: 0.5105 - val_loss: 1.2334 - val_acc: 0.5683\n",
            "Epoch 278/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.3416 - acc: 0.4939 - val_loss: 1.2322 - val_acc: 0.5639\n",
            "Epoch 279/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.3638 - acc: 0.5061 - val_loss: 1.3128 - val_acc: 0.5374\n",
            "Epoch 280/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.3493 - acc: 0.5094 - val_loss: 1.2542 - val_acc: 0.5639\n",
            "Epoch 281/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.3767 - acc: 0.4895 - val_loss: 1.2460 - val_acc: 0.5595\n",
            "Epoch 282/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.2747 - acc: 0.5182 - val_loss: 1.2213 - val_acc: 0.5551\n",
            "Epoch 283/1500\n",
            "907/907 [==============================] - 0s 146us/sample - loss: 1.3812 - acc: 0.4620 - val_loss: 1.2079 - val_acc: 0.5551\n",
            "Epoch 284/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.3253 - acc: 0.5171 - val_loss: 1.1938 - val_acc: 0.5859\n",
            "Epoch 285/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.3422 - acc: 0.4840 - val_loss: 1.2768 - val_acc: 0.5286\n",
            "Epoch 286/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.3746 - acc: 0.5050 - val_loss: 1.2375 - val_acc: 0.5683\n",
            "Epoch 287/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.3402 - acc: 0.5072 - val_loss: 1.2144 - val_acc: 0.5771\n",
            "Epoch 288/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.3330 - acc: 0.5193 - val_loss: 1.2199 - val_acc: 0.5639\n",
            "Epoch 289/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.3499 - acc: 0.5237 - val_loss: 1.2092 - val_acc: 0.5771\n",
            "Epoch 290/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.3249 - acc: 0.5094 - val_loss: 1.2562 - val_acc: 0.5286\n",
            "Epoch 291/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.3497 - acc: 0.4961 - val_loss: 1.2074 - val_acc: 0.5727\n",
            "Epoch 292/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.3429 - acc: 0.4972 - val_loss: 1.2511 - val_acc: 0.5595\n",
            "Epoch 293/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.3298 - acc: 0.4939 - val_loss: 1.2260 - val_acc: 0.5419\n",
            "Epoch 294/1500\n",
            "907/907 [==============================] - 0s 139us/sample - loss: 1.3059 - acc: 0.5017 - val_loss: 1.2665 - val_acc: 0.5551\n",
            "Epoch 295/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.3684 - acc: 0.4884 - val_loss: 1.2548 - val_acc: 0.5595\n",
            "Epoch 296/1500\n",
            "907/907 [==============================] - 0s 133us/sample - loss: 1.2878 - acc: 0.5193 - val_loss: 1.2204 - val_acc: 0.5551\n",
            "Epoch 297/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.3388 - acc: 0.5083 - val_loss: 1.2202 - val_acc: 0.5771\n",
            "Epoch 298/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.3630 - acc: 0.4939 - val_loss: 1.2247 - val_acc: 0.5639\n",
            "Epoch 299/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.3084 - acc: 0.5226 - val_loss: 1.2665 - val_acc: 0.5330\n",
            "Epoch 300/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.3709 - acc: 0.4873 - val_loss: 1.2168 - val_acc: 0.5639\n",
            "Epoch 301/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.3620 - acc: 0.4961 - val_loss: 1.2278 - val_acc: 0.5463\n",
            "Epoch 302/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.3498 - acc: 0.5050 - val_loss: 1.2331 - val_acc: 0.5507\n",
            "Epoch 303/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.3258 - acc: 0.4939 - val_loss: 1.2454 - val_acc: 0.5551\n",
            "Epoch 304/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.3320 - acc: 0.5226 - val_loss: 1.2128 - val_acc: 0.5683\n",
            "Epoch 305/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.3205 - acc: 0.5039 - val_loss: 1.2199 - val_acc: 0.5815\n",
            "Epoch 306/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.3380 - acc: 0.5204 - val_loss: 1.2326 - val_acc: 0.5463\n",
            "Epoch 307/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.3179 - acc: 0.5215 - val_loss: 1.2057 - val_acc: 0.5463\n",
            "Epoch 308/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.3898 - acc: 0.4829 - val_loss: 1.2360 - val_acc: 0.5507\n",
            "Epoch 309/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 1.2770 - acc: 0.5358 - val_loss: 1.2328 - val_acc: 0.5463\n",
            "Epoch 310/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.3331 - acc: 0.4906 - val_loss: 1.2644 - val_acc: 0.5419\n",
            "Epoch 311/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.2741 - acc: 0.5171 - val_loss: 1.2015 - val_acc: 0.5639\n",
            "Epoch 312/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.2977 - acc: 0.5336 - val_loss: 1.2114 - val_acc: 0.5771\n",
            "Epoch 313/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 1.3159 - acc: 0.5358 - val_loss: 1.2049 - val_acc: 0.5463\n",
            "Epoch 314/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 1.3337 - acc: 0.4983 - val_loss: 1.2597 - val_acc: 0.5419\n",
            "Epoch 315/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.3272 - acc: 0.5149 - val_loss: 1.2480 - val_acc: 0.5551\n",
            "Epoch 316/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.3347 - acc: 0.5314 - val_loss: 1.2406 - val_acc: 0.5551\n",
            "Epoch 317/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.3368 - acc: 0.4950 - val_loss: 1.2397 - val_acc: 0.5463\n",
            "Epoch 318/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.3128 - acc: 0.5193 - val_loss: 1.2328 - val_acc: 0.5683\n",
            "Epoch 319/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.3291 - acc: 0.5094 - val_loss: 1.2034 - val_acc: 0.5683\n",
            "Epoch 320/1500\n",
            "907/907 [==============================] - 0s 109us/sample - loss: 1.3330 - acc: 0.5061 - val_loss: 1.1985 - val_acc: 0.5551\n",
            "Epoch 321/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.3343 - acc: 0.5006 - val_loss: 1.2082 - val_acc: 0.5595\n",
            "Epoch 322/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.3027 - acc: 0.5039 - val_loss: 1.2092 - val_acc: 0.5639\n",
            "Epoch 323/1500\n",
            "907/907 [==============================] - 0s 109us/sample - loss: 1.3067 - acc: 0.5105 - val_loss: 1.1942 - val_acc: 0.5683\n",
            "Epoch 324/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.3192 - acc: 0.5039 - val_loss: 1.1809 - val_acc: 0.5639\n",
            "Epoch 325/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.2735 - acc: 0.5160 - val_loss: 1.2115 - val_acc: 0.5463\n",
            "Epoch 326/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.3015 - acc: 0.5083 - val_loss: 1.2161 - val_acc: 0.5771\n",
            "Epoch 327/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.3251 - acc: 0.5028 - val_loss: 1.2378 - val_acc: 0.5507\n",
            "Epoch 328/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.3069 - acc: 0.5248 - val_loss: 1.2510 - val_acc: 0.5374\n",
            "Epoch 329/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.3515 - acc: 0.5072 - val_loss: 1.1912 - val_acc: 0.6079\n",
            "Epoch 330/1500\n",
            "907/907 [==============================] - 0s 138us/sample - loss: 1.3028 - acc: 0.5149 - val_loss: 1.1943 - val_acc: 0.5683\n",
            "Epoch 331/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.3053 - acc: 0.5138 - val_loss: 1.1934 - val_acc: 0.5639\n",
            "Epoch 332/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.3525 - acc: 0.5083 - val_loss: 1.2062 - val_acc: 0.5639\n",
            "Epoch 333/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.3280 - acc: 0.5028 - val_loss: 1.2185 - val_acc: 0.5771\n",
            "Epoch 334/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.3223 - acc: 0.5127 - val_loss: 1.2023 - val_acc: 0.5683\n",
            "Epoch 335/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.3317 - acc: 0.4928 - val_loss: 1.2350 - val_acc: 0.5639\n",
            "Epoch 336/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.2913 - acc: 0.5138 - val_loss: 1.2135 - val_acc: 0.5595\n",
            "Epoch 337/1500\n",
            "907/907 [==============================] - 0s 139us/sample - loss: 1.3183 - acc: 0.5094 - val_loss: 1.1587 - val_acc: 0.6079\n",
            "Epoch 338/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.2639 - acc: 0.5447 - val_loss: 1.1994 - val_acc: 0.5727\n",
            "Epoch 339/1500\n",
            "907/907 [==============================] - 0s 132us/sample - loss: 1.3288 - acc: 0.5204 - val_loss: 1.2423 - val_acc: 0.5595\n",
            "Epoch 340/1500\n",
            "907/907 [==============================] - 0s 131us/sample - loss: 1.3181 - acc: 0.5083 - val_loss: 1.1795 - val_acc: 0.5771\n",
            "Epoch 341/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.2718 - acc: 0.5314 - val_loss: 1.2129 - val_acc: 0.5815\n",
            "Epoch 342/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.3070 - acc: 0.4928 - val_loss: 1.1784 - val_acc: 0.5991\n",
            "Epoch 343/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.2787 - acc: 0.5447 - val_loss: 1.1685 - val_acc: 0.5815\n",
            "Epoch 344/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 1.2891 - acc: 0.5226 - val_loss: 1.1934 - val_acc: 0.5683\n",
            "Epoch 345/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.3026 - acc: 0.5248 - val_loss: 1.1800 - val_acc: 0.5815\n",
            "Epoch 346/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.2797 - acc: 0.5380 - val_loss: 1.2166 - val_acc: 0.5595\n",
            "Epoch 347/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.2531 - acc: 0.5226 - val_loss: 1.1982 - val_acc: 0.5727\n",
            "Epoch 348/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.2653 - acc: 0.5281 - val_loss: 1.1753 - val_acc: 0.5639\n",
            "Epoch 349/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.3042 - acc: 0.5149 - val_loss: 1.1684 - val_acc: 0.5727\n",
            "Epoch 350/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.2785 - acc: 0.5204 - val_loss: 1.1764 - val_acc: 0.5727\n",
            "Epoch 351/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.2788 - acc: 0.5270 - val_loss: 1.1687 - val_acc: 0.5727\n",
            "Epoch 352/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.2626 - acc: 0.5535 - val_loss: 1.1934 - val_acc: 0.5727\n",
            "Epoch 353/1500\n",
            "907/907 [==============================] - 0s 109us/sample - loss: 1.2903 - acc: 0.5237 - val_loss: 1.1868 - val_acc: 0.5815\n",
            "Epoch 354/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 1.2922 - acc: 0.5138 - val_loss: 1.1978 - val_acc: 0.6035\n",
            "Epoch 355/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.3650 - acc: 0.5050 - val_loss: 1.1805 - val_acc: 0.5595\n",
            "Epoch 356/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.2887 - acc: 0.5248 - val_loss: 1.1920 - val_acc: 0.5771\n",
            "Epoch 357/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.2644 - acc: 0.5270 - val_loss: 1.1607 - val_acc: 0.5815\n",
            "Epoch 358/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.2724 - acc: 0.5259 - val_loss: 1.2060 - val_acc: 0.5815\n",
            "Epoch 359/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 1.2390 - acc: 0.5160 - val_loss: 1.1875 - val_acc: 0.5551\n",
            "Epoch 360/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.3375 - acc: 0.5039 - val_loss: 1.2323 - val_acc: 0.5463\n",
            "Epoch 361/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.2627 - acc: 0.5281 - val_loss: 1.1514 - val_acc: 0.5815\n",
            "Epoch 362/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.3231 - acc: 0.5226 - val_loss: 1.1508 - val_acc: 0.6211\n",
            "Epoch 363/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.2407 - acc: 0.5336 - val_loss: 1.1855 - val_acc: 0.5727\n",
            "Epoch 364/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.3272 - acc: 0.5028 - val_loss: 1.1735 - val_acc: 0.5815\n",
            "Epoch 365/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.2574 - acc: 0.5689 - val_loss: 1.1431 - val_acc: 0.5903\n",
            "Epoch 366/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.2657 - acc: 0.5281 - val_loss: 1.2321 - val_acc: 0.5595\n",
            "Epoch 367/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.3081 - acc: 0.5325 - val_loss: 1.1661 - val_acc: 0.5947\n",
            "Epoch 368/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.2475 - acc: 0.5259 - val_loss: 1.1523 - val_acc: 0.6035\n",
            "Epoch 369/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 1.2718 - acc: 0.5612 - val_loss: 1.1541 - val_acc: 0.5815\n",
            "Epoch 370/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.2536 - acc: 0.5116 - val_loss: 1.1389 - val_acc: 0.6079\n",
            "Epoch 371/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.2582 - acc: 0.5579 - val_loss: 1.2021 - val_acc: 0.5595\n",
            "Epoch 372/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 1.2877 - acc: 0.5083 - val_loss: 1.2095 - val_acc: 0.5551\n",
            "Epoch 373/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.2695 - acc: 0.5292 - val_loss: 1.2017 - val_acc: 0.5771\n",
            "Epoch 374/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.3045 - acc: 0.5325 - val_loss: 1.1628 - val_acc: 0.5991\n",
            "Epoch 375/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.2919 - acc: 0.5149 - val_loss: 1.2400 - val_acc: 0.5463\n",
            "Epoch 376/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.3486 - acc: 0.5171 - val_loss: 1.2055 - val_acc: 0.5727\n",
            "Epoch 377/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.2824 - acc: 0.5215 - val_loss: 1.1852 - val_acc: 0.5551\n",
            "Epoch 378/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.2729 - acc: 0.5579 - val_loss: 1.1599 - val_acc: 0.6079\n",
            "Epoch 379/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.2719 - acc: 0.5248 - val_loss: 1.1631 - val_acc: 0.5727\n",
            "Epoch 380/1500\n",
            "907/907 [==============================] - 0s 108us/sample - loss: 1.2786 - acc: 0.5325 - val_loss: 1.1831 - val_acc: 0.5903\n",
            "Epoch 381/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.2964 - acc: 0.5248 - val_loss: 1.1885 - val_acc: 0.5903\n",
            "Epoch 382/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.3065 - acc: 0.4873 - val_loss: 1.1566 - val_acc: 0.5991\n",
            "Epoch 383/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.3079 - acc: 0.5270 - val_loss: 1.1780 - val_acc: 0.5683\n",
            "Epoch 384/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.2829 - acc: 0.5314 - val_loss: 1.1715 - val_acc: 0.5727\n",
            "Epoch 385/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.2243 - acc: 0.5325 - val_loss: 1.1550 - val_acc: 0.5947\n",
            "Epoch 386/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.2357 - acc: 0.5292 - val_loss: 1.1852 - val_acc: 0.5991\n",
            "Epoch 387/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.2556 - acc: 0.5667 - val_loss: 1.1534 - val_acc: 0.5991\n",
            "Epoch 388/1500\n",
            "907/907 [==============================] - 0s 143us/sample - loss: 1.2718 - acc: 0.5226 - val_loss: 1.1807 - val_acc: 0.5727\n",
            "Epoch 389/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.2681 - acc: 0.5292 - val_loss: 1.1418 - val_acc: 0.5903\n",
            "Epoch 390/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.2566 - acc: 0.5303 - val_loss: 1.1584 - val_acc: 0.5903\n",
            "Epoch 391/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.2745 - acc: 0.5171 - val_loss: 1.1575 - val_acc: 0.5859\n",
            "Epoch 392/1500\n",
            "907/907 [==============================] - 0s 144us/sample - loss: 1.2304 - acc: 0.5502 - val_loss: 1.1964 - val_acc: 0.5815\n",
            "Epoch 393/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.2851 - acc: 0.5237 - val_loss: 1.1458 - val_acc: 0.5991\n",
            "Epoch 394/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.2514 - acc: 0.5347 - val_loss: 1.1332 - val_acc: 0.5815\n",
            "Epoch 395/1500\n",
            "907/907 [==============================] - 0s 149us/sample - loss: 1.1881 - acc: 0.5678 - val_loss: 1.1200 - val_acc: 0.5947\n",
            "Epoch 396/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.2395 - acc: 0.5413 - val_loss: 1.1580 - val_acc: 0.5727\n",
            "Epoch 397/1500\n",
            "907/907 [==============================] - 0s 149us/sample - loss: 1.2744 - acc: 0.5325 - val_loss: 1.1668 - val_acc: 0.5595\n",
            "Epoch 398/1500\n",
            "907/907 [==============================] - 0s 131us/sample - loss: 1.2366 - acc: 0.5491 - val_loss: 1.1200 - val_acc: 0.6035\n",
            "Epoch 399/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.2699 - acc: 0.5237 - val_loss: 1.1466 - val_acc: 0.5947\n",
            "Epoch 400/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.2944 - acc: 0.5050 - val_loss: 1.1925 - val_acc: 0.5683\n",
            "Epoch 401/1500\n",
            "907/907 [==============================] - 0s 131us/sample - loss: 1.2689 - acc: 0.5204 - val_loss: 1.1576 - val_acc: 0.5859\n",
            "Epoch 402/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.3129 - acc: 0.5215 - val_loss: 1.1806 - val_acc: 0.5683\n",
            "Epoch 403/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.3033 - acc: 0.5160 - val_loss: 1.1747 - val_acc: 0.5683\n",
            "Epoch 404/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.2293 - acc: 0.5402 - val_loss: 1.1509 - val_acc: 0.5771\n",
            "Epoch 405/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.2667 - acc: 0.5292 - val_loss: 1.1257 - val_acc: 0.6167\n",
            "Epoch 406/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.2513 - acc: 0.5303 - val_loss: 1.1631 - val_acc: 0.5991\n",
            "Epoch 407/1500\n",
            "907/907 [==============================] - 0s 132us/sample - loss: 1.2086 - acc: 0.5689 - val_loss: 1.1418 - val_acc: 0.5947\n",
            "Epoch 408/1500\n",
            "907/907 [==============================] - 0s 137us/sample - loss: 1.2185 - acc: 0.5711 - val_loss: 1.1534 - val_acc: 0.5991\n",
            "Epoch 409/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.3185 - acc: 0.5303 - val_loss: 1.1154 - val_acc: 0.6035\n",
            "Epoch 410/1500\n",
            "907/907 [==============================] - 0s 132us/sample - loss: 1.2140 - acc: 0.5380 - val_loss: 1.1319 - val_acc: 0.5991\n",
            "Epoch 411/1500\n",
            "907/907 [==============================] - 0s 138us/sample - loss: 1.2295 - acc: 0.5513 - val_loss: 1.1760 - val_acc: 0.5771\n",
            "Epoch 412/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.2063 - acc: 0.5513 - val_loss: 1.1350 - val_acc: 0.5947\n",
            "Epoch 413/1500\n",
            "907/907 [==============================] - 0s 133us/sample - loss: 1.2175 - acc: 0.5623 - val_loss: 1.1790 - val_acc: 0.5859\n",
            "Epoch 414/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.2243 - acc: 0.5413 - val_loss: 1.1641 - val_acc: 0.5727\n",
            "Epoch 415/1500\n",
            "907/907 [==============================] - 0s 131us/sample - loss: 1.2119 - acc: 0.5380 - val_loss: 1.1647 - val_acc: 0.5727\n",
            "Epoch 416/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.2394 - acc: 0.5524 - val_loss: 1.1665 - val_acc: 0.5859\n",
            "Epoch 417/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.2527 - acc: 0.5491 - val_loss: 1.1779 - val_acc: 0.5859\n",
            "Epoch 418/1500\n",
            "907/907 [==============================] - 0s 151us/sample - loss: 1.2635 - acc: 0.5171 - val_loss: 1.1763 - val_acc: 0.5683\n",
            "Epoch 419/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.2126 - acc: 0.5524 - val_loss: 1.1516 - val_acc: 0.5859\n",
            "Epoch 420/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.2225 - acc: 0.5524 - val_loss: 1.1267 - val_acc: 0.5859\n",
            "Epoch 421/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.2036 - acc: 0.5513 - val_loss: 1.1397 - val_acc: 0.5771\n",
            "Epoch 422/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.2299 - acc: 0.5336 - val_loss: 1.1578 - val_acc: 0.5771\n",
            "Epoch 423/1500\n",
            "907/907 [==============================] - 0s 132us/sample - loss: 1.1972 - acc: 0.5568 - val_loss: 1.1223 - val_acc: 0.5947\n",
            "Epoch 424/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.2986 - acc: 0.5105 - val_loss: 1.2597 - val_acc: 0.5683\n",
            "Epoch 425/1500\n",
            "907/907 [==============================] - 0s 138us/sample - loss: 1.3043 - acc: 0.5204 - val_loss: 1.2381 - val_acc: 0.5639\n",
            "Epoch 426/1500\n",
            "907/907 [==============================] - 0s 142us/sample - loss: 1.2289 - acc: 0.5237 - val_loss: 1.1672 - val_acc: 0.5771\n",
            "Epoch 427/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.2170 - acc: 0.5347 - val_loss: 1.1720 - val_acc: 0.5683\n",
            "Epoch 428/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.2535 - acc: 0.5204 - val_loss: 1.1554 - val_acc: 0.5727\n",
            "Epoch 429/1500\n",
            "907/907 [==============================] - 0s 138us/sample - loss: 1.2575 - acc: 0.5413 - val_loss: 1.1553 - val_acc: 0.5903\n",
            "Epoch 430/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.1938 - acc: 0.5524 - val_loss: 1.1533 - val_acc: 0.5859\n",
            "Epoch 431/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.1948 - acc: 0.5535 - val_loss: 1.1527 - val_acc: 0.5815\n",
            "Epoch 432/1500\n",
            "907/907 [==============================] - 0s 133us/sample - loss: 1.2042 - acc: 0.5447 - val_loss: 1.1509 - val_acc: 0.5683\n",
            "Epoch 433/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.2142 - acc: 0.5568 - val_loss: 1.1438 - val_acc: 0.5683\n",
            "Epoch 434/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.2420 - acc: 0.5402 - val_loss: 1.1571 - val_acc: 0.5947\n",
            "Epoch 435/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.2688 - acc: 0.5270 - val_loss: 1.1369 - val_acc: 0.6079\n",
            "Epoch 436/1500\n",
            "907/907 [==============================] - 0s 161us/sample - loss: 1.2436 - acc: 0.5424 - val_loss: 1.1542 - val_acc: 0.5683\n",
            "Epoch 437/1500\n",
            "907/907 [==============================] - 0s 139us/sample - loss: 1.2424 - acc: 0.5436 - val_loss: 1.1359 - val_acc: 0.5815\n",
            "Epoch 438/1500\n",
            "907/907 [==============================] - 0s 149us/sample - loss: 1.2523 - acc: 0.5171 - val_loss: 1.1776 - val_acc: 0.5771\n",
            "Epoch 439/1500\n",
            "907/907 [==============================] - 0s 149us/sample - loss: 1.2470 - acc: 0.5314 - val_loss: 1.1345 - val_acc: 0.6167\n",
            "Epoch 440/1500\n",
            "907/907 [==============================] - 0s 145us/sample - loss: 1.2968 - acc: 0.5193 - val_loss: 1.2300 - val_acc: 0.5639\n",
            "Epoch 441/1500\n",
            "907/907 [==============================] - 0s 131us/sample - loss: 1.2304 - acc: 0.5413 - val_loss: 1.1239 - val_acc: 0.5771\n",
            "Epoch 442/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.2283 - acc: 0.5458 - val_loss: 1.1985 - val_acc: 0.5639\n",
            "Epoch 443/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.2186 - acc: 0.5436 - val_loss: 1.1147 - val_acc: 0.6388\n",
            "Epoch 444/1500\n",
            "907/907 [==============================] - 0s 146us/sample - loss: 1.1934 - acc: 0.5502 - val_loss: 1.1324 - val_acc: 0.6035\n",
            "Epoch 445/1500\n",
            "907/907 [==============================] - 0s 131us/sample - loss: 1.1794 - acc: 0.5413 - val_loss: 1.1856 - val_acc: 0.5551\n",
            "Epoch 446/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.2454 - acc: 0.5391 - val_loss: 1.1415 - val_acc: 0.5991\n",
            "Epoch 447/1500\n",
            "907/907 [==============================] - 0s 140us/sample - loss: 1.2582 - acc: 0.5513 - val_loss: 1.0946 - val_acc: 0.6520\n",
            "Epoch 448/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.2326 - acc: 0.5259 - val_loss: 1.1345 - val_acc: 0.5727\n",
            "Epoch 449/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.2288 - acc: 0.5347 - val_loss: 1.1301 - val_acc: 0.6035\n",
            "Epoch 450/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.2055 - acc: 0.5557 - val_loss: 1.1179 - val_acc: 0.5727\n",
            "Epoch 451/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.2151 - acc: 0.5502 - val_loss: 1.1349 - val_acc: 0.5771\n",
            "Epoch 452/1500\n",
            "907/907 [==============================] - 0s 131us/sample - loss: 1.1870 - acc: 0.5645 - val_loss: 1.1561 - val_acc: 0.5947\n",
            "Epoch 453/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.2207 - acc: 0.5447 - val_loss: 1.1700 - val_acc: 0.5771\n",
            "Epoch 454/1500\n",
            "907/907 [==============================] - 0s 131us/sample - loss: 1.1842 - acc: 0.5513 - val_loss: 1.1326 - val_acc: 0.6035\n",
            "Epoch 455/1500\n",
            "907/907 [==============================] - 0s 140us/sample - loss: 1.2805 - acc: 0.5270 - val_loss: 1.1466 - val_acc: 0.6079\n",
            "Epoch 456/1500\n",
            "907/907 [==============================] - 0s 148us/sample - loss: 1.2439 - acc: 0.5413 - val_loss: 1.1209 - val_acc: 0.5771\n",
            "Epoch 457/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.2160 - acc: 0.5469 - val_loss: 1.1357 - val_acc: 0.6035\n",
            "Epoch 458/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.2468 - acc: 0.5380 - val_loss: 1.1870 - val_acc: 0.5507\n",
            "Epoch 459/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.2260 - acc: 0.5491 - val_loss: 1.1560 - val_acc: 0.5991\n",
            "Epoch 460/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.2526 - acc: 0.5325 - val_loss: 1.1632 - val_acc: 0.5771\n",
            "Epoch 461/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.2002 - acc: 0.5579 - val_loss: 1.1501 - val_acc: 0.6035\n",
            "Epoch 462/1500\n",
            "907/907 [==============================] - 0s 136us/sample - loss: 1.2571 - acc: 0.5193 - val_loss: 1.1806 - val_acc: 0.6035\n",
            "Epoch 463/1500\n",
            "907/907 [==============================] - 0s 131us/sample - loss: 1.2220 - acc: 0.5491 - val_loss: 1.1589 - val_acc: 0.5947\n",
            "Epoch 464/1500\n",
            "907/907 [==============================] - 0s 150us/sample - loss: 1.2357 - acc: 0.5303 - val_loss: 1.1055 - val_acc: 0.6123\n",
            "Epoch 465/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.2136 - acc: 0.5634 - val_loss: 1.1493 - val_acc: 0.5991\n",
            "Epoch 466/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.2526 - acc: 0.5281 - val_loss: 1.1875 - val_acc: 0.5815\n",
            "Epoch 467/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.2043 - acc: 0.5634 - val_loss: 1.1723 - val_acc: 0.6123\n",
            "Epoch 468/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.2086 - acc: 0.5667 - val_loss: 1.1459 - val_acc: 0.5947\n",
            "Epoch 469/1500\n",
            "907/907 [==============================] - 0s 148us/sample - loss: 1.1652 - acc: 0.5656 - val_loss: 1.1277 - val_acc: 0.6035\n",
            "Epoch 470/1500\n",
            "907/907 [==============================] - 0s 149us/sample - loss: 1.1852 - acc: 0.5821 - val_loss: 1.1413 - val_acc: 0.5991\n",
            "Epoch 471/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.2463 - acc: 0.5491 - val_loss: 1.1657 - val_acc: 0.5771\n",
            "Epoch 472/1500\n",
            "907/907 [==============================] - 0s 140us/sample - loss: 1.1932 - acc: 0.5535 - val_loss: 1.1318 - val_acc: 0.6079\n",
            "Epoch 473/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.2115 - acc: 0.5436 - val_loss: 1.0936 - val_acc: 0.6123\n",
            "Epoch 474/1500\n",
            "907/907 [==============================] - 0s 149us/sample - loss: 1.2250 - acc: 0.5535 - val_loss: 1.1977 - val_acc: 0.5595\n",
            "Epoch 475/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.2141 - acc: 0.5480 - val_loss: 1.1347 - val_acc: 0.5991\n",
            "Epoch 476/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.2546 - acc: 0.5424 - val_loss: 1.1816 - val_acc: 0.5639\n",
            "Epoch 477/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.2167 - acc: 0.5502 - val_loss: 1.1306 - val_acc: 0.6079\n",
            "Epoch 478/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.2102 - acc: 0.5380 - val_loss: 1.1088 - val_acc: 0.6300\n",
            "Epoch 479/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.1645 - acc: 0.5700 - val_loss: 1.0996 - val_acc: 0.5991\n",
            "Epoch 480/1500\n",
            "907/907 [==============================] - 0s 134us/sample - loss: 1.1955 - acc: 0.5513 - val_loss: 1.1420 - val_acc: 0.5947\n",
            "Epoch 481/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.1866 - acc: 0.5546 - val_loss: 1.1375 - val_acc: 0.5859\n",
            "Epoch 482/1500\n",
            "907/907 [==============================] - 0s 108us/sample - loss: 1.2634 - acc: 0.5424 - val_loss: 1.1389 - val_acc: 0.5771\n",
            "Epoch 483/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.2216 - acc: 0.5447 - val_loss: 1.1344 - val_acc: 0.5859\n",
            "Epoch 484/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.2043 - acc: 0.5568 - val_loss: 1.1394 - val_acc: 0.5859\n",
            "Epoch 485/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.2192 - acc: 0.5369 - val_loss: 1.2119 - val_acc: 0.5859\n",
            "Epoch 486/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.1915 - acc: 0.5623 - val_loss: 1.1741 - val_acc: 0.5815\n",
            "Epoch 487/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.2104 - acc: 0.5447 - val_loss: 1.1151 - val_acc: 0.6035\n",
            "Epoch 488/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.2156 - acc: 0.5336 - val_loss: 1.1172 - val_acc: 0.6035\n",
            "Epoch 489/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.1766 - acc: 0.5546 - val_loss: 1.1643 - val_acc: 0.5991\n",
            "Epoch 490/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.2249 - acc: 0.5513 - val_loss: 1.1258 - val_acc: 0.5903\n",
            "Epoch 491/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 1.1902 - acc: 0.5557 - val_loss: 1.1585 - val_acc: 0.5859\n",
            "Epoch 492/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.2079 - acc: 0.5535 - val_loss: 1.1257 - val_acc: 0.5859\n",
            "Epoch 493/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.1701 - acc: 0.5678 - val_loss: 1.1185 - val_acc: 0.5947\n",
            "Epoch 494/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 1.1965 - acc: 0.5645 - val_loss: 1.1958 - val_acc: 0.5947\n",
            "Epoch 495/1500\n",
            "907/907 [==============================] - 0s 141us/sample - loss: 1.2310 - acc: 0.5424 - val_loss: 1.1538 - val_acc: 0.5903\n",
            "Epoch 496/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.1706 - acc: 0.5535 - val_loss: 1.1206 - val_acc: 0.5815\n",
            "Epoch 497/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.2002 - acc: 0.5358 - val_loss: 1.0930 - val_acc: 0.6123\n",
            "Epoch 498/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.1869 - acc: 0.5755 - val_loss: 1.1395 - val_acc: 0.5815\n",
            "Epoch 499/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.1956 - acc: 0.5391 - val_loss: 1.1779 - val_acc: 0.5859\n",
            "Epoch 500/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.2342 - acc: 0.5380 - val_loss: 1.1451 - val_acc: 0.5639\n",
            "Epoch 501/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.1804 - acc: 0.5700 - val_loss: 1.1508 - val_acc: 0.5815\n",
            "Epoch 502/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.1583 - acc: 0.5656 - val_loss: 1.1043 - val_acc: 0.5991\n",
            "Epoch 503/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.1950 - acc: 0.5491 - val_loss: 1.1250 - val_acc: 0.5815\n",
            "Epoch 504/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 1.2011 - acc: 0.5755 - val_loss: 1.1577 - val_acc: 0.5947\n",
            "Epoch 505/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.1918 - acc: 0.5513 - val_loss: 1.1463 - val_acc: 0.5903\n",
            "Epoch 506/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.2191 - acc: 0.5744 - val_loss: 1.1076 - val_acc: 0.6123\n",
            "Epoch 507/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 1.1405 - acc: 0.5634 - val_loss: 1.1103 - val_acc: 0.6167\n",
            "Epoch 508/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.1889 - acc: 0.5601 - val_loss: 1.1081 - val_acc: 0.5859\n",
            "Epoch 509/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.1776 - acc: 0.5469 - val_loss: 1.0982 - val_acc: 0.6300\n",
            "Epoch 510/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.2389 - acc: 0.5568 - val_loss: 1.1242 - val_acc: 0.6035\n",
            "Epoch 511/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.1877 - acc: 0.5491 - val_loss: 1.1901 - val_acc: 0.5991\n",
            "Epoch 512/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.2158 - acc: 0.5447 - val_loss: 1.1103 - val_acc: 0.6079\n",
            "Epoch 513/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.2636 - acc: 0.5149 - val_loss: 1.1024 - val_acc: 0.5727\n",
            "Epoch 514/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.1616 - acc: 0.5568 - val_loss: 1.1208 - val_acc: 0.6123\n",
            "Epoch 515/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.1580 - acc: 0.5656 - val_loss: 1.1201 - val_acc: 0.5991\n",
            "Epoch 516/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.1953 - acc: 0.5579 - val_loss: 1.1073 - val_acc: 0.6035\n",
            "Epoch 517/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.1821 - acc: 0.5535 - val_loss: 1.1702 - val_acc: 0.5903\n",
            "Epoch 518/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.1773 - acc: 0.5601 - val_loss: 1.0726 - val_acc: 0.6123\n",
            "Epoch 519/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.1906 - acc: 0.5601 - val_loss: 1.0843 - val_acc: 0.6300\n",
            "Epoch 520/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.1742 - acc: 0.5810 - val_loss: 1.1038 - val_acc: 0.5991\n",
            "Epoch 521/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.1781 - acc: 0.5491 - val_loss: 1.1840 - val_acc: 0.5815\n",
            "Epoch 522/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.1853 - acc: 0.5744 - val_loss: 1.0887 - val_acc: 0.6079\n",
            "Epoch 523/1500\n",
            "907/907 [==============================] - 0s 131us/sample - loss: 1.2064 - acc: 0.5524 - val_loss: 1.1497 - val_acc: 0.6035\n",
            "Epoch 524/1500\n",
            "907/907 [==============================] - 0s 147us/sample - loss: 1.2120 - acc: 0.5458 - val_loss: 1.1463 - val_acc: 0.6256\n",
            "Epoch 525/1500\n",
            "907/907 [==============================] - 0s 148us/sample - loss: 1.1661 - acc: 0.5557 - val_loss: 1.1036 - val_acc: 0.6035\n",
            "Epoch 526/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.1817 - acc: 0.5612 - val_loss: 1.1310 - val_acc: 0.6167\n",
            "Epoch 527/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.1555 - acc: 0.5788 - val_loss: 1.0996 - val_acc: 0.6079\n",
            "Epoch 528/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.2067 - acc: 0.5491 - val_loss: 1.1123 - val_acc: 0.5991\n",
            "Epoch 529/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.1662 - acc: 0.5744 - val_loss: 1.1493 - val_acc: 0.5947\n",
            "Epoch 530/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.2099 - acc: 0.5722 - val_loss: 1.1141 - val_acc: 0.5947\n",
            "Epoch 531/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.2343 - acc: 0.5568 - val_loss: 1.1049 - val_acc: 0.6344\n",
            "Epoch 532/1500\n",
            "907/907 [==============================] - 0s 141us/sample - loss: 1.2135 - acc: 0.5579 - val_loss: 1.0843 - val_acc: 0.6123\n",
            "Epoch 533/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.1689 - acc: 0.5689 - val_loss: 1.0765 - val_acc: 0.6300\n",
            "Epoch 534/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.2064 - acc: 0.5358 - val_loss: 1.1501 - val_acc: 0.5903\n",
            "Epoch 535/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.1533 - acc: 0.5733 - val_loss: 1.1166 - val_acc: 0.5947\n",
            "Epoch 536/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 1.1998 - acc: 0.5590 - val_loss: 1.1415 - val_acc: 0.5727\n",
            "Epoch 537/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.2078 - acc: 0.5436 - val_loss: 1.1022 - val_acc: 0.6079\n",
            "Epoch 538/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.1251 - acc: 0.5711 - val_loss: 1.0831 - val_acc: 0.6123\n",
            "Epoch 539/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.1549 - acc: 0.5755 - val_loss: 1.1193 - val_acc: 0.6079\n",
            "Epoch 540/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.2049 - acc: 0.5502 - val_loss: 1.0871 - val_acc: 0.6256\n",
            "Epoch 541/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.1649 - acc: 0.5722 - val_loss: 1.0990 - val_acc: 0.5903\n",
            "Epoch 542/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.1806 - acc: 0.5700 - val_loss: 1.1575 - val_acc: 0.5859\n",
            "Epoch 543/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.1823 - acc: 0.5557 - val_loss: 1.1245 - val_acc: 0.6344\n",
            "Epoch 544/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.1643 - acc: 0.5579 - val_loss: 1.1090 - val_acc: 0.6079\n",
            "Epoch 545/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.1400 - acc: 0.5557 - val_loss: 1.0864 - val_acc: 0.6035\n",
            "Epoch 546/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.1860 - acc: 0.5733 - val_loss: 1.0816 - val_acc: 0.6035\n",
            "Epoch 547/1500\n",
            "907/907 [==============================] - 0s 152us/sample - loss: 1.1508 - acc: 0.5678 - val_loss: 1.1633 - val_acc: 0.5947\n",
            "Epoch 548/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.1548 - acc: 0.5799 - val_loss: 1.1409 - val_acc: 0.5903\n",
            "Epoch 549/1500\n",
            "907/907 [==============================] - 0s 137us/sample - loss: 1.1779 - acc: 0.5755 - val_loss: 1.1777 - val_acc: 0.5903\n",
            "Epoch 550/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.2293 - acc: 0.5546 - val_loss: 1.1332 - val_acc: 0.5991\n",
            "Epoch 551/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.2329 - acc: 0.5314 - val_loss: 1.1343 - val_acc: 0.6035\n",
            "Epoch 552/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.1388 - acc: 0.5733 - val_loss: 1.1120 - val_acc: 0.6167\n",
            "Epoch 553/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.1178 - acc: 0.5888 - val_loss: 1.0997 - val_acc: 0.6123\n",
            "Epoch 554/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.2276 - acc: 0.5557 - val_loss: 1.0819 - val_acc: 0.6211\n",
            "Epoch 555/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.1390 - acc: 0.5821 - val_loss: 1.1108 - val_acc: 0.6079\n",
            "Epoch 556/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.1520 - acc: 0.5601 - val_loss: 1.0760 - val_acc: 0.6388\n",
            "Epoch 557/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.2353 - acc: 0.5424 - val_loss: 1.1384 - val_acc: 0.5903\n",
            "Epoch 558/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 1.2101 - acc: 0.5391 - val_loss: 1.1608 - val_acc: 0.5859\n",
            "Epoch 559/1500\n",
            "907/907 [==============================] - 0s 107us/sample - loss: 1.2463 - acc: 0.5424 - val_loss: 1.1358 - val_acc: 0.5947\n",
            "Epoch 560/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 1.1340 - acc: 0.5788 - val_loss: 1.0678 - val_acc: 0.6211\n",
            "Epoch 561/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.1549 - acc: 0.5524 - val_loss: 1.0802 - val_acc: 0.6167\n",
            "Epoch 562/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.1527 - acc: 0.5744 - val_loss: 1.1237 - val_acc: 0.5815\n",
            "Epoch 563/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.1940 - acc: 0.5380 - val_loss: 1.1119 - val_acc: 0.5947\n",
            "Epoch 564/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 1.1806 - acc: 0.5678 - val_loss: 1.0682 - val_acc: 0.6256\n",
            "Epoch 565/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.1877 - acc: 0.5458 - val_loss: 1.1545 - val_acc: 0.6123\n",
            "Epoch 566/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.2222 - acc: 0.5546 - val_loss: 1.1059 - val_acc: 0.5947\n",
            "Epoch 567/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.2021 - acc: 0.5469 - val_loss: 1.0803 - val_acc: 0.5859\n",
            "Epoch 568/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.1580 - acc: 0.5832 - val_loss: 1.1181 - val_acc: 0.6300\n",
            "Epoch 569/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.2002 - acc: 0.5458 - val_loss: 1.1262 - val_acc: 0.5903\n",
            "Epoch 570/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.1252 - acc: 0.5865 - val_loss: 1.1153 - val_acc: 0.6035\n",
            "Epoch 571/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.1907 - acc: 0.5645 - val_loss: 1.1160 - val_acc: 0.6211\n",
            "Epoch 572/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.1498 - acc: 0.5700 - val_loss: 1.0979 - val_acc: 0.6256\n",
            "Epoch 573/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.1687 - acc: 0.5612 - val_loss: 1.0884 - val_acc: 0.6344\n",
            "Epoch 574/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.1453 - acc: 0.5877 - val_loss: 1.0892 - val_acc: 0.5947\n",
            "Epoch 575/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.1530 - acc: 0.5733 - val_loss: 1.1789 - val_acc: 0.5683\n",
            "Epoch 576/1500\n",
            "907/907 [==============================] - 0s 141us/sample - loss: 1.2170 - acc: 0.5424 - val_loss: 1.0969 - val_acc: 0.6211\n",
            "Epoch 577/1500\n",
            "907/907 [==============================] - 0s 140us/sample - loss: 1.1364 - acc: 0.5711 - val_loss: 1.1207 - val_acc: 0.6123\n",
            "Epoch 578/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.1615 - acc: 0.5799 - val_loss: 1.0880 - val_acc: 0.6564\n",
            "Epoch 579/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.1723 - acc: 0.5524 - val_loss: 1.1234 - val_acc: 0.5947\n",
            "Epoch 580/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.1848 - acc: 0.5612 - val_loss: 1.1211 - val_acc: 0.6300\n",
            "Epoch 581/1500\n",
            "907/907 [==============================] - 0s 108us/sample - loss: 1.1071 - acc: 0.5854 - val_loss: 1.0905 - val_acc: 0.6123\n",
            "Epoch 582/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.1638 - acc: 0.5568 - val_loss: 1.0976 - val_acc: 0.6256\n",
            "Epoch 583/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.1070 - acc: 0.5998 - val_loss: 1.0976 - val_acc: 0.6256\n",
            "Epoch 584/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.1894 - acc: 0.5799 - val_loss: 1.1626 - val_acc: 0.5815\n",
            "Epoch 585/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.1662 - acc: 0.5722 - val_loss: 1.1539 - val_acc: 0.5991\n",
            "Epoch 586/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.2114 - acc: 0.5447 - val_loss: 1.1080 - val_acc: 0.6035\n",
            "Epoch 587/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.2259 - acc: 0.5380 - val_loss: 1.1257 - val_acc: 0.5903\n",
            "Epoch 588/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.1759 - acc: 0.5722 - val_loss: 1.1135 - val_acc: 0.5991\n",
            "Epoch 589/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.2010 - acc: 0.5645 - val_loss: 1.1039 - val_acc: 0.5903\n",
            "Epoch 590/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.1687 - acc: 0.5678 - val_loss: 1.0928 - val_acc: 0.6123\n",
            "Epoch 591/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.1717 - acc: 0.5799 - val_loss: 1.1437 - val_acc: 0.6167\n",
            "Epoch 592/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.1853 - acc: 0.5656 - val_loss: 1.1545 - val_acc: 0.6035\n",
            "Epoch 593/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.1730 - acc: 0.5612 - val_loss: 1.1074 - val_acc: 0.6211\n",
            "Epoch 594/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.1150 - acc: 0.5678 - val_loss: 1.1387 - val_acc: 0.6167\n",
            "Epoch 595/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.1606 - acc: 0.5678 - val_loss: 1.0920 - val_acc: 0.6123\n",
            "Epoch 596/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.1976 - acc: 0.5535 - val_loss: 1.1275 - val_acc: 0.5947\n",
            "Epoch 597/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.1766 - acc: 0.5821 - val_loss: 1.1327 - val_acc: 0.6079\n",
            "Epoch 598/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.1272 - acc: 0.5799 - val_loss: 1.1354 - val_acc: 0.6167\n",
            "Epoch 599/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.1348 - acc: 0.5766 - val_loss: 1.1352 - val_acc: 0.6079\n",
            "Epoch 600/1500\n",
            "907/907 [==============================] - 0s 158us/sample - loss: 1.1606 - acc: 0.5810 - val_loss: 1.0856 - val_acc: 0.6079\n",
            "Epoch 601/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.1566 - acc: 0.5612 - val_loss: 1.0918 - val_acc: 0.6167\n",
            "Epoch 602/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.1133 - acc: 0.5854 - val_loss: 1.1078 - val_acc: 0.6079\n",
            "Epoch 603/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.2520 - acc: 0.5369 - val_loss: 1.1564 - val_acc: 0.5991\n",
            "Epoch 604/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.1570 - acc: 0.5590 - val_loss: 1.1279 - val_acc: 0.6167\n",
            "Epoch 605/1500\n",
            "907/907 [==============================] - 0s 138us/sample - loss: 1.1579 - acc: 0.5612 - val_loss: 1.1553 - val_acc: 0.5903\n",
            "Epoch 606/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.1350 - acc: 0.5733 - val_loss: 1.0979 - val_acc: 0.6079\n",
            "Epoch 607/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.0866 - acc: 0.6053 - val_loss: 1.1170 - val_acc: 0.6211\n",
            "Epoch 608/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.1472 - acc: 0.5634 - val_loss: 1.0786 - val_acc: 0.6388\n",
            "Epoch 609/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.1668 - acc: 0.5821 - val_loss: 1.0843 - val_acc: 0.6256\n",
            "Epoch 610/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.1456 - acc: 0.5612 - val_loss: 1.0846 - val_acc: 0.6123\n",
            "Epoch 611/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.1626 - acc: 0.5590 - val_loss: 1.1486 - val_acc: 0.5991\n",
            "Epoch 612/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.1374 - acc: 0.5821 - val_loss: 1.0861 - val_acc: 0.6344\n",
            "Epoch 613/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.1665 - acc: 0.5601 - val_loss: 1.1258 - val_acc: 0.5947\n",
            "Epoch 614/1500\n",
            "907/907 [==============================] - 0s 134us/sample - loss: 1.1645 - acc: 0.5678 - val_loss: 1.1811 - val_acc: 0.5815\n",
            "Epoch 615/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.1373 - acc: 0.5711 - val_loss: 1.1065 - val_acc: 0.6123\n",
            "Epoch 616/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.1590 - acc: 0.5656 - val_loss: 1.1280 - val_acc: 0.5903\n",
            "Epoch 617/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.1660 - acc: 0.5623 - val_loss: 1.1358 - val_acc: 0.6079\n",
            "Epoch 618/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.1827 - acc: 0.5458 - val_loss: 1.0944 - val_acc: 0.6167\n",
            "Epoch 619/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.1991 - acc: 0.5292 - val_loss: 1.1245 - val_acc: 0.6167\n",
            "Epoch 620/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.2049 - acc: 0.5546 - val_loss: 1.1247 - val_acc: 0.5991\n",
            "Epoch 621/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.1624 - acc: 0.5700 - val_loss: 1.1038 - val_acc: 0.6256\n",
            "Epoch 622/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0867 - acc: 0.5711 - val_loss: 1.0714 - val_acc: 0.6520\n",
            "Epoch 623/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.1299 - acc: 0.5755 - val_loss: 1.0787 - val_acc: 0.6388\n",
            "Epoch 624/1500\n",
            "907/907 [==============================] - 0s 140us/sample - loss: 1.1234 - acc: 0.5711 - val_loss: 1.1264 - val_acc: 0.6167\n",
            "Epoch 625/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.1390 - acc: 0.5766 - val_loss: 1.1186 - val_acc: 0.6167\n",
            "Epoch 626/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.1508 - acc: 0.5689 - val_loss: 1.1299 - val_acc: 0.6079\n",
            "Epoch 627/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.1933 - acc: 0.5667 - val_loss: 1.1225 - val_acc: 0.5991\n",
            "Epoch 628/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.2115 - acc: 0.5656 - val_loss: 1.1549 - val_acc: 0.5859\n",
            "Epoch 629/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.1923 - acc: 0.5667 - val_loss: 1.0790 - val_acc: 0.5991\n",
            "Epoch 630/1500\n",
            "907/907 [==============================] - 0s 139us/sample - loss: 1.1452 - acc: 0.5766 - val_loss: 1.0810 - val_acc: 0.6167\n",
            "Epoch 631/1500\n",
            "907/907 [==============================] - 0s 148us/sample - loss: 1.1392 - acc: 0.5788 - val_loss: 1.0817 - val_acc: 0.6256\n",
            "Epoch 632/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.1179 - acc: 0.5799 - val_loss: 1.0572 - val_acc: 0.6167\n",
            "Epoch 633/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.1500 - acc: 0.5921 - val_loss: 1.0961 - val_acc: 0.6211\n",
            "Epoch 634/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.1424 - acc: 0.5821 - val_loss: 1.0856 - val_acc: 0.6256\n",
            "Epoch 635/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.0893 - acc: 0.5788 - val_loss: 1.0909 - val_acc: 0.6123\n",
            "Epoch 636/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.1072 - acc: 0.5854 - val_loss: 1.0963 - val_acc: 0.5947\n",
            "Epoch 637/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.1708 - acc: 0.5755 - val_loss: 1.1345 - val_acc: 0.6079\n",
            "Epoch 638/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.1793 - acc: 0.5557 - val_loss: 1.1073 - val_acc: 0.6256\n",
            "Epoch 639/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.1984 - acc: 0.5634 - val_loss: 1.1740 - val_acc: 0.5815\n",
            "Epoch 640/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.1737 - acc: 0.5623 - val_loss: 1.1291 - val_acc: 0.6211\n",
            "Epoch 641/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0999 - acc: 0.5854 - val_loss: 1.0891 - val_acc: 0.6079\n",
            "Epoch 642/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.1199 - acc: 0.5788 - val_loss: 1.0880 - val_acc: 0.6123\n",
            "Epoch 643/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.1313 - acc: 0.5998 - val_loss: 1.1101 - val_acc: 0.5903\n",
            "Epoch 644/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.0905 - acc: 0.5810 - val_loss: 1.1615 - val_acc: 0.5947\n",
            "Epoch 645/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.0881 - acc: 0.5700 - val_loss: 1.1298 - val_acc: 0.6035\n",
            "Epoch 646/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.2168 - acc: 0.5491 - val_loss: 1.0872 - val_acc: 0.6256\n",
            "Epoch 647/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.1562 - acc: 0.5766 - val_loss: 1.0968 - val_acc: 0.6079\n",
            "Epoch 648/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.1420 - acc: 0.5788 - val_loss: 1.0882 - val_acc: 0.6211\n",
            "Epoch 649/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.1531 - acc: 0.5755 - val_loss: 1.1363 - val_acc: 0.6211\n",
            "Epoch 650/1500\n",
            "907/907 [==============================] - 0s 134us/sample - loss: 1.1169 - acc: 0.5888 - val_loss: 1.0705 - val_acc: 0.6123\n",
            "Epoch 651/1500\n",
            "907/907 [==============================] - 0s 140us/sample - loss: 1.1608 - acc: 0.5744 - val_loss: 1.0791 - val_acc: 0.6211\n",
            "Epoch 652/1500\n",
            "907/907 [==============================] - 0s 141us/sample - loss: 1.1268 - acc: 0.5810 - val_loss: 1.0884 - val_acc: 0.6211\n",
            "Epoch 653/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.1042 - acc: 0.5987 - val_loss: 1.0777 - val_acc: 0.6123\n",
            "Epoch 654/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.0971 - acc: 0.5755 - val_loss: 1.0811 - val_acc: 0.6211\n",
            "Epoch 655/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.1604 - acc: 0.5777 - val_loss: 1.0682 - val_acc: 0.5991\n",
            "Epoch 656/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.0890 - acc: 0.5998 - val_loss: 1.1129 - val_acc: 0.5947\n",
            "Epoch 657/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.1651 - acc: 0.5877 - val_loss: 1.1317 - val_acc: 0.5815\n",
            "Epoch 658/1500\n",
            "907/907 [==============================] - 0s 138us/sample - loss: 1.1596 - acc: 0.5579 - val_loss: 1.0975 - val_acc: 0.5947\n",
            "Epoch 659/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.1270 - acc: 0.5899 - val_loss: 1.1316 - val_acc: 0.5947\n",
            "Epoch 660/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.0881 - acc: 0.5998 - val_loss: 1.1111 - val_acc: 0.6256\n",
            "Epoch 661/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.1005 - acc: 0.5810 - val_loss: 1.0663 - val_acc: 0.6256\n",
            "Epoch 662/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.1472 - acc: 0.5865 - val_loss: 1.1132 - val_acc: 0.6256\n",
            "Epoch 663/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.1318 - acc: 0.5910 - val_loss: 1.1412 - val_acc: 0.5859\n",
            "Epoch 664/1500\n",
            "907/907 [==============================] - 0s 107us/sample - loss: 1.1569 - acc: 0.5766 - val_loss: 1.1067 - val_acc: 0.6035\n",
            "Epoch 665/1500\n",
            "907/907 [==============================] - 0s 109us/sample - loss: 1.1089 - acc: 0.5832 - val_loss: 1.0974 - val_acc: 0.5991\n",
            "Epoch 666/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.1020 - acc: 0.5821 - val_loss: 1.0884 - val_acc: 0.6300\n",
            "Epoch 667/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.1260 - acc: 0.5954 - val_loss: 1.0922 - val_acc: 0.6123\n",
            "Epoch 668/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.1202 - acc: 0.5899 - val_loss: 1.1009 - val_acc: 0.6079\n",
            "Epoch 669/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.1770 - acc: 0.5722 - val_loss: 1.1700 - val_acc: 0.5991\n",
            "Epoch 670/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.1183 - acc: 0.5865 - val_loss: 1.1077 - val_acc: 0.6388\n",
            "Epoch 671/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.1219 - acc: 0.5976 - val_loss: 1.1636 - val_acc: 0.6079\n",
            "Epoch 672/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.1240 - acc: 0.5700 - val_loss: 1.1666 - val_acc: 0.5991\n",
            "Epoch 673/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.1195 - acc: 0.5832 - val_loss: 1.1354 - val_acc: 0.6035\n",
            "Epoch 674/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.1047 - acc: 0.6108 - val_loss: 1.0994 - val_acc: 0.6344\n",
            "Epoch 675/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.1108 - acc: 0.5899 - val_loss: 1.0702 - val_acc: 0.6167\n",
            "Epoch 676/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.1225 - acc: 0.5733 - val_loss: 1.1464 - val_acc: 0.6035\n",
            "Epoch 677/1500\n",
            "907/907 [==============================] - 0s 108us/sample - loss: 1.1122 - acc: 0.6031 - val_loss: 1.1059 - val_acc: 0.6256\n",
            "Epoch 678/1500\n",
            "907/907 [==============================] - 0s 143us/sample - loss: 1.0797 - acc: 0.5987 - val_loss: 1.0940 - val_acc: 0.6167\n",
            "Epoch 679/1500\n",
            "907/907 [==============================] - 0s 152us/sample - loss: 1.0901 - acc: 0.5954 - val_loss: 1.0879 - val_acc: 0.6432\n",
            "Epoch 680/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.1662 - acc: 0.5590 - val_loss: 1.0779 - val_acc: 0.6256\n",
            "Epoch 681/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.1164 - acc: 0.5788 - val_loss: 1.1572 - val_acc: 0.5947\n",
            "Epoch 682/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.1691 - acc: 0.5623 - val_loss: 1.1226 - val_acc: 0.5859\n",
            "Epoch 683/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.1384 - acc: 0.5700 - val_loss: 1.1092 - val_acc: 0.6079\n",
            "Epoch 684/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.0999 - acc: 0.5777 - val_loss: 1.0790 - val_acc: 0.6300\n",
            "Epoch 685/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.1260 - acc: 0.6064 - val_loss: 1.0962 - val_acc: 0.5991\n",
            "Epoch 686/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.1398 - acc: 0.5843 - val_loss: 1.1416 - val_acc: 0.6079\n",
            "Epoch 687/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.1017 - acc: 0.5821 - val_loss: 1.1015 - val_acc: 0.6211\n",
            "Epoch 688/1500\n",
            "907/907 [==============================] - 0s 142us/sample - loss: 1.1202 - acc: 0.5799 - val_loss: 1.1164 - val_acc: 0.6167\n",
            "Epoch 689/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.1275 - acc: 0.5678 - val_loss: 1.1200 - val_acc: 0.6211\n",
            "Epoch 690/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.1794 - acc: 0.5579 - val_loss: 1.0730 - val_acc: 0.6388\n",
            "Epoch 691/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.1633 - acc: 0.5755 - val_loss: 1.0467 - val_acc: 0.6388\n",
            "Epoch 692/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.1118 - acc: 0.5832 - val_loss: 1.0718 - val_acc: 0.6079\n",
            "Epoch 693/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.1736 - acc: 0.5557 - val_loss: 1.0724 - val_acc: 0.5947\n",
            "Epoch 694/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.1144 - acc: 0.5700 - val_loss: 1.0594 - val_acc: 0.6256\n",
            "Epoch 695/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.1010 - acc: 0.5932 - val_loss: 1.0995 - val_acc: 0.6167\n",
            "Epoch 696/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.1103 - acc: 0.5888 - val_loss: 1.0902 - val_acc: 0.6123\n",
            "Epoch 697/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.0842 - acc: 0.5788 - val_loss: 1.1480 - val_acc: 0.5947\n",
            "Epoch 698/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.1315 - acc: 0.5733 - val_loss: 1.0561 - val_acc: 0.6432\n",
            "Epoch 699/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.1113 - acc: 0.5744 - val_loss: 1.1041 - val_acc: 0.6035\n",
            "Epoch 700/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.0899 - acc: 0.5987 - val_loss: 1.0995 - val_acc: 0.6256\n",
            "Epoch 701/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0455 - acc: 0.6064 - val_loss: 1.1111 - val_acc: 0.6123\n",
            "Epoch 702/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.1021 - acc: 0.5932 - val_loss: 1.0588 - val_acc: 0.6344\n",
            "Epoch 703/1500\n",
            "907/907 [==============================] - 0s 133us/sample - loss: 1.1141 - acc: 0.5711 - val_loss: 1.1255 - val_acc: 0.6035\n",
            "Epoch 704/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.1240 - acc: 0.5700 - val_loss: 1.1116 - val_acc: 0.6211\n",
            "Epoch 705/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.1284 - acc: 0.5854 - val_loss: 1.0905 - val_acc: 0.6300\n",
            "Epoch 706/1500\n",
            "907/907 [==============================] - 0s 135us/sample - loss: 1.1054 - acc: 0.5755 - val_loss: 1.0997 - val_acc: 0.5991\n",
            "Epoch 707/1500\n",
            "907/907 [==============================] - 0s 109us/sample - loss: 1.0994 - acc: 0.5733 - val_loss: 1.0588 - val_acc: 0.6123\n",
            "Epoch 708/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0901 - acc: 0.5954 - val_loss: 1.1225 - val_acc: 0.6035\n",
            "Epoch 709/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.1137 - acc: 0.5832 - val_loss: 1.1093 - val_acc: 0.6035\n",
            "Epoch 710/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.0989 - acc: 0.5899 - val_loss: 1.1070 - val_acc: 0.6079\n",
            "Epoch 711/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.1155 - acc: 0.5843 - val_loss: 1.1269 - val_acc: 0.5859\n",
            "Epoch 712/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.1080 - acc: 0.5755 - val_loss: 1.1081 - val_acc: 0.6035\n",
            "Epoch 713/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.1280 - acc: 0.5612 - val_loss: 1.1103 - val_acc: 0.6256\n",
            "Epoch 714/1500\n",
            "907/907 [==============================] - 0s 149us/sample - loss: 1.0880 - acc: 0.5987 - val_loss: 1.0665 - val_acc: 0.6300\n",
            "Epoch 715/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.1572 - acc: 0.5932 - val_loss: 1.1102 - val_acc: 0.6256\n",
            "Epoch 716/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.1588 - acc: 0.5689 - val_loss: 1.0933 - val_acc: 0.6300\n",
            "Epoch 717/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.0801 - acc: 0.5865 - val_loss: 1.0812 - val_acc: 0.6211\n",
            "Epoch 718/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0866 - acc: 0.5987 - val_loss: 1.0742 - val_acc: 0.6167\n",
            "Epoch 719/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.1080 - acc: 0.5877 - val_loss: 1.1296 - val_acc: 0.6035\n",
            "Epoch 720/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.1106 - acc: 0.5722 - val_loss: 1.1166 - val_acc: 0.6123\n",
            "Epoch 721/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.1139 - acc: 0.5998 - val_loss: 1.1340 - val_acc: 0.5947\n",
            "Epoch 722/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.0847 - acc: 0.5954 - val_loss: 1.0443 - val_acc: 0.6300\n",
            "Epoch 723/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.1644 - acc: 0.5821 - val_loss: 1.0894 - val_acc: 0.6167\n",
            "Epoch 724/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.0697 - acc: 0.6042 - val_loss: 1.0502 - val_acc: 0.6167\n",
            "Epoch 725/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.1501 - acc: 0.5678 - val_loss: 1.1757 - val_acc: 0.5947\n",
            "Epoch 726/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.1444 - acc: 0.5678 - val_loss: 1.1361 - val_acc: 0.5991\n",
            "Epoch 727/1500\n",
            "907/907 [==============================] - 0s 133us/sample - loss: 1.1418 - acc: 0.5766 - val_loss: 1.1359 - val_acc: 0.5815\n",
            "Epoch 728/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.1371 - acc: 0.5744 - val_loss: 1.0957 - val_acc: 0.6211\n",
            "Epoch 729/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.0771 - acc: 0.6163 - val_loss: 1.0785 - val_acc: 0.6344\n",
            "Epoch 730/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.0916 - acc: 0.5943 - val_loss: 1.0995 - val_acc: 0.5903\n",
            "Epoch 731/1500\n",
            "907/907 [==============================] - 0s 131us/sample - loss: 1.1174 - acc: 0.5865 - val_loss: 1.0955 - val_acc: 0.5991\n",
            "Epoch 732/1500\n",
            "907/907 [==============================] - 0s 142us/sample - loss: 1.1038 - acc: 0.5943 - val_loss: 1.0772 - val_acc: 0.6344\n",
            "Epoch 733/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.0915 - acc: 0.5943 - val_loss: 1.1049 - val_acc: 0.5859\n",
            "Epoch 734/1500\n",
            "907/907 [==============================] - 0s 146us/sample - loss: 1.1168 - acc: 0.5899 - val_loss: 1.0869 - val_acc: 0.6167\n",
            "Epoch 735/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.0683 - acc: 0.6108 - val_loss: 1.1005 - val_acc: 0.6211\n",
            "Epoch 736/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0848 - acc: 0.5943 - val_loss: 1.1747 - val_acc: 0.6167\n",
            "Epoch 737/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.1455 - acc: 0.5788 - val_loss: 1.0842 - val_acc: 0.6300\n",
            "Epoch 738/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.1186 - acc: 0.5976 - val_loss: 1.0465 - val_acc: 0.6211\n",
            "Epoch 739/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.1067 - acc: 0.5921 - val_loss: 1.0918 - val_acc: 0.5815\n",
            "Epoch 740/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.1217 - acc: 0.5821 - val_loss: 1.0945 - val_acc: 0.5947\n",
            "Epoch 741/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.0756 - acc: 0.6119 - val_loss: 1.0498 - val_acc: 0.6211\n",
            "Epoch 742/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.0695 - acc: 0.5766 - val_loss: 1.1582 - val_acc: 0.5947\n",
            "Epoch 743/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.1373 - acc: 0.5755 - val_loss: 1.1402 - val_acc: 0.6035\n",
            "Epoch 744/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.1310 - acc: 0.5733 - val_loss: 1.1685 - val_acc: 0.5903\n",
            "Epoch 745/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.1121 - acc: 0.5877 - val_loss: 1.1312 - val_acc: 0.6123\n",
            "Epoch 746/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.2131 - acc: 0.5623 - val_loss: 1.1760 - val_acc: 0.5947\n",
            "Epoch 747/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.1569 - acc: 0.5700 - val_loss: 1.1029 - val_acc: 0.6211\n",
            "Epoch 748/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.1097 - acc: 0.6042 - val_loss: 1.0805 - val_acc: 0.6300\n",
            "Epoch 749/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.0990 - acc: 0.5821 - val_loss: 1.0964 - val_acc: 0.6300\n",
            "Epoch 750/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.0844 - acc: 0.5843 - val_loss: 1.1729 - val_acc: 0.6211\n",
            "Epoch 751/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.0827 - acc: 0.5987 - val_loss: 1.0893 - val_acc: 0.6167\n",
            "Epoch 752/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.0818 - acc: 0.6075 - val_loss: 1.0715 - val_acc: 0.6300\n",
            "Epoch 753/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.1095 - acc: 0.5623 - val_loss: 1.0840 - val_acc: 0.6211\n",
            "Epoch 754/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.0823 - acc: 0.6097 - val_loss: 1.1065 - val_acc: 0.6079\n",
            "Epoch 755/1500\n",
            "907/907 [==============================] - 0s 151us/sample - loss: 1.0869 - acc: 0.5987 - val_loss: 1.1793 - val_acc: 0.6079\n",
            "Epoch 756/1500\n",
            "907/907 [==============================] - 0s 144us/sample - loss: 1.1195 - acc: 0.5645 - val_loss: 1.1018 - val_acc: 0.6300\n",
            "Epoch 757/1500\n",
            "907/907 [==============================] - 0s 152us/sample - loss: 1.0656 - acc: 0.6064 - val_loss: 1.1442 - val_acc: 0.6123\n",
            "Epoch 758/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 1.1374 - acc: 0.5623 - val_loss: 1.0955 - val_acc: 0.6256\n",
            "Epoch 759/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.0750 - acc: 0.6273 - val_loss: 1.1173 - val_acc: 0.6167\n",
            "Epoch 760/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.1045 - acc: 0.5821 - val_loss: 1.1144 - val_acc: 0.6256\n",
            "Epoch 761/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.1835 - acc: 0.5744 - val_loss: 1.0988 - val_acc: 0.5991\n",
            "Epoch 762/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 1.0796 - acc: 0.6053 - val_loss: 1.0970 - val_acc: 0.6035\n",
            "Epoch 763/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.0899 - acc: 0.5965 - val_loss: 1.0917 - val_acc: 0.6300\n",
            "Epoch 764/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.1323 - acc: 0.5921 - val_loss: 1.0960 - val_acc: 0.6256\n",
            "Epoch 765/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.0789 - acc: 0.6009 - val_loss: 1.1014 - val_acc: 0.6167\n",
            "Epoch 766/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.0965 - acc: 0.5821 - val_loss: 1.1687 - val_acc: 0.5859\n",
            "Epoch 767/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.0851 - acc: 0.5987 - val_loss: 1.0920 - val_acc: 0.6211\n",
            "Epoch 768/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.0619 - acc: 0.6185 - val_loss: 1.0898 - val_acc: 0.6344\n",
            "Epoch 769/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.1455 - acc: 0.5744 - val_loss: 1.1370 - val_acc: 0.6476\n",
            "Epoch 770/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.1002 - acc: 0.5810 - val_loss: 1.1052 - val_acc: 0.6300\n",
            "Epoch 771/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.0849 - acc: 0.5843 - val_loss: 1.1158 - val_acc: 0.6520\n",
            "Epoch 772/1500\n",
            "907/907 [==============================] - 0s 109us/sample - loss: 1.0427 - acc: 0.6163 - val_loss: 1.0889 - val_acc: 0.6211\n",
            "Epoch 773/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.0419 - acc: 0.6042 - val_loss: 1.0906 - val_acc: 0.6564\n",
            "Epoch 774/1500\n",
            "907/907 [==============================] - 0s 109us/sample - loss: 1.0629 - acc: 0.5921 - val_loss: 1.0567 - val_acc: 0.5991\n",
            "Epoch 775/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.0516 - acc: 0.6163 - val_loss: 1.0577 - val_acc: 0.6256\n",
            "Epoch 776/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.0598 - acc: 0.6064 - val_loss: 1.1418 - val_acc: 0.6123\n",
            "Epoch 777/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.0999 - acc: 0.5976 - val_loss: 1.1544 - val_acc: 0.6035\n",
            "Epoch 778/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.0749 - acc: 0.6108 - val_loss: 1.0879 - val_acc: 0.6300\n",
            "Epoch 779/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.1180 - acc: 0.6042 - val_loss: 1.1214 - val_acc: 0.6256\n",
            "Epoch 780/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0983 - acc: 0.6009 - val_loss: 1.0717 - val_acc: 0.6167\n",
            "Epoch 781/1500\n",
            "907/907 [==============================] - 0s 140us/sample - loss: 1.0412 - acc: 0.6119 - val_loss: 1.0581 - val_acc: 0.6344\n",
            "Epoch 782/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.1752 - acc: 0.5513 - val_loss: 1.1387 - val_acc: 0.5991\n",
            "Epoch 783/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0971 - acc: 0.5865 - val_loss: 1.1218 - val_acc: 0.6388\n",
            "Epoch 784/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.1050 - acc: 0.5954 - val_loss: 1.1916 - val_acc: 0.5859\n",
            "Epoch 785/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.1731 - acc: 0.5546 - val_loss: 1.0793 - val_acc: 0.6388\n",
            "Epoch 786/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.1058 - acc: 0.5932 - val_loss: 1.0981 - val_acc: 0.6167\n",
            "Epoch 787/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.1131 - acc: 0.5810 - val_loss: 1.1314 - val_acc: 0.6035\n",
            "Epoch 788/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.1104 - acc: 0.5965 - val_loss: 1.0602 - val_acc: 0.6388\n",
            "Epoch 789/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.0839 - acc: 0.6020 - val_loss: 1.1342 - val_acc: 0.6300\n",
            "Epoch 790/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0889 - acc: 0.5998 - val_loss: 1.0618 - val_acc: 0.6388\n",
            "Epoch 791/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.0645 - acc: 0.6086 - val_loss: 1.0953 - val_acc: 0.6300\n",
            "Epoch 792/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.0775 - acc: 0.6020 - val_loss: 1.0953 - val_acc: 0.6564\n",
            "Epoch 793/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.0809 - acc: 0.5888 - val_loss: 1.0898 - val_acc: 0.6256\n",
            "Epoch 794/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.1303 - acc: 0.6042 - val_loss: 1.1314 - val_acc: 0.6211\n",
            "Epoch 795/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.1031 - acc: 0.5865 - val_loss: 1.1379 - val_acc: 0.6123\n",
            "Epoch 796/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.0615 - acc: 0.5976 - val_loss: 1.0565 - val_acc: 0.6167\n",
            "Epoch 797/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.0574 - acc: 0.5954 - val_loss: 1.1215 - val_acc: 0.6300\n",
            "Epoch 798/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.0862 - acc: 0.5954 - val_loss: 1.1018 - val_acc: 0.5903\n",
            "Epoch 799/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.0993 - acc: 0.6130 - val_loss: 1.1469 - val_acc: 0.6167\n",
            "Epoch 800/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.0709 - acc: 0.6053 - val_loss: 1.1276 - val_acc: 0.6211\n",
            "Epoch 801/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0963 - acc: 0.5888 - val_loss: 1.1837 - val_acc: 0.6167\n",
            "Epoch 802/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.0945 - acc: 0.5877 - val_loss: 1.1153 - val_acc: 0.6432\n",
            "Epoch 803/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.1002 - acc: 0.5832 - val_loss: 1.1254 - val_acc: 0.6035\n",
            "Epoch 804/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.0778 - acc: 0.5854 - val_loss: 1.1107 - val_acc: 0.6256\n",
            "Epoch 805/1500\n",
            "907/907 [==============================] - 0s 155us/sample - loss: 1.0950 - acc: 0.5843 - val_loss: 1.1128 - val_acc: 0.6256\n",
            "Epoch 806/1500\n",
            "907/907 [==============================] - 0s 149us/sample - loss: 1.0849 - acc: 0.5910 - val_loss: 1.1082 - val_acc: 0.6520\n",
            "Epoch 807/1500\n",
            "907/907 [==============================] - 0s 151us/sample - loss: 1.0155 - acc: 0.6251 - val_loss: 1.0589 - val_acc: 0.6564\n",
            "Epoch 808/1500\n",
            "907/907 [==============================] - 0s 150us/sample - loss: 1.0067 - acc: 0.6229 - val_loss: 1.1209 - val_acc: 0.6388\n",
            "Epoch 809/1500\n",
            "907/907 [==============================] - 0s 145us/sample - loss: 1.1187 - acc: 0.5943 - val_loss: 1.1191 - val_acc: 0.6035\n",
            "Epoch 810/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.1781 - acc: 0.5932 - val_loss: 1.1425 - val_acc: 0.6476\n",
            "Epoch 811/1500\n",
            "907/907 [==============================] - 0s 134us/sample - loss: 1.1585 - acc: 0.5877 - val_loss: 1.1220 - val_acc: 0.6256\n",
            "Epoch 812/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.0507 - acc: 0.6064 - val_loss: 1.2163 - val_acc: 0.6035\n",
            "Epoch 813/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0451 - acc: 0.5987 - val_loss: 1.0865 - val_acc: 0.6300\n",
            "Epoch 814/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.0266 - acc: 0.6119 - val_loss: 1.1550 - val_acc: 0.5903\n",
            "Epoch 815/1500\n",
            "907/907 [==============================] - 0s 139us/sample - loss: 1.0475 - acc: 0.6097 - val_loss: 1.1072 - val_acc: 0.6211\n",
            "Epoch 816/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.0598 - acc: 0.6086 - val_loss: 1.1002 - val_acc: 0.6344\n",
            "Epoch 817/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0992 - acc: 0.5921 - val_loss: 1.1115 - val_acc: 0.6344\n",
            "Epoch 818/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.1656 - acc: 0.5799 - val_loss: 1.1365 - val_acc: 0.6079\n",
            "Epoch 819/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.0392 - acc: 0.5987 - val_loss: 1.1712 - val_acc: 0.6123\n",
            "Epoch 820/1500\n",
            "907/907 [==============================] - 0s 132us/sample - loss: 1.0852 - acc: 0.6042 - val_loss: 1.0628 - val_acc: 0.6476\n",
            "Epoch 821/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.1129 - acc: 0.5843 - val_loss: 1.0618 - val_acc: 0.6476\n",
            "Epoch 822/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.1975 - acc: 0.5568 - val_loss: 1.1595 - val_acc: 0.6167\n",
            "Epoch 823/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0883 - acc: 0.6053 - val_loss: 1.0981 - val_acc: 0.6256\n",
            "Epoch 824/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.1339 - acc: 0.5678 - val_loss: 1.0842 - val_acc: 0.6344\n",
            "Epoch 825/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.0516 - acc: 0.5877 - val_loss: 1.0829 - val_acc: 0.6432\n",
            "Epoch 826/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.0602 - acc: 0.6086 - val_loss: 1.1158 - val_acc: 0.6344\n",
            "Epoch 827/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.1050 - acc: 0.6031 - val_loss: 1.1267 - val_acc: 0.6167\n",
            "Epoch 828/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 1.1140 - acc: 0.5799 - val_loss: 1.1421 - val_acc: 0.6211\n",
            "Epoch 829/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.0669 - acc: 0.5910 - val_loss: 1.1107 - val_acc: 0.6476\n",
            "Epoch 830/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.0969 - acc: 0.5888 - val_loss: 1.1654 - val_acc: 0.6123\n",
            "Epoch 831/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.0251 - acc: 0.6130 - val_loss: 1.1146 - val_acc: 0.6344\n",
            "Epoch 832/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.0481 - acc: 0.6174 - val_loss: 1.1092 - val_acc: 0.6256\n",
            "Epoch 833/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.1084 - acc: 0.5843 - val_loss: 1.0949 - val_acc: 0.6344\n",
            "Epoch 834/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.0759 - acc: 0.5832 - val_loss: 1.1693 - val_acc: 0.6035\n",
            "Epoch 835/1500\n",
            "907/907 [==============================] - 0s 145us/sample - loss: 1.0795 - acc: 0.6086 - val_loss: 1.0906 - val_acc: 0.6256\n",
            "Epoch 836/1500\n",
            "907/907 [==============================] - 0s 132us/sample - loss: 1.0570 - acc: 0.6130 - val_loss: 1.1342 - val_acc: 0.6079\n",
            "Epoch 837/1500\n",
            "907/907 [==============================] - 0s 141us/sample - loss: 1.0652 - acc: 0.6196 - val_loss: 1.1373 - val_acc: 0.6123\n",
            "Epoch 838/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.1245 - acc: 0.5711 - val_loss: 1.0532 - val_acc: 0.6432\n",
            "Epoch 839/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.0645 - acc: 0.6119 - val_loss: 1.0892 - val_acc: 0.6344\n",
            "Epoch 840/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.0675 - acc: 0.5943 - val_loss: 1.1859 - val_acc: 0.5991\n",
            "Epoch 841/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.0734 - acc: 0.6020 - val_loss: 1.1350 - val_acc: 0.6388\n",
            "Epoch 842/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.1254 - acc: 0.6075 - val_loss: 1.0873 - val_acc: 0.6300\n",
            "Epoch 843/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.0467 - acc: 0.6031 - val_loss: 1.1241 - val_acc: 0.6256\n",
            "Epoch 844/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.0875 - acc: 0.5998 - val_loss: 1.0245 - val_acc: 0.6432\n",
            "Epoch 845/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.0446 - acc: 0.6031 - val_loss: 1.0575 - val_acc: 0.6520\n",
            "Epoch 846/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.0726 - acc: 0.5888 - val_loss: 1.0614 - val_acc: 0.6476\n",
            "Epoch 847/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.1232 - acc: 0.5899 - val_loss: 1.0808 - val_acc: 0.6167\n",
            "Epoch 848/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.0432 - acc: 0.6439 - val_loss: 1.1234 - val_acc: 0.6344\n",
            "Epoch 849/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.0845 - acc: 0.6064 - val_loss: 1.1170 - val_acc: 0.6123\n",
            "Epoch 850/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.1097 - acc: 0.5788 - val_loss: 1.1386 - val_acc: 0.6256\n",
            "Epoch 851/1500\n",
            "907/907 [==============================] - 0s 135us/sample - loss: 1.0606 - acc: 0.5998 - val_loss: 1.1258 - val_acc: 0.6256\n",
            "Epoch 852/1500\n",
            "907/907 [==============================] - 0s 143us/sample - loss: 1.0500 - acc: 0.6295 - val_loss: 1.1104 - val_acc: 0.6256\n",
            "Epoch 853/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.0771 - acc: 0.6097 - val_loss: 1.1048 - val_acc: 0.6520\n",
            "Epoch 854/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.0899 - acc: 0.5888 - val_loss: 1.0802 - val_acc: 0.6388\n",
            "Epoch 855/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.0354 - acc: 0.6185 - val_loss: 1.0826 - val_acc: 0.6300\n",
            "Epoch 856/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0731 - acc: 0.5954 - val_loss: 1.1508 - val_acc: 0.5991\n",
            "Epoch 857/1500\n",
            "907/907 [==============================] - 0s 139us/sample - loss: 1.0522 - acc: 0.5965 - val_loss: 1.0994 - val_acc: 0.6432\n",
            "Epoch 858/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.0691 - acc: 0.5965 - val_loss: 1.0606 - val_acc: 0.6388\n",
            "Epoch 859/1500\n",
            "907/907 [==============================] - 0s 143us/sample - loss: 1.0379 - acc: 0.5888 - val_loss: 1.0576 - val_acc: 0.6256\n",
            "Epoch 860/1500\n",
            "907/907 [==============================] - 0s 154us/sample - loss: 1.0878 - acc: 0.5910 - val_loss: 1.1413 - val_acc: 0.6035\n",
            "Epoch 861/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.0832 - acc: 0.5888 - val_loss: 1.0997 - val_acc: 0.6388\n",
            "Epoch 862/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.1042 - acc: 0.5976 - val_loss: 1.0616 - val_acc: 0.6344\n",
            "Epoch 863/1500\n",
            "907/907 [==============================] - 0s 109us/sample - loss: 1.0411 - acc: 0.6251 - val_loss: 1.0811 - val_acc: 0.6344\n",
            "Epoch 864/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.0399 - acc: 0.6240 - val_loss: 1.1117 - val_acc: 0.6211\n",
            "Epoch 865/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.1065 - acc: 0.5821 - val_loss: 1.1070 - val_acc: 0.6256\n",
            "Epoch 866/1500\n",
            "907/907 [==============================] - 0s 131us/sample - loss: 1.0675 - acc: 0.6174 - val_loss: 1.1400 - val_acc: 0.6123\n",
            "Epoch 867/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.0868 - acc: 0.5932 - val_loss: 1.1235 - val_acc: 0.6300\n",
            "Epoch 868/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.0892 - acc: 0.6086 - val_loss: 1.1098 - val_acc: 0.6564\n",
            "Epoch 869/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.0260 - acc: 0.6064 - val_loss: 1.0798 - val_acc: 0.6300\n",
            "Epoch 870/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.0517 - acc: 0.6064 - val_loss: 1.1143 - val_acc: 0.6256\n",
            "Epoch 871/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.0261 - acc: 0.6174 - val_loss: 1.1211 - val_acc: 0.6035\n",
            "Epoch 872/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 1.0580 - acc: 0.6185 - val_loss: 1.1490 - val_acc: 0.6123\n",
            "Epoch 873/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.0117 - acc: 0.6307 - val_loss: 1.1464 - val_acc: 0.6123\n",
            "Epoch 874/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.1221 - acc: 0.5821 - val_loss: 1.0877 - val_acc: 0.6211\n",
            "Epoch 875/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.1285 - acc: 0.5766 - val_loss: 1.0522 - val_acc: 0.6388\n",
            "Epoch 876/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.0862 - acc: 0.6031 - val_loss: 1.1233 - val_acc: 0.6035\n",
            "Epoch 877/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.1128 - acc: 0.5755 - val_loss: 1.0921 - val_acc: 0.6344\n",
            "Epoch 878/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0675 - acc: 0.5921 - val_loss: 1.1052 - val_acc: 0.6167\n",
            "Epoch 879/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 1.0673 - acc: 0.5965 - val_loss: 1.1303 - val_acc: 0.6344\n",
            "Epoch 880/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.0324 - acc: 0.6075 - val_loss: 1.0745 - val_acc: 0.6476\n",
            "Epoch 881/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.1221 - acc: 0.5943 - val_loss: 1.1007 - val_acc: 0.6256\n",
            "Epoch 882/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.0054 - acc: 0.6130 - val_loss: 1.1606 - val_acc: 0.6167\n",
            "Epoch 883/1500\n",
            "907/907 [==============================] - 0s 145us/sample - loss: 1.0824 - acc: 0.6130 - val_loss: 1.0568 - val_acc: 0.6344\n",
            "Epoch 884/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.0767 - acc: 0.5854 - val_loss: 1.1430 - val_acc: 0.6123\n",
            "Epoch 885/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.0434 - acc: 0.5854 - val_loss: 1.1363 - val_acc: 0.5903\n",
            "Epoch 886/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.0352 - acc: 0.6196 - val_loss: 1.1033 - val_acc: 0.6432\n",
            "Epoch 887/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0215 - acc: 0.6130 - val_loss: 1.0876 - val_acc: 0.6035\n",
            "Epoch 888/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.0468 - acc: 0.6152 - val_loss: 1.0744 - val_acc: 0.6476\n",
            "Epoch 889/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.0792 - acc: 0.6053 - val_loss: 1.1077 - val_acc: 0.6123\n",
            "Epoch 890/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0880 - acc: 0.5899 - val_loss: 1.1688 - val_acc: 0.6035\n",
            "Epoch 891/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.0654 - acc: 0.6108 - val_loss: 1.1222 - val_acc: 0.6256\n",
            "Epoch 892/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.1289 - acc: 0.5877 - val_loss: 1.1160 - val_acc: 0.6256\n",
            "Epoch 893/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.0918 - acc: 0.6053 - val_loss: 1.1402 - val_acc: 0.6476\n",
            "Epoch 894/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.0724 - acc: 0.6097 - val_loss: 1.1040 - val_acc: 0.6476\n",
            "Epoch 895/1500\n",
            "907/907 [==============================] - 0s 140us/sample - loss: 1.0386 - acc: 0.6086 - val_loss: 1.1501 - val_acc: 0.5947\n",
            "Epoch 896/1500\n",
            "907/907 [==============================] - 0s 141us/sample - loss: 1.0724 - acc: 0.5976 - val_loss: 1.1360 - val_acc: 0.6079\n",
            "Epoch 897/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0483 - acc: 0.6031 - val_loss: 1.1035 - val_acc: 0.6079\n",
            "Epoch 898/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.0400 - acc: 0.6108 - val_loss: 1.0529 - val_acc: 0.6520\n",
            "Epoch 899/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0258 - acc: 0.6185 - val_loss: 1.0860 - val_acc: 0.6300\n",
            "Epoch 900/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.1227 - acc: 0.5799 - val_loss: 1.0900 - val_acc: 0.6476\n",
            "Epoch 901/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0488 - acc: 0.6152 - val_loss: 1.0979 - val_acc: 0.6256\n",
            "Epoch 902/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.0484 - acc: 0.6218 - val_loss: 1.0767 - val_acc: 0.6432\n",
            "Epoch 903/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.0224 - acc: 0.6174 - val_loss: 1.1137 - val_acc: 0.6388\n",
            "Epoch 904/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.1015 - acc: 0.6108 - val_loss: 1.1122 - val_acc: 0.6476\n",
            "Epoch 905/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.0812 - acc: 0.6185 - val_loss: 1.1336 - val_acc: 0.6211\n",
            "Epoch 906/1500\n",
            "907/907 [==============================] - 0s 139us/sample - loss: 1.0589 - acc: 0.5976 - val_loss: 1.1607 - val_acc: 0.5771\n",
            "Epoch 907/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.0559 - acc: 0.6020 - val_loss: 1.0900 - val_acc: 0.6344\n",
            "Epoch 908/1500\n",
            "907/907 [==============================] - 0s 151us/sample - loss: 1.0379 - acc: 0.6428 - val_loss: 1.1895 - val_acc: 0.6167\n",
            "Epoch 909/1500\n",
            "907/907 [==============================] - 0s 161us/sample - loss: 1.0589 - acc: 0.6075 - val_loss: 1.1166 - val_acc: 0.6300\n",
            "Epoch 910/1500\n",
            "907/907 [==============================] - 0s 152us/sample - loss: 1.1134 - acc: 0.6053 - val_loss: 1.1822 - val_acc: 0.6256\n",
            "Epoch 911/1500\n",
            "907/907 [==============================] - 0s 153us/sample - loss: 1.0995 - acc: 0.6009 - val_loss: 1.1466 - val_acc: 0.6256\n",
            "Epoch 912/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.0536 - acc: 0.5987 - val_loss: 1.1283 - val_acc: 0.6300\n",
            "Epoch 913/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.0842 - acc: 0.5910 - val_loss: 1.1879 - val_acc: 0.5991\n",
            "Epoch 914/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0822 - acc: 0.6020 - val_loss: 1.1557 - val_acc: 0.6123\n",
            "Epoch 915/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.0534 - acc: 0.5943 - val_loss: 1.0661 - val_acc: 0.6432\n",
            "Epoch 916/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.0595 - acc: 0.5910 - val_loss: 1.1017 - val_acc: 0.6167\n",
            "Epoch 917/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0926 - acc: 0.5921 - val_loss: 1.1137 - val_acc: 0.6167\n",
            "Epoch 918/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.0365 - acc: 0.6273 - val_loss: 1.0716 - val_acc: 0.6520\n",
            "Epoch 919/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.0360 - acc: 0.6196 - val_loss: 1.1818 - val_acc: 0.6167\n",
            "Epoch 920/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.0472 - acc: 0.6075 - val_loss: 1.1049 - val_acc: 0.6476\n",
            "Epoch 921/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 1.0615 - acc: 0.6064 - val_loss: 1.1367 - val_acc: 0.6256\n",
            "Epoch 922/1500\n",
            "907/907 [==============================] - 0s 109us/sample - loss: 1.0848 - acc: 0.6218 - val_loss: 1.0823 - val_acc: 0.6432\n",
            "Epoch 923/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.0301 - acc: 0.6284 - val_loss: 1.1251 - val_acc: 0.6476\n",
            "Epoch 924/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 1.1145 - acc: 0.5799 - val_loss: 1.0709 - val_acc: 0.6300\n",
            "Epoch 925/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.1225 - acc: 0.5921 - val_loss: 1.1367 - val_acc: 0.6123\n",
            "Epoch 926/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.0988 - acc: 0.5965 - val_loss: 1.1037 - val_acc: 0.6300\n",
            "Epoch 927/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.0762 - acc: 0.6064 - val_loss: 1.0622 - val_acc: 0.6432\n",
            "Epoch 928/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.0546 - acc: 0.5965 - val_loss: 1.0915 - val_acc: 0.6344\n",
            "Epoch 929/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.0755 - acc: 0.5921 - val_loss: 1.0987 - val_acc: 0.6388\n",
            "Epoch 930/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.0644 - acc: 0.6130 - val_loss: 1.1137 - val_acc: 0.6167\n",
            "Epoch 931/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 0.9886 - acc: 0.6439 - val_loss: 1.0759 - val_acc: 0.6388\n",
            "Epoch 932/1500\n",
            "907/907 [==============================] - 0s 138us/sample - loss: 1.0157 - acc: 0.6207 - val_loss: 1.1152 - val_acc: 0.6344\n",
            "Epoch 933/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.0425 - acc: 0.5932 - val_loss: 1.0938 - val_acc: 0.6211\n",
            "Epoch 934/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.0231 - acc: 0.6130 - val_loss: 1.1002 - val_acc: 0.6476\n",
            "Epoch 935/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.0798 - acc: 0.5976 - val_loss: 1.0635 - val_acc: 0.6564\n",
            "Epoch 936/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.0630 - acc: 0.6075 - val_loss: 1.1560 - val_acc: 0.6079\n",
            "Epoch 937/1500\n",
            "907/907 [==============================] - 0s 109us/sample - loss: 1.0430 - acc: 0.6273 - val_loss: 1.0918 - val_acc: 0.6211\n",
            "Epoch 938/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.0126 - acc: 0.6240 - val_loss: 1.1035 - val_acc: 0.6300\n",
            "Epoch 939/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.0804 - acc: 0.6174 - val_loss: 1.1187 - val_acc: 0.6344\n",
            "Epoch 940/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.0621 - acc: 0.6295 - val_loss: 1.0983 - val_acc: 0.6344\n",
            "Epoch 941/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.1186 - acc: 0.5865 - val_loss: 1.1248 - val_acc: 0.6211\n",
            "Epoch 942/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.0924 - acc: 0.5954 - val_loss: 1.0832 - val_acc: 0.6388\n",
            "Epoch 943/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.0883 - acc: 0.5943 - val_loss: 1.0932 - val_acc: 0.6388\n",
            "Epoch 944/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.0138 - acc: 0.6483 - val_loss: 1.0997 - val_acc: 0.6300\n",
            "Epoch 945/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.0428 - acc: 0.6318 - val_loss: 1.0591 - val_acc: 0.6652\n",
            "Epoch 946/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0139 - acc: 0.6130 - val_loss: 1.1322 - val_acc: 0.6079\n",
            "Epoch 947/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.0485 - acc: 0.6075 - val_loss: 1.1528 - val_acc: 0.6123\n",
            "Epoch 948/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.0991 - acc: 0.5921 - val_loss: 1.0662 - val_acc: 0.6520\n",
            "Epoch 949/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.0600 - acc: 0.6031 - val_loss: 1.0805 - val_acc: 0.6344\n",
            "Epoch 950/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.0144 - acc: 0.6273 - val_loss: 1.0699 - val_acc: 0.6520\n",
            "Epoch 951/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.0504 - acc: 0.6207 - val_loss: 1.0830 - val_acc: 0.6344\n",
            "Epoch 952/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 0.9844 - acc: 0.6351 - val_loss: 1.0453 - val_acc: 0.6696\n",
            "Epoch 953/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0413 - acc: 0.6064 - val_loss: 1.0973 - val_acc: 0.6256\n",
            "Epoch 954/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.0029 - acc: 0.6273 - val_loss: 1.1012 - val_acc: 0.6432\n",
            "Epoch 955/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.0028 - acc: 0.6450 - val_loss: 1.1093 - val_acc: 0.6388\n",
            "Epoch 956/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.0472 - acc: 0.6075 - val_loss: 1.1002 - val_acc: 0.6211\n",
            "Epoch 957/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.0578 - acc: 0.6108 - val_loss: 1.1246 - val_acc: 0.6035\n",
            "Epoch 958/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0482 - acc: 0.6152 - val_loss: 1.0889 - val_acc: 0.6300\n",
            "Epoch 959/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.0582 - acc: 0.5998 - val_loss: 1.0836 - val_acc: 0.6300\n",
            "Epoch 960/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.0161 - acc: 0.6196 - val_loss: 1.1253 - val_acc: 0.6344\n",
            "Epoch 961/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.0250 - acc: 0.6163 - val_loss: 1.1253 - val_acc: 0.6388\n",
            "Epoch 962/1500\n",
            "907/907 [==============================] - 0s 133us/sample - loss: 1.0218 - acc: 0.6439 - val_loss: 1.0898 - val_acc: 0.6167\n",
            "Epoch 963/1500\n",
            "907/907 [==============================] - 0s 148us/sample - loss: 1.0086 - acc: 0.6329 - val_loss: 1.0876 - val_acc: 0.6608\n",
            "Epoch 964/1500\n",
            "907/907 [==============================] - 0s 133us/sample - loss: 1.0348 - acc: 0.6163 - val_loss: 1.0762 - val_acc: 0.6520\n",
            "Epoch 965/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 0.9968 - acc: 0.6351 - val_loss: 1.1031 - val_acc: 0.6564\n",
            "Epoch 966/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.0074 - acc: 0.6251 - val_loss: 1.0889 - val_acc: 0.6432\n",
            "Epoch 967/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.0129 - acc: 0.6362 - val_loss: 1.0891 - val_acc: 0.6608\n",
            "Epoch 968/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.0539 - acc: 0.6174 - val_loss: 1.1738 - val_acc: 0.5991\n",
            "Epoch 969/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0864 - acc: 0.5987 - val_loss: 1.1620 - val_acc: 0.6388\n",
            "Epoch 970/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0329 - acc: 0.6185 - val_loss: 1.0860 - val_acc: 0.6344\n",
            "Epoch 971/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.0369 - acc: 0.6362 - val_loss: 1.1016 - val_acc: 0.6344\n",
            "Epoch 972/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.0290 - acc: 0.6340 - val_loss: 1.0716 - val_acc: 0.6476\n",
            "Epoch 973/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.0222 - acc: 0.6251 - val_loss: 1.0800 - val_acc: 0.6256\n",
            "Epoch 974/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.0150 - acc: 0.6307 - val_loss: 1.1373 - val_acc: 0.6564\n",
            "Epoch 975/1500\n",
            "907/907 [==============================] - 0s 139us/sample - loss: 1.0544 - acc: 0.6053 - val_loss: 1.1482 - val_acc: 0.6211\n",
            "Epoch 976/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.0135 - acc: 0.6086 - val_loss: 1.0815 - val_acc: 0.6476\n",
            "Epoch 977/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.0420 - acc: 0.6185 - val_loss: 1.1329 - val_acc: 0.6388\n",
            "Epoch 978/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.0452 - acc: 0.6229 - val_loss: 1.0860 - val_acc: 0.6344\n",
            "Epoch 979/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0491 - acc: 0.6130 - val_loss: 1.0979 - val_acc: 0.6520\n",
            "Epoch 980/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.0306 - acc: 0.6428 - val_loss: 1.1170 - val_acc: 0.6123\n",
            "Epoch 981/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0220 - acc: 0.6207 - val_loss: 1.0814 - val_acc: 0.6432\n",
            "Epoch 982/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 0.9739 - acc: 0.6461 - val_loss: 1.0974 - val_acc: 0.6476\n",
            "Epoch 983/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 0.9738 - acc: 0.6439 - val_loss: 1.0830 - val_acc: 0.6608\n",
            "Epoch 984/1500\n",
            "907/907 [==============================] - 0s 137us/sample - loss: 1.0080 - acc: 0.6196 - val_loss: 1.0816 - val_acc: 0.6608\n",
            "Epoch 985/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.0350 - acc: 0.6141 - val_loss: 1.1206 - val_acc: 0.6079\n",
            "Epoch 986/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.0587 - acc: 0.6119 - val_loss: 1.1289 - val_acc: 0.6388\n",
            "Epoch 987/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0011 - acc: 0.6273 - val_loss: 1.1050 - val_acc: 0.6652\n",
            "Epoch 988/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.0587 - acc: 0.6086 - val_loss: 1.1297 - val_acc: 0.6388\n",
            "Epoch 989/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.0487 - acc: 0.6108 - val_loss: 1.1261 - val_acc: 0.6256\n",
            "Epoch 990/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.0589 - acc: 0.6251 - val_loss: 1.1213 - val_acc: 0.6300\n",
            "Epoch 991/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.0222 - acc: 0.6196 - val_loss: 1.1048 - val_acc: 0.6300\n",
            "Epoch 992/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.0601 - acc: 0.5965 - val_loss: 1.1388 - val_acc: 0.6476\n",
            "Epoch 993/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 1.0470 - acc: 0.6273 - val_loss: 1.1285 - val_acc: 0.6123\n",
            "Epoch 994/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.0240 - acc: 0.6119 - val_loss: 1.1485 - val_acc: 0.6256\n",
            "Epoch 995/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.0478 - acc: 0.6097 - val_loss: 1.1353 - val_acc: 0.6123\n",
            "Epoch 996/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.0355 - acc: 0.6196 - val_loss: 1.1192 - val_acc: 0.6608\n",
            "Epoch 997/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0948 - acc: 0.6031 - val_loss: 1.0941 - val_acc: 0.6432\n",
            "Epoch 998/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.0492 - acc: 0.6097 - val_loss: 1.1700 - val_acc: 0.6167\n",
            "Epoch 999/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.0826 - acc: 0.5954 - val_loss: 1.1086 - val_acc: 0.6123\n",
            "Epoch 1000/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.0680 - acc: 0.6042 - val_loss: 1.1374 - val_acc: 0.6432\n",
            "Epoch 1001/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0519 - acc: 0.6119 - val_loss: 1.0981 - val_acc: 0.6256\n",
            "Epoch 1002/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.0693 - acc: 0.6086 - val_loss: 1.0756 - val_acc: 0.6696\n",
            "Epoch 1003/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.0228 - acc: 0.6284 - val_loss: 1.0530 - val_acc: 0.6652\n",
            "Epoch 1004/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.0227 - acc: 0.6229 - val_loss: 1.0624 - val_acc: 0.6608\n",
            "Epoch 1005/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.0536 - acc: 0.6064 - val_loss: 1.1258 - val_acc: 0.6211\n",
            "Epoch 1006/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 0.9897 - acc: 0.6218 - val_loss: 1.1332 - val_acc: 0.6608\n",
            "Epoch 1007/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 1.0621 - acc: 0.6174 - val_loss: 1.1483 - val_acc: 0.6300\n",
            "Epoch 1008/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.0597 - acc: 0.6185 - val_loss: 1.1345 - val_acc: 0.6256\n",
            "Epoch 1009/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.0406 - acc: 0.6152 - val_loss: 1.0753 - val_acc: 0.6608\n",
            "Epoch 1010/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.0803 - acc: 0.5899 - val_loss: 1.1620 - val_acc: 0.6123\n",
            "Epoch 1011/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.0334 - acc: 0.5954 - val_loss: 1.0802 - val_acc: 0.6740\n",
            "Epoch 1012/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.0907 - acc: 0.6163 - val_loss: 1.1124 - val_acc: 0.6300\n",
            "Epoch 1013/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.0225 - acc: 0.6307 - val_loss: 1.1647 - val_acc: 0.6211\n",
            "Epoch 1014/1500\n",
            "907/907 [==============================] - 0s 138us/sample - loss: 1.0444 - acc: 0.6097 - val_loss: 1.0551 - val_acc: 0.6652\n",
            "Epoch 1015/1500\n",
            "907/907 [==============================] - 0s 156us/sample - loss: 0.9949 - acc: 0.6207 - val_loss: 1.1471 - val_acc: 0.6520\n",
            "Epoch 1016/1500\n",
            "907/907 [==============================] - 0s 140us/sample - loss: 1.0367 - acc: 0.6152 - val_loss: 1.0973 - val_acc: 0.6432\n",
            "Epoch 1017/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.0088 - acc: 0.6318 - val_loss: 1.0969 - val_acc: 0.6476\n",
            "Epoch 1018/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.0269 - acc: 0.6152 - val_loss: 1.1323 - val_acc: 0.6608\n",
            "Epoch 1019/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.0255 - acc: 0.6262 - val_loss: 1.0972 - val_acc: 0.6520\n",
            "Epoch 1020/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.0356 - acc: 0.6273 - val_loss: 1.1473 - val_acc: 0.6167\n",
            "Epoch 1021/1500\n",
            "907/907 [==============================] - 0s 109us/sample - loss: 0.9812 - acc: 0.6218 - val_loss: 1.0914 - val_acc: 0.6564\n",
            "Epoch 1022/1500\n",
            "907/907 [==============================] - 0s 108us/sample - loss: 1.0406 - acc: 0.6009 - val_loss: 1.1430 - val_acc: 0.6300\n",
            "Epoch 1023/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.0414 - acc: 0.6130 - val_loss: 1.1871 - val_acc: 0.6300\n",
            "Epoch 1024/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 0.9942 - acc: 0.6362 - val_loss: 1.2004 - val_acc: 0.6256\n",
            "Epoch 1025/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 0.9465 - acc: 0.6516 - val_loss: 1.1529 - val_acc: 0.6388\n",
            "Epoch 1026/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 0.9525 - acc: 0.6560 - val_loss: 1.1254 - val_acc: 0.6476\n",
            "Epoch 1027/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0085 - acc: 0.6273 - val_loss: 1.1172 - val_acc: 0.6344\n",
            "Epoch 1028/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.0827 - acc: 0.6086 - val_loss: 1.1508 - val_acc: 0.6432\n",
            "Epoch 1029/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.0310 - acc: 0.6086 - val_loss: 1.1034 - val_acc: 0.6608\n",
            "Epoch 1030/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 0.9399 - acc: 0.6549 - val_loss: 1.1355 - val_acc: 0.6432\n",
            "Epoch 1031/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.0166 - acc: 0.6185 - val_loss: 1.0611 - val_acc: 0.6608\n",
            "Epoch 1032/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.0807 - acc: 0.6185 - val_loss: 1.0809 - val_acc: 0.6300\n",
            "Epoch 1033/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.0498 - acc: 0.6097 - val_loss: 1.0965 - val_acc: 0.6476\n",
            "Epoch 1034/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.0426 - acc: 0.6053 - val_loss: 1.1086 - val_acc: 0.6256\n",
            "Epoch 1035/1500\n",
            "907/907 [==============================] - 0s 146us/sample - loss: 1.0415 - acc: 0.6185 - val_loss: 1.1464 - val_acc: 0.6564\n",
            "Epoch 1036/1500\n",
            "907/907 [==============================] - 0s 138us/sample - loss: 1.0156 - acc: 0.6251 - val_loss: 1.0785 - val_acc: 0.6167\n",
            "Epoch 1037/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.0039 - acc: 0.6163 - val_loss: 1.0855 - val_acc: 0.6300\n",
            "Epoch 1038/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.0288 - acc: 0.6207 - val_loss: 1.1502 - val_acc: 0.6167\n",
            "Epoch 1039/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.0441 - acc: 0.6251 - val_loss: 1.1719 - val_acc: 0.6211\n",
            "Epoch 1040/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.0457 - acc: 0.6163 - val_loss: 1.1266 - val_acc: 0.6344\n",
            "Epoch 1041/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.0923 - acc: 0.5965 - val_loss: 1.0987 - val_acc: 0.6388\n",
            "Epoch 1042/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.0226 - acc: 0.6196 - val_loss: 1.1669 - val_acc: 0.6256\n",
            "Epoch 1043/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.0253 - acc: 0.6439 - val_loss: 1.1363 - val_acc: 0.6256\n",
            "Epoch 1044/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0131 - acc: 0.6240 - val_loss: 1.1153 - val_acc: 0.6564\n",
            "Epoch 1045/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.0871 - acc: 0.5877 - val_loss: 1.1360 - val_acc: 0.6123\n",
            "Epoch 1046/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0295 - acc: 0.6141 - val_loss: 1.1119 - val_acc: 0.6211\n",
            "Epoch 1047/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 0.9583 - acc: 0.6428 - val_loss: 1.1732 - val_acc: 0.6123\n",
            "Epoch 1048/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 1.0047 - acc: 0.6329 - val_loss: 1.0630 - val_acc: 0.6432\n",
            "Epoch 1049/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 0.9881 - acc: 0.6373 - val_loss: 1.0932 - val_acc: 0.6476\n",
            "Epoch 1050/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.0279 - acc: 0.6108 - val_loss: 1.1070 - val_acc: 0.6256\n",
            "Epoch 1051/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 1.0478 - acc: 0.6086 - val_loss: 1.1625 - val_acc: 0.6167\n",
            "Epoch 1052/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.0570 - acc: 0.6108 - val_loss: 1.1042 - val_acc: 0.6344\n",
            "Epoch 1053/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.0037 - acc: 0.6329 - val_loss: 1.0884 - val_acc: 0.6300\n",
            "Epoch 1054/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.0415 - acc: 0.6251 - val_loss: 1.1798 - val_acc: 0.6123\n",
            "Epoch 1055/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.0818 - acc: 0.6119 - val_loss: 1.1903 - val_acc: 0.6211\n",
            "Epoch 1056/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.0246 - acc: 0.6218 - val_loss: 1.0976 - val_acc: 0.6520\n",
            "Epoch 1057/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.0710 - acc: 0.6130 - val_loss: 1.1704 - val_acc: 0.6256\n",
            "Epoch 1058/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.0189 - acc: 0.6262 - val_loss: 1.0753 - val_acc: 0.6608\n",
            "Epoch 1059/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.0592 - acc: 0.6251 - val_loss: 1.1164 - val_acc: 0.6476\n",
            "Epoch 1060/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.0329 - acc: 0.6097 - val_loss: 1.1290 - val_acc: 0.6476\n",
            "Epoch 1061/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.0027 - acc: 0.6251 - val_loss: 1.1082 - val_acc: 0.6167\n",
            "Epoch 1062/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.0026 - acc: 0.6130 - val_loss: 1.0992 - val_acc: 0.6388\n",
            "Epoch 1063/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.0287 - acc: 0.6329 - val_loss: 1.1193 - val_acc: 0.6211\n",
            "Epoch 1064/1500\n",
            "907/907 [==============================] - 0s 148us/sample - loss: 1.0284 - acc: 0.6009 - val_loss: 1.0691 - val_acc: 0.6564\n",
            "Epoch 1065/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0221 - acc: 0.6439 - val_loss: 1.0646 - val_acc: 0.6740\n",
            "Epoch 1066/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 0.9964 - acc: 0.6329 - val_loss: 1.1162 - val_acc: 0.6564\n",
            "Epoch 1067/1500\n",
            "907/907 [==============================] - 0s 135us/sample - loss: 1.0545 - acc: 0.5998 - val_loss: 1.1211 - val_acc: 0.6211\n",
            "Epoch 1068/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 0.9912 - acc: 0.6284 - val_loss: 1.1080 - val_acc: 0.6740\n",
            "Epoch 1069/1500\n",
            "907/907 [==============================] - 0s 138us/sample - loss: 1.0582 - acc: 0.6163 - val_loss: 1.1810 - val_acc: 0.6123\n",
            "Epoch 1070/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 0.9426 - acc: 0.6549 - val_loss: 1.1101 - val_acc: 0.6432\n",
            "Epoch 1071/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.0119 - acc: 0.6152 - val_loss: 1.0929 - val_acc: 0.6608\n",
            "Epoch 1072/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0451 - acc: 0.5954 - val_loss: 1.1467 - val_acc: 0.6211\n",
            "Epoch 1073/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 0.9943 - acc: 0.6262 - val_loss: 1.1152 - val_acc: 0.6520\n",
            "Epoch 1074/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.0447 - acc: 0.6174 - val_loss: 1.2122 - val_acc: 0.6432\n",
            "Epoch 1075/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.0286 - acc: 0.6240 - val_loss: 1.0891 - val_acc: 0.6476\n",
            "Epoch 1076/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 1.0936 - acc: 0.6042 - val_loss: 1.1777 - val_acc: 0.6344\n",
            "Epoch 1077/1500\n",
            "907/907 [==============================] - 0s 138us/sample - loss: 1.0704 - acc: 0.5910 - val_loss: 1.1364 - val_acc: 0.6211\n",
            "Epoch 1078/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 0.9864 - acc: 0.6229 - val_loss: 1.0719 - val_acc: 0.6608\n",
            "Epoch 1079/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.0179 - acc: 0.6185 - val_loss: 1.1625 - val_acc: 0.6256\n",
            "Epoch 1080/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.0458 - acc: 0.5998 - val_loss: 1.0916 - val_acc: 0.6476\n",
            "Epoch 1081/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.0114 - acc: 0.6273 - val_loss: 1.1127 - val_acc: 0.6476\n",
            "Epoch 1082/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.0190 - acc: 0.6097 - val_loss: 1.0740 - val_acc: 0.6476\n",
            "Epoch 1083/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 0.9816 - acc: 0.6273 - val_loss: 1.0976 - val_acc: 0.6784\n",
            "Epoch 1084/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.0107 - acc: 0.6295 - val_loss: 1.0383 - val_acc: 0.6520\n",
            "Epoch 1085/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 0.9442 - acc: 0.6351 - val_loss: 1.1104 - val_acc: 0.6432\n",
            "Epoch 1086/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0458 - acc: 0.6130 - val_loss: 1.1563 - val_acc: 0.6300\n",
            "Epoch 1087/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.0252 - acc: 0.6218 - val_loss: 1.1126 - val_acc: 0.6652\n",
            "Epoch 1088/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.0109 - acc: 0.6042 - val_loss: 1.0691 - val_acc: 0.6652\n",
            "Epoch 1089/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0040 - acc: 0.6174 - val_loss: 1.0987 - val_acc: 0.6344\n",
            "Epoch 1090/1500\n",
            "907/907 [==============================] - 0s 158us/sample - loss: 1.0188 - acc: 0.6340 - val_loss: 1.1863 - val_acc: 0.6167\n",
            "Epoch 1091/1500\n",
            "907/907 [==============================] - 0s 161us/sample - loss: 0.9937 - acc: 0.6141 - val_loss: 1.0720 - val_acc: 0.6388\n",
            "Epoch 1092/1500\n",
            "907/907 [==============================] - 0s 166us/sample - loss: 0.9997 - acc: 0.6262 - val_loss: 1.0845 - val_acc: 0.6608\n",
            "Epoch 1093/1500\n",
            "907/907 [==============================] - 0s 163us/sample - loss: 1.0244 - acc: 0.6229 - val_loss: 1.0721 - val_acc: 0.6564\n",
            "Epoch 1094/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.0604 - acc: 0.6031 - val_loss: 1.1078 - val_acc: 0.6256\n",
            "Epoch 1095/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.0079 - acc: 0.6218 - val_loss: 1.0614 - val_acc: 0.6608\n",
            "Epoch 1096/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 1.0185 - acc: 0.6262 - val_loss: 1.0633 - val_acc: 0.6476\n",
            "Epoch 1097/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 0.9892 - acc: 0.6262 - val_loss: 1.1098 - val_acc: 0.6608\n",
            "Epoch 1098/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.0393 - acc: 0.6141 - val_loss: 1.1279 - val_acc: 0.6476\n",
            "Epoch 1099/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0934 - acc: 0.6318 - val_loss: 1.1157 - val_acc: 0.6520\n",
            "Epoch 1100/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.0719 - acc: 0.6108 - val_loss: 1.1403 - val_acc: 0.6300\n",
            "Epoch 1101/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.0341 - acc: 0.6218 - val_loss: 1.1489 - val_acc: 0.6123\n",
            "Epoch 1102/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.0453 - acc: 0.6229 - val_loss: 1.1027 - val_acc: 0.6564\n",
            "Epoch 1103/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 1.0266 - acc: 0.6318 - val_loss: 1.1014 - val_acc: 0.6476\n",
            "Epoch 1104/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.0292 - acc: 0.6218 - val_loss: 1.1449 - val_acc: 0.6167\n",
            "Epoch 1105/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0474 - acc: 0.6196 - val_loss: 1.0891 - val_acc: 0.6476\n",
            "Epoch 1106/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.0101 - acc: 0.6240 - val_loss: 1.0813 - val_acc: 0.6652\n",
            "Epoch 1107/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0380 - acc: 0.6141 - val_loss: 1.1393 - val_acc: 0.6300\n",
            "Epoch 1108/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.0122 - acc: 0.6395 - val_loss: 1.1124 - val_acc: 0.6211\n",
            "Epoch 1109/1500\n",
            "907/907 [==============================] - 0s 141us/sample - loss: 1.0113 - acc: 0.6373 - val_loss: 1.2123 - val_acc: 0.6300\n",
            "Epoch 1110/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.0061 - acc: 0.6229 - val_loss: 1.1905 - val_acc: 0.6344\n",
            "Epoch 1111/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.0241 - acc: 0.6262 - val_loss: 1.1366 - val_acc: 0.6344\n",
            "Epoch 1112/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.0618 - acc: 0.6119 - val_loss: 1.1738 - val_acc: 0.6256\n",
            "Epoch 1113/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 0.9959 - acc: 0.6273 - val_loss: 1.1060 - val_acc: 0.6608\n",
            "Epoch 1114/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 0.9982 - acc: 0.6229 - val_loss: 1.1009 - val_acc: 0.6388\n",
            "Epoch 1115/1500\n",
            "907/907 [==============================] - 0s 147us/sample - loss: 1.0468 - acc: 0.6218 - val_loss: 1.0897 - val_acc: 0.6564\n",
            "Epoch 1116/1500\n",
            "907/907 [==============================] - 0s 142us/sample - loss: 1.0051 - acc: 0.6273 - val_loss: 1.1486 - val_acc: 0.6300\n",
            "Epoch 1117/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 0.9885 - acc: 0.6152 - val_loss: 1.1689 - val_acc: 0.6211\n",
            "Epoch 1118/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 0.9895 - acc: 0.6351 - val_loss: 1.1129 - val_acc: 0.6476\n",
            "Epoch 1119/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.0177 - acc: 0.6262 - val_loss: 1.1135 - val_acc: 0.6388\n",
            "Epoch 1120/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 0.9739 - acc: 0.6527 - val_loss: 1.1249 - val_acc: 0.6388\n",
            "Epoch 1121/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 0.9910 - acc: 0.6163 - val_loss: 1.1078 - val_acc: 0.6740\n",
            "Epoch 1122/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.0369 - acc: 0.6185 - val_loss: 1.0959 - val_acc: 0.6608\n",
            "Epoch 1123/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0205 - acc: 0.6229 - val_loss: 1.1334 - val_acc: 0.6608\n",
            "Epoch 1124/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.0275 - acc: 0.6351 - val_loss: 1.0934 - val_acc: 0.6388\n",
            "Epoch 1125/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.0155 - acc: 0.6174 - val_loss: 1.1190 - val_acc: 0.6652\n",
            "Epoch 1126/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0350 - acc: 0.6196 - val_loss: 1.0968 - val_acc: 0.6564\n",
            "Epoch 1127/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0089 - acc: 0.6384 - val_loss: 1.0672 - val_acc: 0.6608\n",
            "Epoch 1128/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.0001 - acc: 0.6229 - val_loss: 1.0942 - val_acc: 0.6784\n",
            "Epoch 1129/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0259 - acc: 0.5965 - val_loss: 1.1112 - val_acc: 0.6520\n",
            "Epoch 1130/1500\n",
            "907/907 [==============================] - 0s 141us/sample - loss: 0.9961 - acc: 0.6351 - val_loss: 1.1011 - val_acc: 0.6828\n",
            "Epoch 1131/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0501 - acc: 0.5976 - val_loss: 1.1215 - val_acc: 0.6432\n",
            "Epoch 1132/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.0688 - acc: 0.5910 - val_loss: 1.1397 - val_acc: 0.6564\n",
            "Epoch 1133/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0043 - acc: 0.6428 - val_loss: 1.0753 - val_acc: 0.6696\n",
            "Epoch 1134/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.0471 - acc: 0.6031 - val_loss: 1.0772 - val_acc: 0.6652\n",
            "Epoch 1135/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 0.9890 - acc: 0.6384 - val_loss: 1.1250 - val_acc: 0.6740\n",
            "Epoch 1136/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0474 - acc: 0.6086 - val_loss: 1.0891 - val_acc: 0.6476\n",
            "Epoch 1137/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 0.9867 - acc: 0.6329 - val_loss: 1.0613 - val_acc: 0.6916\n",
            "Epoch 1138/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 0.9529 - acc: 0.6516 - val_loss: 1.1390 - val_acc: 0.6388\n",
            "Epoch 1139/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 0.9774 - acc: 0.6571 - val_loss: 1.1143 - val_acc: 0.6432\n",
            "Epoch 1140/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 1.0717 - acc: 0.6185 - val_loss: 1.0763 - val_acc: 0.6608\n",
            "Epoch 1141/1500\n",
            "907/907 [==============================] - 0s 144us/sample - loss: 0.9894 - acc: 0.6307 - val_loss: 1.0761 - val_acc: 0.6740\n",
            "Epoch 1142/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 0.9877 - acc: 0.6340 - val_loss: 1.1031 - val_acc: 0.6476\n",
            "Epoch 1143/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 0.9712 - acc: 0.6450 - val_loss: 1.0898 - val_acc: 0.6520\n",
            "Epoch 1144/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 0.9550 - acc: 0.6582 - val_loss: 1.0798 - val_acc: 0.6652\n",
            "Epoch 1145/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.0362 - acc: 0.6196 - val_loss: 1.1212 - val_acc: 0.6432\n",
            "Epoch 1146/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.0015 - acc: 0.6295 - val_loss: 1.0797 - val_acc: 0.6696\n",
            "Epoch 1147/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 0.9792 - acc: 0.6318 - val_loss: 1.0913 - val_acc: 0.6916\n",
            "Epoch 1148/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 0.9977 - acc: 0.6373 - val_loss: 1.2222 - val_acc: 0.6256\n",
            "Epoch 1149/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.0183 - acc: 0.6119 - val_loss: 1.1191 - val_acc: 0.6344\n",
            "Epoch 1150/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 0.9761 - acc: 0.6318 - val_loss: 1.0702 - val_acc: 0.6652\n",
            "Epoch 1151/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.0001 - acc: 0.6218 - val_loss: 1.1343 - val_acc: 0.6432\n",
            "Epoch 1152/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.0286 - acc: 0.6340 - val_loss: 1.1269 - val_acc: 0.6388\n",
            "Epoch 1153/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.0208 - acc: 0.6472 - val_loss: 1.1200 - val_acc: 0.6520\n",
            "Epoch 1154/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.0200 - acc: 0.6163 - val_loss: 1.1413 - val_acc: 0.6388\n",
            "Epoch 1155/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 0.9463 - acc: 0.6494 - val_loss: 1.1329 - val_acc: 0.6388\n",
            "Epoch 1156/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.0302 - acc: 0.6240 - val_loss: 1.1822 - val_acc: 0.6520\n",
            "Epoch 1157/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.0816 - acc: 0.5854 - val_loss: 1.0931 - val_acc: 0.6388\n",
            "Epoch 1158/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.0179 - acc: 0.6262 - val_loss: 1.1393 - val_acc: 0.6652\n",
            "Epoch 1159/1500\n",
            "907/907 [==============================] - 0s 133us/sample - loss: 0.9854 - acc: 0.6185 - val_loss: 1.1639 - val_acc: 0.6300\n",
            "Epoch 1160/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.0226 - acc: 0.6108 - val_loss: 1.1315 - val_acc: 0.6696\n",
            "Epoch 1161/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 0.9731 - acc: 0.6340 - val_loss: 1.1053 - val_acc: 0.6388\n",
            "Epoch 1162/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.0179 - acc: 0.6284 - val_loss: 1.0658 - val_acc: 0.6564\n",
            "Epoch 1163/1500\n",
            "907/907 [==============================] - 0s 142us/sample - loss: 0.9936 - acc: 0.6163 - val_loss: 1.0814 - val_acc: 0.6652\n",
            "Epoch 1164/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.0366 - acc: 0.6086 - val_loss: 1.0887 - val_acc: 0.6476\n",
            "Epoch 1165/1500\n",
            "907/907 [==============================] - 0s 131us/sample - loss: 1.0380 - acc: 0.6141 - val_loss: 1.1549 - val_acc: 0.6123\n",
            "Epoch 1166/1500\n",
            "907/907 [==============================] - 0s 141us/sample - loss: 0.9933 - acc: 0.6406 - val_loss: 1.0460 - val_acc: 0.6608\n",
            "Epoch 1167/1500\n",
            "907/907 [==============================] - 0s 148us/sample - loss: 1.0712 - acc: 0.6042 - val_loss: 1.1969 - val_acc: 0.6432\n",
            "Epoch 1168/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 0.9692 - acc: 0.6461 - val_loss: 1.0965 - val_acc: 0.6564\n",
            "Epoch 1169/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.0186 - acc: 0.6196 - val_loss: 1.1191 - val_acc: 0.6608\n",
            "Epoch 1170/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 0.9881 - acc: 0.6174 - val_loss: 1.0882 - val_acc: 0.6872\n",
            "Epoch 1171/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 0.9621 - acc: 0.6384 - val_loss: 1.1567 - val_acc: 0.6432\n",
            "Epoch 1172/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0193 - acc: 0.6152 - val_loss: 1.1649 - val_acc: 0.6520\n",
            "Epoch 1173/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 0.9896 - acc: 0.6461 - val_loss: 1.1012 - val_acc: 0.6608\n",
            "Epoch 1174/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 0.9243 - acc: 0.6549 - val_loss: 1.0789 - val_acc: 0.6828\n",
            "Epoch 1175/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0197 - acc: 0.6262 - val_loss: 1.1101 - val_acc: 0.6520\n",
            "Epoch 1176/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 0.9541 - acc: 0.6406 - val_loss: 1.1143 - val_acc: 0.6344\n",
            "Epoch 1177/1500\n",
            "907/907 [==============================] - 0s 133us/sample - loss: 0.9787 - acc: 0.6483 - val_loss: 1.0784 - val_acc: 0.6872\n",
            "Epoch 1178/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.0373 - acc: 0.6218 - val_loss: 1.0967 - val_acc: 0.6608\n",
            "Epoch 1179/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 0.9334 - acc: 0.6637 - val_loss: 1.1137 - val_acc: 0.6652\n",
            "Epoch 1180/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.0259 - acc: 0.6108 - val_loss: 1.1181 - val_acc: 0.6476\n",
            "Epoch 1181/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 0.9429 - acc: 0.6582 - val_loss: 1.1416 - val_acc: 0.6211\n",
            "Epoch 1182/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.0255 - acc: 0.6064 - val_loss: 1.1687 - val_acc: 0.6211\n",
            "Epoch 1183/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 0.9931 - acc: 0.6174 - val_loss: 1.1124 - val_acc: 0.6784\n",
            "Epoch 1184/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0188 - acc: 0.6351 - val_loss: 1.1087 - val_acc: 0.6916\n",
            "Epoch 1185/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.0187 - acc: 0.6284 - val_loss: 1.1176 - val_acc: 0.6652\n",
            "Epoch 1186/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 0.9700 - acc: 0.6307 - val_loss: 1.0994 - val_acc: 0.6432\n",
            "Epoch 1187/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.0304 - acc: 0.6196 - val_loss: 1.1548 - val_acc: 0.6123\n",
            "Epoch 1188/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 0.9888 - acc: 0.6351 - val_loss: 1.0836 - val_acc: 0.6740\n",
            "Epoch 1189/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.0101 - acc: 0.6284 - val_loss: 1.1139 - val_acc: 0.6652\n",
            "Epoch 1190/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 1.0544 - acc: 0.6130 - val_loss: 1.0885 - val_acc: 0.6256\n",
            "Epoch 1191/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 0.9940 - acc: 0.6207 - val_loss: 1.0745 - val_acc: 0.6432\n",
            "Epoch 1192/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 1.0298 - acc: 0.6229 - val_loss: 1.1586 - val_acc: 0.6388\n",
            "Epoch 1193/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 0.9730 - acc: 0.6439 - val_loss: 1.1059 - val_acc: 0.6564\n",
            "Epoch 1194/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.0144 - acc: 0.6273 - val_loss: 1.1313 - val_acc: 0.6564\n",
            "Epoch 1195/1500\n",
            "907/907 [==============================] - 0s 132us/sample - loss: 0.9324 - acc: 0.6472 - val_loss: 1.1039 - val_acc: 0.6520\n",
            "Epoch 1196/1500\n",
            "907/907 [==============================] - 0s 131us/sample - loss: 1.0638 - acc: 0.6196 - val_loss: 1.2127 - val_acc: 0.6300\n",
            "Epoch 1197/1500\n",
            "907/907 [==============================] - 0s 133us/sample - loss: 1.0230 - acc: 0.6185 - val_loss: 1.1517 - val_acc: 0.6388\n",
            "Epoch 1198/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.0509 - acc: 0.6064 - val_loss: 1.1792 - val_acc: 0.6300\n",
            "Epoch 1199/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.0352 - acc: 0.6229 - val_loss: 1.1329 - val_acc: 0.6300\n",
            "Epoch 1200/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.1081 - acc: 0.5921 - val_loss: 1.1073 - val_acc: 0.6476\n",
            "Epoch 1201/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.0963 - acc: 0.5965 - val_loss: 1.0455 - val_acc: 0.6916\n",
            "Epoch 1202/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.0068 - acc: 0.6362 - val_loss: 1.1138 - val_acc: 0.6344\n",
            "Epoch 1203/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 0.9537 - acc: 0.6505 - val_loss: 1.0979 - val_acc: 0.6476\n",
            "Epoch 1204/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.0405 - acc: 0.6284 - val_loss: 1.1281 - val_acc: 0.6652\n",
            "Epoch 1205/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0446 - acc: 0.6086 - val_loss: 1.1213 - val_acc: 0.6476\n",
            "Epoch 1206/1500\n",
            "907/907 [==============================] - 0s 137us/sample - loss: 1.0140 - acc: 0.6108 - val_loss: 1.0827 - val_acc: 0.6608\n",
            "Epoch 1207/1500\n",
            "907/907 [==============================] - 0s 131us/sample - loss: 1.0194 - acc: 0.6163 - val_loss: 1.1178 - val_acc: 0.6344\n",
            "Epoch 1208/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.0174 - acc: 0.6340 - val_loss: 1.1468 - val_acc: 0.6035\n",
            "Epoch 1209/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.0125 - acc: 0.6251 - val_loss: 1.1015 - val_acc: 0.6344\n",
            "Epoch 1210/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 0.9600 - acc: 0.6373 - val_loss: 1.1429 - val_acc: 0.6211\n",
            "Epoch 1211/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.0088 - acc: 0.6340 - val_loss: 1.0987 - val_acc: 0.6608\n",
            "Epoch 1212/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.0258 - acc: 0.6218 - val_loss: 1.1105 - val_acc: 0.6784\n",
            "Epoch 1213/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.0096 - acc: 0.6273 - val_loss: 1.1267 - val_acc: 0.6520\n",
            "Epoch 1214/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 0.9846 - acc: 0.6307 - val_loss: 1.1503 - val_acc: 0.6564\n",
            "Epoch 1215/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0023 - acc: 0.6307 - val_loss: 1.1726 - val_acc: 0.6344\n",
            "Epoch 1216/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0242 - acc: 0.6251 - val_loss: 1.0873 - val_acc: 0.6740\n",
            "Epoch 1217/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 0.9879 - acc: 0.6251 - val_loss: 1.0767 - val_acc: 0.6608\n",
            "Epoch 1218/1500\n",
            "907/907 [==============================] - 0s 132us/sample - loss: 0.9899 - acc: 0.6351 - val_loss: 1.0926 - val_acc: 0.6696\n",
            "Epoch 1219/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.0228 - acc: 0.6196 - val_loss: 1.1454 - val_acc: 0.6520\n",
            "Epoch 1220/1500\n",
            "907/907 [==============================] - 0s 141us/sample - loss: 1.0206 - acc: 0.6251 - val_loss: 1.1015 - val_acc: 0.6608\n",
            "Epoch 1221/1500\n",
            "907/907 [==============================] - 0s 163us/sample - loss: 0.9850 - acc: 0.6505 - val_loss: 1.1302 - val_acc: 0.6432\n",
            "Epoch 1222/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 0.9792 - acc: 0.6439 - val_loss: 1.1711 - val_acc: 0.5991\n",
            "Epoch 1223/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 0.9894 - acc: 0.6395 - val_loss: 1.1226 - val_acc: 0.6388\n",
            "Epoch 1224/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.0208 - acc: 0.6384 - val_loss: 1.0699 - val_acc: 0.6784\n",
            "Epoch 1225/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.0466 - acc: 0.6174 - val_loss: 1.0710 - val_acc: 0.6696\n",
            "Epoch 1226/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0100 - acc: 0.6329 - val_loss: 1.1276 - val_acc: 0.6520\n",
            "Epoch 1227/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0242 - acc: 0.6163 - val_loss: 1.0964 - val_acc: 0.6564\n",
            "Epoch 1228/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.0042 - acc: 0.6240 - val_loss: 1.1469 - val_acc: 0.6564\n",
            "Epoch 1229/1500\n",
            "907/907 [==============================] - 0s 132us/sample - loss: 1.0419 - acc: 0.6108 - val_loss: 1.0975 - val_acc: 0.6652\n",
            "Epoch 1230/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 1.0378 - acc: 0.6141 - val_loss: 1.2150 - val_acc: 0.6167\n",
            "Epoch 1231/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0943 - acc: 0.6141 - val_loss: 1.1312 - val_acc: 0.6344\n",
            "Epoch 1232/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 0.9962 - acc: 0.6351 - val_loss: 1.1337 - val_acc: 0.6564\n",
            "Epoch 1233/1500\n",
            "907/907 [==============================] - 0s 132us/sample - loss: 1.0677 - acc: 0.6185 - val_loss: 1.1324 - val_acc: 0.6388\n",
            "Epoch 1234/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 0.9836 - acc: 0.6318 - val_loss: 1.0757 - val_acc: 0.6784\n",
            "Epoch 1235/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 0.9571 - acc: 0.6483 - val_loss: 1.0875 - val_acc: 0.6344\n",
            "Epoch 1236/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.0191 - acc: 0.6340 - val_loss: 1.1292 - val_acc: 0.6432\n",
            "Epoch 1237/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0374 - acc: 0.6185 - val_loss: 1.0849 - val_acc: 0.6740\n",
            "Epoch 1238/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 0.9481 - acc: 0.6505 - val_loss: 1.1201 - val_acc: 0.6696\n",
            "Epoch 1239/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 0.9490 - acc: 0.6428 - val_loss: 1.1396 - val_acc: 0.6476\n",
            "Epoch 1240/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.0140 - acc: 0.6251 - val_loss: 1.1365 - val_acc: 0.6740\n",
            "Epoch 1241/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 0.9669 - acc: 0.6549 - val_loss: 1.0684 - val_acc: 0.6828\n",
            "Epoch 1242/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 0.9702 - acc: 0.6472 - val_loss: 1.0433 - val_acc: 0.6872\n",
            "Epoch 1243/1500\n",
            "907/907 [==============================] - 0s 161us/sample - loss: 1.0020 - acc: 0.6362 - val_loss: 1.0653 - val_acc: 0.6608\n",
            "Epoch 1244/1500\n",
            "907/907 [==============================] - 0s 147us/sample - loss: 1.0165 - acc: 0.6108 - val_loss: 1.1236 - val_acc: 0.6696\n",
            "Epoch 1245/1500\n",
            "907/907 [==============================] - 0s 152us/sample - loss: 0.9838 - acc: 0.6351 - val_loss: 1.0983 - val_acc: 0.6608\n",
            "Epoch 1246/1500\n",
            "907/907 [==============================] - 0s 163us/sample - loss: 1.0417 - acc: 0.6174 - val_loss: 1.1394 - val_acc: 0.6344\n",
            "Epoch 1247/1500\n",
            "907/907 [==============================] - 0s 150us/sample - loss: 1.0169 - acc: 0.6240 - val_loss: 1.1051 - val_acc: 0.6520\n",
            "Epoch 1248/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 0.9301 - acc: 0.6560 - val_loss: 1.1114 - val_acc: 0.6520\n",
            "Epoch 1249/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 0.9515 - acc: 0.6670 - val_loss: 1.0911 - val_acc: 0.6300\n",
            "Epoch 1250/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 0.9586 - acc: 0.6428 - val_loss: 1.0746 - val_acc: 0.6696\n",
            "Epoch 1251/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.0040 - acc: 0.6295 - val_loss: 1.1248 - val_acc: 0.6740\n",
            "Epoch 1252/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0235 - acc: 0.6351 - val_loss: 1.0935 - val_acc: 0.6432\n",
            "Epoch 1253/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 0.9821 - acc: 0.6307 - val_loss: 1.1264 - val_acc: 0.6256\n",
            "Epoch 1254/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.0210 - acc: 0.6196 - val_loss: 1.1130 - val_acc: 0.6344\n",
            "Epoch 1255/1500\n",
            "907/907 [==============================] - 0s 149us/sample - loss: 0.9764 - acc: 0.6439 - val_loss: 1.1294 - val_acc: 0.6344\n",
            "Epoch 1256/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 0.9433 - acc: 0.6461 - val_loss: 1.1247 - val_acc: 0.6608\n",
            "Epoch 1257/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 1.0025 - acc: 0.6362 - val_loss: 1.1204 - val_acc: 0.6300\n",
            "Epoch 1258/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 0.9163 - acc: 0.6505 - val_loss: 1.0855 - val_acc: 0.6696\n",
            "Epoch 1259/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 0.9968 - acc: 0.6185 - val_loss: 1.0927 - val_acc: 0.6696\n",
            "Epoch 1260/1500\n",
            "907/907 [==============================] - 0s 109us/sample - loss: 0.9884 - acc: 0.6362 - val_loss: 1.1005 - val_acc: 0.6564\n",
            "Epoch 1261/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 0.9547 - acc: 0.6494 - val_loss: 1.1233 - val_acc: 0.6564\n",
            "Epoch 1262/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 0.9413 - acc: 0.6637 - val_loss: 1.1174 - val_acc: 0.6696\n",
            "Epoch 1263/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 0.9698 - acc: 0.6295 - val_loss: 1.1297 - val_acc: 0.6520\n",
            "Epoch 1264/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 0.9283 - acc: 0.6549 - val_loss: 1.1423 - val_acc: 0.6476\n",
            "Epoch 1265/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0015 - acc: 0.6130 - val_loss: 1.1184 - val_acc: 0.6432\n",
            "Epoch 1266/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.0136 - acc: 0.6439 - val_loss: 1.1137 - val_acc: 0.6652\n",
            "Epoch 1267/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 0.9582 - acc: 0.6461 - val_loss: 1.1837 - val_acc: 0.6300\n",
            "Epoch 1268/1500\n",
            "907/907 [==============================] - 0s 147us/sample - loss: 1.0347 - acc: 0.6340 - val_loss: 1.1790 - val_acc: 0.6476\n",
            "Epoch 1269/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.0046 - acc: 0.6284 - val_loss: 1.0823 - val_acc: 0.6784\n",
            "Epoch 1270/1500\n",
            "907/907 [==============================] - 0s 140us/sample - loss: 0.9959 - acc: 0.6262 - val_loss: 1.1276 - val_acc: 0.6388\n",
            "Epoch 1271/1500\n",
            "907/907 [==============================] - 0s 161us/sample - loss: 1.0114 - acc: 0.6240 - val_loss: 1.1598 - val_acc: 0.6432\n",
            "Epoch 1272/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.0011 - acc: 0.6240 - val_loss: 1.1286 - val_acc: 0.6564\n",
            "Epoch 1273/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 0.9374 - acc: 0.6692 - val_loss: 1.1090 - val_acc: 0.6432\n",
            "Epoch 1274/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 0.9591 - acc: 0.6251 - val_loss: 1.1805 - val_acc: 0.6344\n",
            "Epoch 1275/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 0.9878 - acc: 0.6251 - val_loss: 1.0936 - val_acc: 0.6520\n",
            "Epoch 1276/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 0.9474 - acc: 0.6406 - val_loss: 1.1559 - val_acc: 0.6344\n",
            "Epoch 1277/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 0.9673 - acc: 0.6428 - val_loss: 1.1120 - val_acc: 0.6344\n",
            "Epoch 1278/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 0.9896 - acc: 0.6229 - val_loss: 1.1957 - val_acc: 0.6520\n",
            "Epoch 1279/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.0459 - acc: 0.6329 - val_loss: 1.1431 - val_acc: 0.6608\n",
            "Epoch 1280/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.0079 - acc: 0.6229 - val_loss: 1.1303 - val_acc: 0.6388\n",
            "Epoch 1281/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 0.9739 - acc: 0.6284 - val_loss: 1.1452 - val_acc: 0.6432\n",
            "Epoch 1282/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.0205 - acc: 0.6307 - val_loss: 1.1525 - val_acc: 0.6476\n",
            "Epoch 1283/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 0.9291 - acc: 0.6450 - val_loss: 1.0989 - val_acc: 0.6652\n",
            "Epoch 1284/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 1.0097 - acc: 0.6009 - val_loss: 1.0880 - val_acc: 0.6652\n",
            "Epoch 1285/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0216 - acc: 0.6196 - val_loss: 1.1926 - val_acc: 0.6167\n",
            "Epoch 1286/1500\n",
            "907/907 [==============================] - 0s 108us/sample - loss: 0.9964 - acc: 0.6240 - val_loss: 1.0949 - val_acc: 0.6608\n",
            "Epoch 1287/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 0.9715 - acc: 0.6428 - val_loss: 1.0978 - val_acc: 0.6564\n",
            "Epoch 1288/1500\n",
            "907/907 [==============================] - 0s 108us/sample - loss: 0.9473 - acc: 0.6472 - val_loss: 1.1846 - val_acc: 0.6564\n",
            "Epoch 1289/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 0.9832 - acc: 0.6428 - val_loss: 1.0867 - val_acc: 0.6476\n",
            "Epoch 1290/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.0283 - acc: 0.6373 - val_loss: 1.0775 - val_acc: 0.6696\n",
            "Epoch 1291/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 0.9833 - acc: 0.6472 - val_loss: 1.1535 - val_acc: 0.6388\n",
            "Epoch 1292/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 0.9823 - acc: 0.6439 - val_loss: 1.1103 - val_acc: 0.6740\n",
            "Epoch 1293/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 0.9423 - acc: 0.6450 - val_loss: 1.1391 - val_acc: 0.6256\n",
            "Epoch 1294/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 0.9585 - acc: 0.6240 - val_loss: 1.1017 - val_acc: 0.6784\n",
            "Epoch 1295/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.0120 - acc: 0.6251 - val_loss: 1.1071 - val_acc: 0.6608\n",
            "Epoch 1296/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.0140 - acc: 0.6229 - val_loss: 1.1087 - val_acc: 0.6256\n",
            "Epoch 1297/1500\n",
            "907/907 [==============================] - 0s 142us/sample - loss: 0.9844 - acc: 0.6417 - val_loss: 1.0828 - val_acc: 0.6432\n",
            "Epoch 1298/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 0.8992 - acc: 0.6825 - val_loss: 1.1044 - val_acc: 0.6652\n",
            "Epoch 1299/1500\n",
            "907/907 [==============================] - 0s 131us/sample - loss: 1.0152 - acc: 0.6240 - val_loss: 1.1175 - val_acc: 0.6520\n",
            "Epoch 1300/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 0.9597 - acc: 0.6340 - val_loss: 1.1721 - val_acc: 0.6520\n",
            "Epoch 1301/1500\n",
            "907/907 [==============================] - 0s 130us/sample - loss: 1.0021 - acc: 0.6351 - val_loss: 1.0959 - val_acc: 0.6608\n",
            "Epoch 1302/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 0.9117 - acc: 0.6461 - val_loss: 1.1378 - val_acc: 0.6344\n",
            "Epoch 1303/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 0.9550 - acc: 0.6516 - val_loss: 1.0880 - val_acc: 0.6828\n",
            "Epoch 1304/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 0.9548 - acc: 0.6505 - val_loss: 1.1454 - val_acc: 0.6564\n",
            "Epoch 1305/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 0.9813 - acc: 0.6406 - val_loss: 1.1448 - val_acc: 0.6696\n",
            "Epoch 1306/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.0034 - acc: 0.6251 - val_loss: 1.0848 - val_acc: 0.6652\n",
            "Epoch 1307/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0105 - acc: 0.6152 - val_loss: 1.1050 - val_acc: 0.6784\n",
            "Epoch 1308/1500\n",
            "907/907 [==============================] - 0s 138us/sample - loss: 0.9574 - acc: 0.6472 - val_loss: 1.1069 - val_acc: 0.6828\n",
            "Epoch 1309/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0262 - acc: 0.6152 - val_loss: 1.1092 - val_acc: 0.6652\n",
            "Epoch 1310/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 0.9631 - acc: 0.6560 - val_loss: 1.1294 - val_acc: 0.6520\n",
            "Epoch 1311/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 0.9533 - acc: 0.6340 - val_loss: 1.0781 - val_acc: 0.6564\n",
            "Epoch 1312/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 0.9991 - acc: 0.6329 - val_loss: 1.0974 - val_acc: 0.6696\n",
            "Epoch 1313/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 0.9726 - acc: 0.6516 - val_loss: 1.1286 - val_acc: 0.6432\n",
            "Epoch 1314/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 1.0245 - acc: 0.6318 - val_loss: 1.1260 - val_acc: 0.6608\n",
            "Epoch 1315/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 0.9753 - acc: 0.6295 - val_loss: 1.1436 - val_acc: 0.6696\n",
            "Epoch 1316/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0341 - acc: 0.6064 - val_loss: 1.1077 - val_acc: 0.6828\n",
            "Epoch 1317/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.0129 - acc: 0.6229 - val_loss: 1.0789 - val_acc: 0.6696\n",
            "Epoch 1318/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0126 - acc: 0.6218 - val_loss: 1.1534 - val_acc: 0.6256\n",
            "Epoch 1319/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 0.9794 - acc: 0.6417 - val_loss: 1.1580 - val_acc: 0.6476\n",
            "Epoch 1320/1500\n",
            "907/907 [==============================] - 0s 160us/sample - loss: 0.9548 - acc: 0.6428 - val_loss: 1.1206 - val_acc: 0.6696\n",
            "Epoch 1321/1500\n",
            "907/907 [==============================] - 0s 139us/sample - loss: 0.9651 - acc: 0.6428 - val_loss: 1.1378 - val_acc: 0.6652\n",
            "Epoch 1322/1500\n",
            "907/907 [==============================] - 0s 149us/sample - loss: 0.9978 - acc: 0.6240 - val_loss: 1.1265 - val_acc: 0.6652\n",
            "Epoch 1323/1500\n",
            "907/907 [==============================] - 0s 176us/sample - loss: 1.0139 - acc: 0.6284 - val_loss: 1.1388 - val_acc: 0.6696\n",
            "Epoch 1324/1500\n",
            "907/907 [==============================] - 0s 162us/sample - loss: 0.9654 - acc: 0.6549 - val_loss: 1.1071 - val_acc: 0.6520\n",
            "Epoch 1325/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 0.9868 - acc: 0.6307 - val_loss: 1.1473 - val_acc: 0.6520\n",
            "Epoch 1326/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 0.9959 - acc: 0.6329 - val_loss: 1.1338 - val_acc: 0.6608\n",
            "Epoch 1327/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 0.9954 - acc: 0.6362 - val_loss: 1.1368 - val_acc: 0.6608\n",
            "Epoch 1328/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0067 - acc: 0.6251 - val_loss: 1.0786 - val_acc: 0.6872\n",
            "Epoch 1329/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 0.9170 - acc: 0.6626 - val_loss: 1.1064 - val_acc: 0.6608\n",
            "Epoch 1330/1500\n",
            "907/907 [==============================] - 0s 131us/sample - loss: 0.9788 - acc: 0.6240 - val_loss: 1.1280 - val_acc: 0.6828\n",
            "Epoch 1331/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 0.9418 - acc: 0.6450 - val_loss: 1.1311 - val_acc: 0.6432\n",
            "Epoch 1332/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 0.9522 - acc: 0.6516 - val_loss: 1.1374 - val_acc: 0.6652\n",
            "Epoch 1333/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 0.9873 - acc: 0.6373 - val_loss: 1.2026 - val_acc: 0.6256\n",
            "Epoch 1334/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 0.9839 - acc: 0.6417 - val_loss: 1.1714 - val_acc: 0.6388\n",
            "Epoch 1335/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 0.9792 - acc: 0.6406 - val_loss: 1.1558 - val_acc: 0.6520\n",
            "Epoch 1336/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 0.9761 - acc: 0.6218 - val_loss: 1.1026 - val_acc: 0.6696\n",
            "Epoch 1337/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 0.9626 - acc: 0.6439 - val_loss: 1.2346 - val_acc: 0.6256\n",
            "Epoch 1338/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 0.9535 - acc: 0.6472 - val_loss: 1.1104 - val_acc: 0.6564\n",
            "Epoch 1339/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.0442 - acc: 0.6119 - val_loss: 1.1209 - val_acc: 0.6608\n",
            "Epoch 1340/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 0.9589 - acc: 0.6483 - val_loss: 1.1003 - val_acc: 0.6608\n",
            "Epoch 1341/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 0.9532 - acc: 0.6196 - val_loss: 1.1419 - val_acc: 0.6432\n",
            "Epoch 1342/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 0.9919 - acc: 0.6174 - val_loss: 1.1044 - val_acc: 0.6652\n",
            "Epoch 1343/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 0.9081 - acc: 0.6803 - val_loss: 1.1626 - val_acc: 0.6300\n",
            "Epoch 1344/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 0.9930 - acc: 0.6273 - val_loss: 1.1586 - val_acc: 0.6564\n",
            "Epoch 1345/1500\n",
            "907/907 [==============================] - 0s 160us/sample - loss: 0.9791 - acc: 0.6329 - val_loss: 1.1436 - val_acc: 0.6608\n",
            "Epoch 1346/1500\n",
            "907/907 [==============================] - 0s 164us/sample - loss: 0.9264 - acc: 0.6450 - val_loss: 1.1596 - val_acc: 0.6211\n",
            "Epoch 1347/1500\n",
            "907/907 [==============================] - 0s 161us/sample - loss: 0.9832 - acc: 0.6428 - val_loss: 1.1568 - val_acc: 0.6652\n",
            "Epoch 1348/1500\n",
            "907/907 [==============================] - 0s 156us/sample - loss: 0.9636 - acc: 0.6340 - val_loss: 1.0951 - val_acc: 0.6388\n",
            "Epoch 1349/1500\n",
            "907/907 [==============================] - 0s 150us/sample - loss: 1.0063 - acc: 0.6329 - val_loss: 1.1201 - val_acc: 0.6740\n",
            "Epoch 1350/1500\n",
            "907/907 [==============================] - 0s 146us/sample - loss: 0.9649 - acc: 0.6461 - val_loss: 1.0989 - val_acc: 0.6652\n",
            "Epoch 1351/1500\n",
            "907/907 [==============================] - 0s 140us/sample - loss: 0.9826 - acc: 0.6318 - val_loss: 1.1389 - val_acc: 0.6476\n",
            "Epoch 1352/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.0197 - acc: 0.6196 - val_loss: 1.2351 - val_acc: 0.6211\n",
            "Epoch 1353/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 1.0070 - acc: 0.6218 - val_loss: 1.1481 - val_acc: 0.6696\n",
            "Epoch 1354/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.0004 - acc: 0.6362 - val_loss: 1.1384 - val_acc: 0.6520\n",
            "Epoch 1355/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 0.9548 - acc: 0.6362 - val_loss: 1.0809 - val_acc: 0.6784\n",
            "Epoch 1356/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 0.8846 - acc: 0.6847 - val_loss: 1.1202 - val_acc: 0.6784\n",
            "Epoch 1357/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 0.9592 - acc: 0.6395 - val_loss: 1.1186 - val_acc: 0.6432\n",
            "Epoch 1358/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 0.9372 - acc: 0.6648 - val_loss: 1.1134 - val_acc: 0.6652\n",
            "Epoch 1359/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 0.9497 - acc: 0.6461 - val_loss: 1.1322 - val_acc: 0.6476\n",
            "Epoch 1360/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 0.9471 - acc: 0.6417 - val_loss: 1.1149 - val_acc: 0.6652\n",
            "Epoch 1361/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 0.9748 - acc: 0.6417 - val_loss: 1.1317 - val_acc: 0.7004\n",
            "Epoch 1362/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 0.9074 - acc: 0.6626 - val_loss: 1.1721 - val_acc: 0.6696\n",
            "Epoch 1363/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.0336 - acc: 0.6318 - val_loss: 1.1880 - val_acc: 0.6564\n",
            "Epoch 1364/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 1.0782 - acc: 0.6042 - val_loss: 1.1742 - val_acc: 0.6388\n",
            "Epoch 1365/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 0.9688 - acc: 0.6340 - val_loss: 1.1086 - val_acc: 0.6740\n",
            "Epoch 1366/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 0.9840 - acc: 0.6395 - val_loss: 1.1697 - val_acc: 0.6432\n",
            "Epoch 1367/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 0.9964 - acc: 0.6229 - val_loss: 1.1273 - val_acc: 0.6608\n",
            "Epoch 1368/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 0.9785 - acc: 0.6340 - val_loss: 1.0962 - val_acc: 0.6784\n",
            "Epoch 1369/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 1.0339 - acc: 0.6295 - val_loss: 1.1330 - val_acc: 0.6520\n",
            "Epoch 1370/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 1.0501 - acc: 0.6086 - val_loss: 1.1266 - val_acc: 0.6652\n",
            "Epoch 1371/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 0.9780 - acc: 0.6483 - val_loss: 1.1686 - val_acc: 0.6564\n",
            "Epoch 1372/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 0.9829 - acc: 0.6406 - val_loss: 1.1136 - val_acc: 0.6740\n",
            "Epoch 1373/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 0.9037 - acc: 0.6681 - val_loss: 1.1285 - val_acc: 0.6872\n",
            "Epoch 1374/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 1.0054 - acc: 0.6340 - val_loss: 1.1043 - val_acc: 0.6696\n",
            "Epoch 1375/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 0.9452 - acc: 0.6604 - val_loss: 1.1208 - val_acc: 0.6696\n",
            "Epoch 1376/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 0.9915 - acc: 0.6295 - val_loss: 1.1102 - val_acc: 0.6652\n",
            "Epoch 1377/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 0.9530 - acc: 0.6538 - val_loss: 1.1626 - val_acc: 0.6432\n",
            "Epoch 1378/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.0417 - acc: 0.6141 - val_loss: 1.1318 - val_acc: 0.6564\n",
            "Epoch 1379/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 1.0047 - acc: 0.6163 - val_loss: 1.1469 - val_acc: 0.6652\n",
            "Epoch 1380/1500\n",
            "907/907 [==============================] - 0s 131us/sample - loss: 0.9329 - acc: 0.6527 - val_loss: 1.1925 - val_acc: 0.6211\n",
            "Epoch 1381/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0130 - acc: 0.6196 - val_loss: 1.1114 - val_acc: 0.6784\n",
            "Epoch 1382/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 0.9711 - acc: 0.6516 - val_loss: 1.0967 - val_acc: 0.6872\n",
            "Epoch 1383/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 0.9648 - acc: 0.6439 - val_loss: 1.1164 - val_acc: 0.6608\n",
            "Epoch 1384/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 0.9930 - acc: 0.6384 - val_loss: 1.1247 - val_acc: 0.6608\n",
            "Epoch 1385/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 0.9299 - acc: 0.6648 - val_loss: 1.1322 - val_acc: 0.6740\n",
            "Epoch 1386/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 0.9237 - acc: 0.6450 - val_loss: 1.1276 - val_acc: 0.6696\n",
            "Epoch 1387/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 0.9275 - acc: 0.6428 - val_loss: 1.1034 - val_acc: 0.6564\n",
            "Epoch 1388/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 0.9276 - acc: 0.6626 - val_loss: 1.0884 - val_acc: 0.6784\n",
            "Epoch 1389/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 0.9512 - acc: 0.6406 - val_loss: 1.1483 - val_acc: 0.6652\n",
            "Epoch 1390/1500\n",
            "907/907 [==============================] - 0s 109us/sample - loss: 0.9117 - acc: 0.6417 - val_loss: 1.1580 - val_acc: 0.6432\n",
            "Epoch 1391/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 0.9147 - acc: 0.6593 - val_loss: 1.1272 - val_acc: 0.6740\n",
            "Epoch 1392/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 1.0670 - acc: 0.6119 - val_loss: 1.1383 - val_acc: 0.6652\n",
            "Epoch 1393/1500\n",
            "907/907 [==============================] - 0s 106us/sample - loss: 0.9855 - acc: 0.6461 - val_loss: 1.1434 - val_acc: 0.6476\n",
            "Epoch 1394/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 0.9608 - acc: 0.6428 - val_loss: 1.1105 - val_acc: 0.6432\n",
            "Epoch 1395/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 0.9197 - acc: 0.6494 - val_loss: 1.1018 - val_acc: 0.6564\n",
            "Epoch 1396/1500\n",
            "907/907 [==============================] - 0s 142us/sample - loss: 0.9559 - acc: 0.6516 - val_loss: 1.1048 - val_acc: 0.6784\n",
            "Epoch 1397/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 0.9900 - acc: 0.6406 - val_loss: 1.0910 - val_acc: 0.6696\n",
            "Epoch 1398/1500\n",
            "907/907 [==============================] - 0s 131us/sample - loss: 0.9908 - acc: 0.6273 - val_loss: 1.1241 - val_acc: 0.6696\n",
            "Epoch 1399/1500\n",
            "907/907 [==============================] - 0s 153us/sample - loss: 1.0155 - acc: 0.6472 - val_loss: 1.0977 - val_acc: 0.6388\n",
            "Epoch 1400/1500\n",
            "907/907 [==============================] - 0s 132us/sample - loss: 0.9158 - acc: 0.6538 - val_loss: 1.1354 - val_acc: 0.6388\n",
            "Epoch 1401/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 0.9437 - acc: 0.6439 - val_loss: 1.1792 - val_acc: 0.6476\n",
            "Epoch 1402/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 0.9306 - acc: 0.6439 - val_loss: 1.1191 - val_acc: 0.6740\n",
            "Epoch 1403/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 0.9689 - acc: 0.6494 - val_loss: 1.1459 - val_acc: 0.6696\n",
            "Epoch 1404/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.0455 - acc: 0.6174 - val_loss: 1.2265 - val_acc: 0.6388\n",
            "Epoch 1405/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 0.9773 - acc: 0.6571 - val_loss: 1.1267 - val_acc: 0.6608\n",
            "Epoch 1406/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 0.9444 - acc: 0.6538 - val_loss: 1.1014 - val_acc: 0.6740\n",
            "Epoch 1407/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 0.9702 - acc: 0.6549 - val_loss: 1.1311 - val_acc: 0.6784\n",
            "Epoch 1408/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 0.9581 - acc: 0.6417 - val_loss: 1.0943 - val_acc: 0.6828\n",
            "Epoch 1409/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 1.0050 - acc: 0.6218 - val_loss: 1.1388 - val_acc: 0.6740\n",
            "Epoch 1410/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 0.9308 - acc: 0.6538 - val_loss: 1.1410 - val_acc: 0.6564\n",
            "Epoch 1411/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 0.9973 - acc: 0.6273 - val_loss: 1.1138 - val_acc: 0.6608\n",
            "Epoch 1412/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0076 - acc: 0.6340 - val_loss: 1.1185 - val_acc: 0.6608\n",
            "Epoch 1413/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 0.9411 - acc: 0.6516 - val_loss: 1.1729 - val_acc: 0.6432\n",
            "Epoch 1414/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 0.9377 - acc: 0.6483 - val_loss: 1.1692 - val_acc: 0.6520\n",
            "Epoch 1415/1500\n",
            "907/907 [==============================] - 0s 126us/sample - loss: 0.9509 - acc: 0.6461 - val_loss: 1.1364 - val_acc: 0.6872\n",
            "Epoch 1416/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 1.0296 - acc: 0.6075 - val_loss: 1.1532 - val_acc: 0.6696\n",
            "Epoch 1417/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 0.9571 - acc: 0.6373 - val_loss: 1.0538 - val_acc: 0.6872\n",
            "Epoch 1418/1500\n",
            "907/907 [==============================] - 0s 141us/sample - loss: 0.9831 - acc: 0.6295 - val_loss: 1.2006 - val_acc: 0.6388\n",
            "Epoch 1419/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 0.9615 - acc: 0.6538 - val_loss: 1.1081 - val_acc: 0.7137\n",
            "Epoch 1420/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 0.9685 - acc: 0.6340 - val_loss: 1.1011 - val_acc: 0.6784\n",
            "Epoch 1421/1500\n",
            "907/907 [==============================] - 0s 143us/sample - loss: 1.0555 - acc: 0.6086 - val_loss: 1.1228 - val_acc: 0.6696\n",
            "Epoch 1422/1500\n",
            "907/907 [==============================] - 0s 145us/sample - loss: 1.0086 - acc: 0.6240 - val_loss: 1.1327 - val_acc: 0.6520\n",
            "Epoch 1423/1500\n",
            "907/907 [==============================] - 0s 172us/sample - loss: 0.8995 - acc: 0.6748 - val_loss: 1.1092 - val_acc: 0.6696\n",
            "Epoch 1424/1500\n",
            "907/907 [==============================] - 0s 188us/sample - loss: 0.9408 - acc: 0.6373 - val_loss: 1.1113 - val_acc: 0.6784\n",
            "Epoch 1425/1500\n",
            "907/907 [==============================] - 0s 162us/sample - loss: 0.9363 - acc: 0.6494 - val_loss: 1.1198 - val_acc: 0.6740\n",
            "Epoch 1426/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 0.9322 - acc: 0.6549 - val_loss: 1.1244 - val_acc: 0.6828\n",
            "Epoch 1427/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 0.9225 - acc: 0.6593 - val_loss: 1.1388 - val_acc: 0.6608\n",
            "Epoch 1428/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 0.9366 - acc: 0.6384 - val_loss: 1.1123 - val_acc: 0.6696\n",
            "Epoch 1429/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 0.9240 - acc: 0.6461 - val_loss: 1.1509 - val_acc: 0.6608\n",
            "Epoch 1430/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 0.9489 - acc: 0.6406 - val_loss: 1.1372 - val_acc: 0.6608\n",
            "Epoch 1431/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 0.9741 - acc: 0.6439 - val_loss: 1.1094 - val_acc: 0.6564\n",
            "Epoch 1432/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 0.9864 - acc: 0.6406 - val_loss: 1.0902 - val_acc: 0.6740\n",
            "Epoch 1433/1500\n",
            "907/907 [==============================] - 0s 135us/sample - loss: 0.9195 - acc: 0.6648 - val_loss: 1.1130 - val_acc: 0.6564\n",
            "Epoch 1434/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 0.9341 - acc: 0.6483 - val_loss: 1.0802 - val_acc: 0.6740\n",
            "Epoch 1435/1500\n",
            "907/907 [==============================] - 0s 138us/sample - loss: 0.9989 - acc: 0.6395 - val_loss: 1.1169 - val_acc: 0.6608\n",
            "Epoch 1436/1500\n",
            "907/907 [==============================] - 0s 109us/sample - loss: 0.9752 - acc: 0.6395 - val_loss: 1.1187 - val_acc: 0.6872\n",
            "Epoch 1437/1500\n",
            "907/907 [==============================] - 0s 106us/sample - loss: 0.9242 - acc: 0.6748 - val_loss: 1.1319 - val_acc: 0.6784\n",
            "Epoch 1438/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 0.9658 - acc: 0.6472 - val_loss: 1.1560 - val_acc: 0.6740\n",
            "Epoch 1439/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 0.9007 - acc: 0.6913 - val_loss: 1.0898 - val_acc: 0.6872\n",
            "Epoch 1440/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 0.9263 - acc: 0.6615 - val_loss: 1.1346 - val_acc: 0.6652\n",
            "Epoch 1441/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 0.9504 - acc: 0.6362 - val_loss: 1.1423 - val_acc: 0.6916\n",
            "Epoch 1442/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 0.9175 - acc: 0.6659 - val_loss: 1.1474 - val_acc: 0.6696\n",
            "Epoch 1443/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 0.9884 - acc: 0.6395 - val_loss: 1.1906 - val_acc: 0.6300\n",
            "Epoch 1444/1500\n",
            "907/907 [==============================] - 0s 113us/sample - loss: 1.0546 - acc: 0.6196 - val_loss: 1.1208 - val_acc: 0.6784\n",
            "Epoch 1445/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.0252 - acc: 0.6218 - val_loss: 1.1367 - val_acc: 0.6520\n",
            "Epoch 1446/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 0.9286 - acc: 0.6538 - val_loss: 1.1446 - val_acc: 0.6300\n",
            "Epoch 1447/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 0.9229 - acc: 0.6560 - val_loss: 1.1256 - val_acc: 0.6696\n",
            "Epoch 1448/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 0.9528 - acc: 0.6307 - val_loss: 1.1547 - val_acc: 0.6476\n",
            "Epoch 1449/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 0.9805 - acc: 0.6295 - val_loss: 1.0993 - val_acc: 0.6828\n",
            "Epoch 1450/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 0.9373 - acc: 0.6439 - val_loss: 1.1194 - val_acc: 0.6696\n",
            "Epoch 1451/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 0.9714 - acc: 0.6295 - val_loss: 1.1466 - val_acc: 0.6696\n",
            "Epoch 1452/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 0.9679 - acc: 0.6351 - val_loss: 1.1447 - val_acc: 0.6564\n",
            "Epoch 1453/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 0.9596 - acc: 0.6384 - val_loss: 1.1436 - val_acc: 0.6564\n",
            "Epoch 1454/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 0.9391 - acc: 0.6626 - val_loss: 1.1324 - val_acc: 0.7225\n",
            "Epoch 1455/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 0.9784 - acc: 0.6295 - val_loss: 1.1699 - val_acc: 0.6432\n",
            "Epoch 1456/1500\n",
            "907/907 [==============================] - 0s 124us/sample - loss: 0.9799 - acc: 0.6395 - val_loss: 1.1227 - val_acc: 0.6828\n",
            "Epoch 1457/1500\n",
            "907/907 [==============================] - 0s 133us/sample - loss: 0.9451 - acc: 0.6560 - val_loss: 1.1098 - val_acc: 0.6960\n",
            "Epoch 1458/1500\n",
            "907/907 [==============================] - 0s 150us/sample - loss: 0.9249 - acc: 0.6604 - val_loss: 1.1185 - val_acc: 0.7093\n",
            "Epoch 1459/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 0.9079 - acc: 0.6450 - val_loss: 1.1998 - val_acc: 0.6211\n",
            "Epoch 1460/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 0.9948 - acc: 0.6196 - val_loss: 1.1283 - val_acc: 0.6520\n",
            "Epoch 1461/1500\n",
            "907/907 [==============================] - 0s 121us/sample - loss: 0.9774 - acc: 0.6439 - val_loss: 1.1159 - val_acc: 0.7269\n",
            "Epoch 1462/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 1.0156 - acc: 0.6240 - val_loss: 1.1636 - val_acc: 0.6388\n",
            "Epoch 1463/1500\n",
            "907/907 [==============================] - 0s 115us/sample - loss: 0.9621 - acc: 0.6593 - val_loss: 1.1422 - val_acc: 0.6344\n",
            "Epoch 1464/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 0.9284 - acc: 0.6395 - val_loss: 1.1842 - val_acc: 0.6652\n",
            "Epoch 1465/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 0.9572 - acc: 0.6483 - val_loss: 1.1087 - val_acc: 0.6564\n",
            "Epoch 1466/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 0.9498 - acc: 0.6549 - val_loss: 1.0867 - val_acc: 0.6872\n",
            "Epoch 1467/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 0.9492 - acc: 0.6439 - val_loss: 1.1043 - val_acc: 0.6696\n",
            "Epoch 1468/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 0.9539 - acc: 0.6604 - val_loss: 1.1195 - val_acc: 0.6872\n",
            "Epoch 1469/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 0.9660 - acc: 0.6538 - val_loss: 1.1444 - val_acc: 0.6696\n",
            "Epoch 1470/1500\n",
            "907/907 [==============================] - 0s 111us/sample - loss: 0.9937 - acc: 0.6395 - val_loss: 1.1770 - val_acc: 0.6740\n",
            "Epoch 1471/1500\n",
            "907/907 [==============================] - 0s 112us/sample - loss: 1.0234 - acc: 0.6284 - val_loss: 1.1624 - val_acc: 0.6740\n",
            "Epoch 1472/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 0.9771 - acc: 0.6406 - val_loss: 1.1365 - val_acc: 0.6564\n",
            "Epoch 1473/1500\n",
            "907/907 [==============================] - 0s 128us/sample - loss: 1.0223 - acc: 0.6439 - val_loss: 1.1153 - val_acc: 0.6256\n",
            "Epoch 1474/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 0.9600 - acc: 0.6560 - val_loss: 1.0805 - val_acc: 0.6608\n",
            "Epoch 1475/1500\n",
            "907/907 [==============================] - 0s 138us/sample - loss: 0.9484 - acc: 0.6516 - val_loss: 1.0849 - val_acc: 0.6652\n",
            "Epoch 1476/1500\n",
            "907/907 [==============================] - 0s 149us/sample - loss: 0.9211 - acc: 0.6549 - val_loss: 1.1254 - val_acc: 0.6564\n",
            "Epoch 1477/1500\n",
            "907/907 [==============================] - 0s 132us/sample - loss: 0.9902 - acc: 0.6329 - val_loss: 1.1489 - val_acc: 0.6476\n",
            "Epoch 1478/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 0.9578 - acc: 0.6494 - val_loss: 1.1074 - val_acc: 0.7048\n",
            "Epoch 1479/1500\n",
            "907/907 [==============================] - 0s 118us/sample - loss: 0.9306 - acc: 0.6516 - val_loss: 1.0901 - val_acc: 0.7137\n",
            "Epoch 1480/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 0.9023 - acc: 0.6527 - val_loss: 1.1293 - val_acc: 0.6828\n",
            "Epoch 1481/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 0.9104 - acc: 0.6648 - val_loss: 1.1471 - val_acc: 0.6520\n",
            "Epoch 1482/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 0.9820 - acc: 0.6417 - val_loss: 1.2023 - val_acc: 0.6520\n",
            "Epoch 1483/1500\n",
            "907/907 [==============================] - 0s 129us/sample - loss: 0.9435 - acc: 0.6527 - val_loss: 1.1187 - val_acc: 0.6476\n",
            "Epoch 1484/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 0.9721 - acc: 0.6340 - val_loss: 1.1658 - val_acc: 0.6960\n",
            "Epoch 1485/1500\n",
            "907/907 [==============================] - 0s 132us/sample - loss: 0.9347 - acc: 0.6527 - val_loss: 1.1850 - val_acc: 0.6476\n",
            "Epoch 1486/1500\n",
            "907/907 [==============================] - 0s 122us/sample - loss: 0.8975 - acc: 0.6659 - val_loss: 1.1502 - val_acc: 0.6652\n",
            "Epoch 1487/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 0.9164 - acc: 0.6593 - val_loss: 1.1238 - val_acc: 0.6916\n",
            "Epoch 1488/1500\n",
            "907/907 [==============================] - 0s 127us/sample - loss: 0.9985 - acc: 0.6406 - val_loss: 1.1311 - val_acc: 0.6740\n",
            "Epoch 1489/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 0.9716 - acc: 0.6351 - val_loss: 1.1438 - val_acc: 0.6740\n",
            "Epoch 1490/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 0.9511 - acc: 0.6670 - val_loss: 1.1534 - val_acc: 0.6740\n",
            "Epoch 1491/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 0.9215 - acc: 0.6417 - val_loss: 1.1680 - val_acc: 0.6872\n",
            "Epoch 1492/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 0.9081 - acc: 0.6681 - val_loss: 1.1468 - val_acc: 0.6872\n",
            "Epoch 1493/1500\n",
            "907/907 [==============================] - 0s 117us/sample - loss: 0.9752 - acc: 0.6329 - val_loss: 1.1387 - val_acc: 0.6344\n",
            "Epoch 1494/1500\n",
            "907/907 [==============================] - 0s 114us/sample - loss: 1.0127 - acc: 0.6240 - val_loss: 1.0858 - val_acc: 0.6740\n",
            "Epoch 1495/1500\n",
            "907/907 [==============================] - 0s 119us/sample - loss: 1.0059 - acc: 0.6340 - val_loss: 1.1618 - val_acc: 0.6432\n",
            "Epoch 1496/1500\n",
            "907/907 [==============================] - 0s 120us/sample - loss: 0.9962 - acc: 0.6351 - val_loss: 1.1531 - val_acc: 0.6652\n",
            "Epoch 1497/1500\n",
            "907/907 [==============================] - 0s 110us/sample - loss: 0.9539 - acc: 0.6604 - val_loss: 1.1233 - val_acc: 0.6872\n",
            "Epoch 1498/1500\n",
            "907/907 [==============================] - 0s 125us/sample - loss: 0.9184 - acc: 0.6703 - val_loss: 1.1182 - val_acc: 0.7048\n",
            "Epoch 1499/1500\n",
            "907/907 [==============================] - 0s 116us/sample - loss: 0.9869 - acc: 0.6428 - val_loss: 1.1311 - val_acc: 0.6740\n",
            "Epoch 1500/1500\n",
            "907/907 [==============================] - 0s 123us/sample - loss: 0.9156 - acc: 0.6604 - val_loss: 1.1576 - val_acc: 0.6652\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbVu0RRmTNaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Abrindo o modelo JSON e salvando\n",
        "def save(model):\n",
        "    model_json = model.to_json()\n",
        "    with open(\"model.json\", \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "    # serialize weights to HDF5\n",
        "    model.save_weights(\"model.h5\")\n",
        "    print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-TT7vSqUai1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Carregando e criando o modelo \n",
        "def load(modelname,wname):\n",
        "    json_file = open(modelname, 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    loaded_model = model_from_json(loaded_model_json)\n",
        "    # load weights into new model\n",
        "    loaded_model.load_weights(wname)\n",
        "    print(\"Loaded model from disk\")\n",
        "    return loaded_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtwOcmHlUdQU",
        "colab_type": "code",
        "outputId": "cb66364b-0747-4676-acb4-c03d2a3d0b50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "save(model)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hYc0lBQUfln",
        "colab_type": "code",
        "outputId": "4a8c85b9-131e-459f-f724-c1626b3d52ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loaded_model = load(\"model.json\",\"model.h5\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYtp9vSUUjE6",
        "colab_type": "code",
        "outputId": "063438d3-3259-4901-e810-b149096a0266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# evaluate loaded model on test data\n",
        "loaded_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# evaluate the model\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acc: 66.52%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okkFqpD-O5wf",
        "colab_type": "text"
      },
      "source": [
        "# **Plot da acurácia e acurácia da validação**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r-qav1mJS5F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "0945118c-2ed3-4841-85cc-2ae9d4555097"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "\n",
        "pyplot.plot(history.history['acc'])\n",
        "pyplot.show()"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw5KOILMO1TN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "7d202fb1-b84a-4166-8c1c-21c2cb8b8116"
      },
      "source": [
        "pyplot.plot(history.history['val_acc'])\n",
        "pyplot.show()"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnRrCb_LQEkw",
        "colab_type": "text"
      },
      "source": [
        "# **Métricas de desempenho**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gaa724ZX2EvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicts = model.predict(x_test)\n",
        "y_pred = (predicts > 0.5)\n",
        "\n",
        "y_pred = np.argmax(y_pred, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a0smycuIJ6v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "ec43a2e5-53e6-40a8-bbfc-1c964339687a"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.89      0.72        19\n",
            "           1       1.00      0.93      0.97        15\n",
            "           2       1.00      0.75      0.86        16\n",
            "           3       0.93      0.93      0.93        15\n",
            "           4       0.94      1.00      0.97        16\n",
            "           5       1.00      1.00      1.00        17\n",
            "           6       0.93      0.93      0.93        14\n",
            "           7       0.95      1.00      0.98        20\n",
            "           8       1.00      0.71      0.83         7\n",
            "           9       0.94      1.00      0.97        17\n",
            "          10       0.95      1.00      0.97        19\n",
            "          11       1.00      0.75      0.86        20\n",
            "          12       1.00      0.71      0.83        14\n",
            "          13       0.86      1.00      0.92        18\n",
            "\n",
            "    accuracy                           0.91       227\n",
            "   macro avg       0.94      0.90      0.91       227\n",
            "weighted avg       0.93      0.91      0.91       227\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAuqrKjtPf5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}